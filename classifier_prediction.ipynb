{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6L_98L8_9_A6",
    "outputId": "b5acc7eb-337c-4db0-b588-13a05c8d3b60"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Signal Classification Model Prediction & Evaluation\n",
    "This notebook provides comprehensive evaluation of trained signal classification models.\n",
    "Updated to match the training notebook structure and handle 3 known signal types + UNKNOWN class.\n",
    "\n",
    "Author: Jason Niow  \n",
    "Date: Updated for consistency with classifier_training.ipynb\n",
    "'''\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "\n",
    "print(f'tensorflow version: {tf.__version__}')\n",
    "print(f'pandas version: {pd.__version__}')\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'seaborn version: {sns.__version__}')\n",
    "\n",
    "# check tensorflow GPU device support\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs.\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f'GPU present: {gpus}')\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set at program startup.\n",
    "        print(e)\n",
    "else:\n",
    "    print('GPU absent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Classification Model Prediction & Evaluation\n",
    "\n",
    "This notebook provides prediction capabilities and comprehensive evaluation of trained signal classification models.\n",
    "\n",
    "Features\n",
    "- Load trained ResNet model for signal classification\n",
    "- Process test datasets (16QAM, 8CPSK, FM, Unknown)\n",
    "- Generate predictions and evaluate model performance\n",
    "- Comprehensive metrics including confusion matrix, F1-score, precision, recall\n",
    "- Prediction confidence analysis and visualization\n",
    "\n",
    "Model Architecture\n",
    "- ResNet-based CNN for I/Q signal classification\n",
    "- Supports 4-class classification: 16QAM, 8CPSK, FM, UNKNOWN\n",
    "- Input: 2-channel I/Q data with 1024 samples per record\n",
    "- Output: Softmax probability distribution over signal classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "This section loads and prepares the test dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rms_normalize(iq, target_rms=1.0, eps=1e-12, remove_dc=True):\n",
    "    \"\"\"\n",
    "    iq: complex64/complex128 numpy array, shape (N,)\n",
    "    target_rms: desired RMS after scaling\n",
    "    \"\"\"\n",
    "    x = iq.astype(np.complex64, copy=False)\n",
    "    if remove_dc:\n",
    "        x = x - x.mean()\n",
    "    rms = np.sqrt(np.mean(np.abs(x)**2) + eps)\n",
    "    return x * (target_rms / rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose test folder. Extract test files information from test folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "baseline_rms = True\n",
    "test_rms = True # whether to apply RMS normalization to test data\n",
    "\n",
    "# paths to load datasets from\n",
    "test_folder = 'test_mixed_3'  # 'real' or 'combined' or 'syn'\n",
    "pred_store_path = f'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/test_datasets/{test_folder}'\n",
    "\n",
    "# ONLY FOR EXPERIMENTAL PURPOSES\n",
    "# test_folder = 'train_jul22'\n",
    "# pred_store_path = f'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/train_datasets/{test_folder}'\n",
    "\n",
    "\n",
    "# convert to pathlib Path objects\n",
    "pred_dir = pathlib.Path(pred_store_path)\n",
    "\n",
    "# get list of datasets paths in dir\n",
    "pred_ds_paths = sorted(list(pred_dir.glob('*.csv')))\n",
    "\n",
    "\n",
    "# extract classification target from file names\n",
    "pred_ds_type = np.array([x.parts[-1].split('_')[:2] for x in pred_ds_paths])\n",
    "\n",
    "# Get list of classification labels of dataset e.g. 16QAM, 8CPSK, FM\n",
    "pred_ds_mod = [s.upper() for s in pred_ds_type[:,0]]\n",
    "\n",
    "# Get list of classification frequency\n",
    "pred_ds_freq = [s.upper() for s in pred_ds_type[:, 1]]\n",
    "\n",
    "# generate signal type tags - match training notebook exactly\n",
    "known_signal_tags = {'16QAM', '8CPSK', 'FM'}\n",
    "signal_tags = {'16QAM': 0, '8CPSK': 1, 'FM': 2, 'UNKNOWN': 3}\n",
    "\n",
    "print(signal_tags)\n",
    "print(f\"Known signal types: {list(known_signal_tags)}\")\n",
    "print(f\"Detected signal types in test data: {list(set(pred_ds_mod))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads and processes test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYz_dHmr8jVE",
    "outputId": "f469edd9-a826-41d0-ee4b-6ef13243761b"
   },
   "outputs": [],
   "source": [
    "# Get list of classification labels of dataset e.g. 16QAM, 8CPSK\n",
    "pred_ds_mod = [s.upper() for s in pred_ds_type[:,0]]\n",
    "\n",
    "# Get list of classification frequencym\n",
    "pred_ds_freq = [s.upper() for s in pred_ds_type[:, 1]]\n",
    "\n",
    "# load the dataset(s)\n",
    "\n",
    "# load dataset information\n",
    "specs = []\n",
    "predData = []\n",
    "\n",
    "for path in pred_ds_paths:\n",
    "    print(f'loading {path}...', end=' ')\n",
    "\n",
    "    # load dataset details - Sampling frequency, Number of Samples, Number of Records\n",
    "    df_spec = pd.read_csv(path, skiprows=10, header=None, index_col=0, names=['info'])\n",
    "    # df_spec = pd.read_csv(path, nrows=10, header=None, index_col=0, names=['info'])\n",
    "    # df_spec = df_spec.drop(['Version', 'DateTime', 'TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'], axis=0).astype('int')\n",
    "\n",
    "    specs.append(df_spec)\n",
    "\n",
    "    # load data, strip unnecessary bits out - I/Q data\n",
    "    df = pd.read_csv(path, skiprows=10, names=['I', 'Q'])\n",
    "    df = df.loc[~df['I'].isin(['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'])]\n",
    "    df['I'] = df['I'].astype('float')\n",
    "\n",
    "    print(f'loaded')\n",
    "\n",
    "    predData.append(df)\n",
    "\n",
    "print('done.')\n",
    "\n",
    "# split dataset(s) into records, extract test dataset\n",
    "processed = []\n",
    "processed_baseline = []\n",
    "\n",
    "# number of test records to extract and record length\n",
    "ntest = 100  # can be adjusted based on available data\n",
    "rlength = 1024\n",
    "\n",
    "nsamples = 10000\n",
    "nrecords = 1\n",
    "\n",
    "print(f'\\nType\\t\\tLocation\\tTotal Records\\tSamples/Record')\n",
    "for i, data in enumerate(predData):\n",
    "    # nrecords = specs[i].loc['NumberRecords']['info']\n",
    "    # nsamples = specs[i].loc['NumberSamples']['info']\n",
    "    ds_length = data.shape[0]\n",
    "    filename = pred_ds_paths[i].name\n",
    "\n",
    "    # make life easier\n",
    "    ds_mod = pred_ds_mod\n",
    "    ds_freq = pred_ds_freq\n",
    "\n",
    "    # sanity check\n",
    "    print(f'{ds_mod[i]:<13}\\t{ds_freq[i]:<15}\\t{nrecords:<7}\\t\\t{nsamples:<7}')\n",
    "\n",
    "    # loop through dataset to split\n",
    "    for j in range(nrecords):\n",
    "        # extract sample length worth of samples for each record, then transpose for easier access later\n",
    "\n",
    "        # Baseline model does not perform RMS normalization\n",
    "        # Hence it has to be processed differently\n",
    "        if baseline_rms:\n",
    "            iq = data.iloc[(nsamples * j):(nsamples * (j+1))].values\n",
    "            iq_complex = iq[:, 0] + 1j * iq[:, 1]\n",
    "            iq_rms = rms_normalize(iq_complex, target_rms=1.0, remove_dc=False).T\n",
    "            i_rms = iq_rms.real.reshape((1, -1))\n",
    "            q_rms = iq_rms.imag.reshape((1, -1))\n",
    "            record_baseline = np.vstack((i_rms, q_rms))\n",
    "        else:\n",
    "            record_baseline = data.iloc[(nsamples * j):(nsamples * (j+1))].values.T\n",
    "\n",
    "        # Test model may or may not perform RMS normalization\n",
    "        if test_rms:\n",
    "            iq = data.iloc[(nsamples * j):(nsamples * (j+1))].values\n",
    "            iq_complex = iq[:, 0] + 1j * iq[:, 1]\n",
    "            iq_rms = rms_normalize(iq_complex, target_rms=1.0, remove_dc=False).T\n",
    "            i_rms = iq_rms.real.reshape((1, -1))\n",
    "            q_rms = iq_rms.imag.reshape((1, -1))\n",
    "            record = np.vstack((i_rms, q_rms))\n",
    "        else:\n",
    "            record = data.iloc[(nsamples * j):(nsamples * (j+1))].values.T\n",
    "\n",
    "        # pad shorter records with random padding to rlength\n",
    "        if nsamples < rlength:\n",
    "            print(f\"i: {i} j : {j} Sample length {nsamples} is lesser than {rlength}\")\n",
    "            # determine pad amount\n",
    "            pad_length = rlength - nsamples\n",
    "            lpad_length = np.random.randint(0, pad_length+1)\n",
    "            rpad_length = pad_length - lpad_length\n",
    "\n",
    "            # generate pad\n",
    "            lpad = np.zeros((2, lpad_length))\n",
    "            rpad = np.zeros((2, rpad_length))\n",
    "\n",
    "            # concatenate pad\n",
    "            record = np.concatenate([lpad, record, rpad], axis=1)\n",
    "            record_baseline = np.concatenate([lpad, record_baseline, rpad], axis=1)\n",
    "\n",
    "        # truncate longer records to rlength\n",
    "        elif nsamples > rlength:\n",
    "            record = record[:,:rlength]\n",
    "            record_baseline = record_baseline[:,:rlength]\n",
    "\n",
    "        # add processed record to list with proper signal tag handling\n",
    "        signal_tag = signal_tags.get(ds_mod[i], signal_tags['UNKNOWN'])  # default to UNKNOWN if not one of the known classes\n",
    "        processed.append([filename, ds_mod[i], signal_tag, ds_freq[i], record])\n",
    "        processed_baseline.append([filename, ds_mod[i], signal_tag, ds_freq[i], record_baseline])\n",
    "\n",
    "# convert list into dataframes for later use, randomise, extract test records\n",
    "df_test = pd.DataFrame(processed, columns=['filename', 'signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
    "df_test_baseline = pd.DataFrame(processed_baseline, columns=['filename', 'signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
    "\n",
    "# print dataset statistics\n",
    "print(f'\\n{\"Stats\":^30}')\n",
    "print(f'Dataset\\tLength\\tRecords/Sample')\n",
    "print(f'Test\\t{df_test.shape[0]:<5}\\t{df_test[\"record\"].iloc[0].shape[1]}')\n",
    "\n",
    "# Show signal distribution\n",
    "print(f'\\nSignal Distribution:')\n",
    "signal_counts = df_test['signal_type'].value_counts()\n",
    "for signal_type, count in signal_counts.items():\n",
    "    print(f'{signal_type}: {count} samples')\n",
    "\n",
    "# define one hot encode function\n",
    "def one_hot(arr, n_cat):\n",
    "    output = []\n",
    "    for n in arr:\n",
    "        result = np.zeros(n_cat)\n",
    "        result[n] = 1\n",
    "        output.append(result)\n",
    "    return np.array(output, dtype=int)\n",
    "\n",
    "# extract test data\n",
    "X_test = np.concatenate(df_test['record'].values).reshape((df_test.shape[0], 2, rlength, 1))\n",
    "y_test = one_hot(df_test['tag'].values, len(signal_tags))\n",
    "\n",
    "X_test_baseline = np.concatenate(df_test_baseline['record'].values).reshape((df_test_baseline.shape[0], 2, rlength, 1))\n",
    "y_test_baseline = one_hot(df_test_baseline['tag'].values, len(signal_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Accuracy Metric Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model with Entropy Regularization\n",
    "\n",
    "# Custom accuracy metric that only considers in-distribution samples\n",
    "class IDAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='id_accuracy', **kwargs):\n",
    "        super(IDAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Extract class labels and OOD flags\n",
    "        class_labels = y_true[:, :-1]\n",
    "        is_ood = y_true[:, -1]\n",
    "        \n",
    "        # Only consider in-distribution samples (is_ood == 0)\n",
    "        id_mask = tf.equal(is_ood, 0)\n",
    "        \n",
    "        # Get predictions for ID samples only\n",
    "        id_predictions = tf.boolean_mask(y_pred, id_mask)\n",
    "        id_true_labels = tf.boolean_mask(class_labels, id_mask)\n",
    "        \n",
    "        # Calculate accuracy for ID samples\n",
    "        if tf.size(id_predictions) > 0:\n",
    "            matches = tf.equal(\n",
    "                tf.argmax(id_true_labels, axis=1),\n",
    "                tf.argmax(id_predictions, axis=1)\n",
    "            )\n",
    "            matches = tf.cast(matches, tf.float32)\n",
    "            \n",
    "            self.total.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.size(matches), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.total, self.count)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total.assign(0)\n",
    "        self.count.assign(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Entropy Loss Metric Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def masked_mean(values, mask):\n",
    "    mask = tf.cast(mask, values.dtype)            # [B]\n",
    "    num = tf.reduce_sum(values * mask)            # sum over selected\n",
    "    den = tf.reduce_sum(mask)                     # count selected\n",
    "    return tf.math.divide_no_nan(num, den)        # safe mean\n",
    "\n",
    "class EntropyRegularizedLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Custom loss function that combines classification loss with entropy regularization\n",
    "    for better OOD detection performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, entropy_weight=0.1, ood_entropy_weight=1.0, reduction=tf.keras.losses.Reduction.AUTO, name=\"entropy_regularized\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            entropy_weight: Weight for entropy regularization on in-distribution samples\n",
    "            ood_entropy_weight: Weight for entropy maximization on OOD proxy samples\n",
    "            reduction: Type of reduction to apply to loss\n",
    "            name: Name of the loss\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.ood_entropy_weight = ood_entropy_weight\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute combined loss: classification + entropy regularization\n",
    "        \n",
    "        y_true format: [class_labels, is_ood_flag]\n",
    "        - class_labels: one-hot encoded class labels \n",
    "        - is_ood_flag: 1 for OOD samples, 0 for in-distribution samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split true labels and OOD flag\n",
    "        class_labels = y_true[:, :-1]  # All columns except last\n",
    "        is_ood = y_true[:, -1:] # Last column indicates OOD (1) or ID (0)\n",
    "        \n",
    "        # Standard classification loss (only for in-distribution samples)\n",
    "        id_mask = tf.equal(is_ood, 0)\n",
    "        id_mask = tf.squeeze(id_mask, axis=1)\n",
    "        \n",
    "        # Classification loss for in-distribution samples\n",
    "        classification_loss = tf.where(\n",
    "            id_mask,\n",
    "            tf.keras.losses.categorical_crossentropy(class_labels, y_pred),\n",
    "            0.0\n",
    "        )\n",
    "        classification_loss = tf.reduce_mean(classification_loss)\n",
    "        \n",
    "        # Entropy calculation: H(p) = -sum(p * log(p))\n",
    "        epsilon = 1e-8  # Small constant to prevent log(0)\n",
    "        entropy = -tf.reduce_sum(y_pred * tf.math.log(y_pred + epsilon), axis=1)\n",
    "        max_entropy = tf.math.log(tf.cast(tf.shape(y_pred)[1], tf.float32))  # log(num_classes)\n",
    "        \n",
    "        # Entropy regularization for in-distribution samples (encourage lower entropy/higher confidence)\n",
    "        id_entropy_loss = tf.where(\n",
    "            id_mask,\n",
    "            entropy,  # Minimize entropy for ID samples (encourage high confidence)\n",
    "            0.0\n",
    "        )\n",
    "        id_entropy_loss = tf.reduce_mean(id_entropy_loss) * self.entropy_weight\n",
    "        \n",
    "        # Entropy maximization for OOD samples (encourage higher entropy/lower confidence)\n",
    "        ood_mask = tf.equal(is_ood, 1)\n",
    "        ood_mask = tf.squeeze(ood_mask, axis=1)\n",
    "        \n",
    "        ood_entropy_loss = tf.where(\n",
    "            ood_mask,\n",
    "            -(entropy),  # Maximize entropy for OOD samples (encourage low confidence)\n",
    "            0.0\n",
    "        )\n",
    "        ood_entropy_loss = tf.reduce_mean(ood_entropy_loss) * self.ood_entropy_weight\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = classification_loss + id_entropy_loss + ood_entropy_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the models. Select model to test here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_phmrOgutF6J",
    "outputId": "119230ee-43b3-4c5b-9693-6cabc75c754a"
   },
   "outputs": [],
   "source": [
    "# evaluate model performance on test dataset\n",
    "\n",
    "# LOAD TRAINED MODEL\n",
    "model_baseline_name = 'jul22_rms_1'\n",
    "model_test_name = 'jul30_entropy_1'\n",
    "model_baseline = tf.keras.models.load_model(f'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/{model_baseline_name}')  # No extension - uses SavedModel format\n",
    "model_test = tf.keras.models.load_model(f'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/{model_test_name}',\n",
    "                                        custom_objects={'IDAccuracy': IDAccuracy,\n",
    "                                                        'EntropyRegularizedLoss': EntropyRegularizedLoss})  # No extension - uses SavedModel format\n",
    "# Unknown threshold\n",
    "threshold = 0.9\n",
    "\n",
    "# print(\"Evaluating model performance...\")\n",
    "# results = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# print(f'\\nModel Performance:')\n",
    "# print(f'Loss: {results[0]:.4f}')\n",
    "# print(f'Accuracy: {results[1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate predictions\n",
    "print(\"\\nGenerating predictions for baseline model...\")\n",
    "cm_pred_baseline = model_baseline.predict(X_test_baseline, verbose=1)\n",
    "\n",
    "# process results - get predicted class indices\n",
    "highest_probabilities_baseline = np.max(cm_pred_baseline, axis=1)\n",
    "# filter predictions based on threshold\n",
    "predicted_classes_baseline = np.argmax(cm_pred_baseline, axis=1)\n",
    "# if probability is below threshold, classify as UNKNOWN\n",
    "predicted_classes_baseline = np.where(highest_probabilities_baseline < threshold, signal_tags['UNKNOWN'], predicted_classes_baseline)\n",
    "true_classes = df_test['tag'].values\n",
    "\n",
    "# generate confusion matrix\n",
    "tags = list(signal_tags.keys())\n",
    "cm_matrix_baseline = tf.math.confusion_matrix(true_classes, predicted_classes_baseline, num_classes=len(tags)).numpy()\n",
    "df_cm_baseline = pd.DataFrame(cm_matrix_baseline, index=tags, columns=tags)\n",
    "\n",
    "\n",
    "print(\"\\nGenerating predictions for test model...\")\n",
    "cm_pred_test = model_test.predict(X_test, verbose=1)\n",
    "\n",
    "# process results - get predicted class indices\n",
    "highest_probabilities_test = np.max(cm_pred_test, axis=1)\n",
    "# filter predictions based on threshold\n",
    "predicted_classes_test = np.argmax(cm_pred_test, axis=1)\n",
    "# if probability is below threshold, classify as UNKNOWN\n",
    "predicted_classes_test = np.where(highest_probabilities_test < threshold, signal_tags['UNKNOWN'], predicted_classes_test)\n",
    "\n",
    "# generate confusion matrix\n",
    "tags = list(signal_tags.keys())\n",
    "cm_matrix_test = tf.math.confusion_matrix(true_classes, predicted_classes_test, num_classes=len(tags)).numpy()\n",
    "df_cm_test = pd.DataFrame(cm_matrix_test, index=tags, columns=tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.suptitle(f\"\"\"Base model: {model_baseline_name} \\n\n",
    "             Test model: {model_test_name} \\n\n",
    "             Confusion Matrices - Tested on {test_folder} Data\"\"\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df_cm_baseline, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Signal Classification (Baseline)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(df_cm_test, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Signal Classification (My Model)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_test['signal_type'])):\n",
    "    print(f\"Baseline Record {i+1}: {df_test['signal_type'].iloc[i]}, Predicted: {predicted_classes_baseline[i]}, Probability: {highest_probabilities_baseline[i]:.4f}\")\n",
    "    print(f\"Test Record {i+1} {df_test['signal_type'].iloc[i]}, Predicted: {predicted_classes_test[i]}, Probability: {highest_probabilities_test[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Misclassification Analysis for Both Models\n",
    "\n",
    "def analyze_misclassifications(predicted_classes, model_name, cm_pred):\n",
    "    \"\"\"Analyze misclassifications for a given model\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MISCLASSIFICATION ANALYSIS - {model_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Find indices where predictions are wrong\n",
    "    wrong_predictions = predicted_classes != true_classes\n",
    "    wrong_indices = np.where(wrong_predictions)[0]\n",
    "    accuracy = (~wrong_predictions).mean()\n",
    "    \n",
    "    print(f\"Total samples: {len(true_classes)}\")\n",
    "    print(f\"Correct predictions: {(~wrong_predictions).sum()}\")\n",
    "    print(f\"Wrong predictions: {wrong_predictions.sum()}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Error rate: {wrong_predictions.mean():.4f}\")\n",
    "    \n",
    "    # Analyze misclassification patterns\n",
    "    print(f\"\\nMISCLASSIFICATION PATTERNS:\")\n",
    "    print(f\"{'True â†’ Predicted':<20} {'Count':<8} {'% of True Class':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for true_idx, true_label in enumerate(tags):\n",
    "        true_mask = true_classes == true_idx\n",
    "        true_count = true_mask.sum()\n",
    "        \n",
    "        if true_count > 0:\n",
    "            for pred_idx, pred_label in enumerate(tags):\n",
    "                if true_idx != pred_idx:  # Only show wrong predictions\n",
    "                    wrong_mask = wrong_predictions & true_mask & (predicted_classes == pred_idx)\n",
    "                    wrong_count = wrong_mask.sum()\n",
    "                    \n",
    "                    if wrong_count > 0:\n",
    "                        percentage = (wrong_count / true_count) * 100\n",
    "                        print(f\"{true_label} â†’ {pred_label:<12} {wrong_count:<8} {percentage:<15.1f}%\")\n",
    "    \n",
    "    # Get detailed information about misclassified samples\n",
    "    misclassified_data = []\n",
    "    for idx in wrong_indices:\n",
    "        highest_prob = np.max(cm_pred[idx])\n",
    "        sample_info = {\n",
    "            'index': idx,\n",
    "            'true_class': true_classes[idx],\n",
    "            'true_label': tags[true_classes[idx]],\n",
    "            'predicted_class': predicted_classes[idx], \n",
    "            'predicted_label': tags[predicted_classes[idx]],\n",
    "            'confidence': highest_prob,\n",
    "            'signal_type': df_test.iloc[idx]['signal_type'],\n",
    "            'location': df_test.iloc[idx]['location']\n",
    "        }\n",
    "        misclassified_data.append(sample_info)\n",
    "    \n",
    "    # Sort by confidence (most confident wrong predictions first)\n",
    "    misclassified_data.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTOP 10 MOST CONFIDENT WRONG PREDICTIONS:\")\n",
    "    print(f\"{'Idx':<5} {'True':<8} {'Pred':<8} {'Conf':<8} {'Signal':<12} {'Location':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, sample in enumerate(misclassified_data[:10]):\n",
    "        print(f\"{sample['index']:<5} {sample['true_label']:<8} {sample['predicted_label']:<8} \"\n",
    "              f\"{sample['confidence']:<8.3f} {sample['signal_type']:<12} {sample['location']:<12}\")\n",
    "    \n",
    "    # Create a DataFrame for easier analysis\n",
    "    df_misclassified = pd.DataFrame(misclassified_data)\n",
    "    \n",
    "    print(f\"\\nMISCLASSIFICATION BY SIGNAL TYPE AND LOCATION:\")\n",
    "    if len(df_misclassified) > 0:\n",
    "        misclass_summary = df_misclassified.groupby(['signal_type', 'location']).agg({\n",
    "            'index': 'count',\n",
    "            'confidence': 'mean'\n",
    "        }).round(3)\n",
    "        misclass_summary.columns = ['Count', 'Avg_Confidence']\n",
    "        print(misclass_summary)\n",
    "    else:\n",
    "        print(\"No misclassifications found!\")\n",
    "    \n",
    "    return accuracy, misclassified_data\n",
    "\n",
    "# Analyze both models\n",
    "accuracy_baseline, misclassified_baseline = analyze_misclassifications(\n",
    "    predicted_classes_baseline, \"BASELINE MODEL\", cm_pred_baseline\n",
    ")\n",
    "\n",
    "accuracy_test, misclassified_test = analyze_misclassifications(\n",
    "    predicted_classes_test, \"TEST MODEL\", cm_pred_test\n",
    ")\n",
    "\n",
    "# Compare models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MODEL COMPARISON SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Model':<15} {'Accuracy':<12} {'Error Rate':<12} {'Misclassified':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Baseline':<15} {accuracy_baseline:<12.4f} {(1-accuracy_baseline):<12.4f} {len(misclassified_baseline):<15}\")\n",
    "print(f\"{'Test':<15} {accuracy_test:<12.4f} {(1-accuracy_test):<12.4f} {len(misclassified_test):<15}\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = accuracy_test - accuracy_baseline\n",
    "print(f\"\\nAccuracy Improvement: {improvement:+.4f} ({improvement*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fddbe53",
    "outputId": "63e197dd-a1d5-4ccb-dab3-1499a4a699ef"
   },
   "outputs": [],
   "source": [
    "# Comparative Evaluation Metrics for Both Models\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report\n",
    "\n",
    "def evaluate_model(predicted_classes, model_name):\n",
    "    \"\"\"Calculate and display evaluation metrics for a model\"\"\"\n",
    "    labels = [0, 1, 2, 3]\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = (predicted_classes == true_classes).mean()\n",
    "    f1 = f1_score(true_classes, predicted_classes, average='weighted', labels=labels, zero_division=0)\n",
    "    recall = recall_score(true_classes, predicted_classes, average='weighted', labels=labels, zero_division=0)\n",
    "    precision = precision_score(true_classes, predicted_classes, average='weighted', labels=labels, zero_division=0)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    f1_per_class = f1_score(true_classes, predicted_classes, average=None, labels=labels, zero_division=0)\n",
    "    recall_per_class = recall_score(true_classes, predicted_classes, average=None, labels=labels, zero_division=0)\n",
    "    precision_per_class = precision_score(true_classes, predicted_classes, average=None, labels=labels, zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION RESULTS - {model_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Test Dataset: {pred_store_path}\")\n",
    "    print(f\"Date: {pd.Timestamp.now().strftime('%B %d, %Y')}\")\n",
    "    print(f\"Total Test Samples: {len(true_classes)}\")\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(f\"\\nOVERALL METRICS:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPER-CLASS METRICS:\")\n",
    "    for i, signal_type in enumerate(tags):\n",
    "        count = (true_classes == i).sum()\n",
    "        print(f\"{signal_type:>8}: F1={f1_per_class[i]:.3f}, Recall={recall_per_class[i]:.3f}, Precision={precision_per_class[i]:.3f}, Count={count}\")\n",
    "    \n",
    "    print(f\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=tags, labels=labels, zero_division=0))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'precision_per_class': precision_per_class\n",
    "    }\n",
    "\n",
    "# Evaluate both models\n",
    "baseline_metrics = evaluate_model(predicted_classes_baseline, \"BASELINE MODEL\")\n",
    "test_metrics = evaluate_model(predicted_classes_test, \"TEST MODEL\")\n",
    "\n",
    "# Create comparison table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPARATIVE METRICS SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Metric':<15} {'Baseline':<12} {'Test Model':<12} {'Improvement':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "metrics_to_compare = ['accuracy', 'f1', 'recall', 'precision']\n",
    "for metric in metrics_to_compare:\n",
    "    baseline_val = baseline_metrics[metric]\n",
    "    test_val = test_metrics[metric]\n",
    "    improvement = test_val - baseline_val\n",
    "    print(f\"{metric.capitalize():<15} {baseline_val:<12.4f} {test_val:<12.4f} {improvement:+12.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PER-CLASS COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Class':<10} {'Metric':<10} {'Baseline':<10} {'Test':<10} {'Improvement':<12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, signal_type in enumerate(tags):\n",
    "    for metric in ['f1', 'recall', 'precision']:\n",
    "        baseline_val = baseline_metrics[f'{metric}_per_class'][i]\n",
    "        test_val = test_metrics[f'{metric}_per_class'][i]\n",
    "        improvement = test_val - baseline_val\n",
    "        print(f\"{signal_type:<10} {metric.upper():<10} {baseline_val:<10.3f} {test_val:<10.3f} {improvement:+12.3f}\")\n",
    "    print(\"-\" * 65)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves evaluation results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results to JSON\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_evaluation_metrics_to_json():\n",
    "    \"\"\"Save evaluation metrics comparison to JSON file\"\"\"\n",
    "    \n",
    "    # Prepare evaluation results dictionary\n",
    "    evaluation_results = {\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"test_dataset_path\": pred_store_path,\n",
    "            \"total_test_samples\": len(true_classes),\n",
    "            \"threshold\": threshold,\n",
    "            \"signal_classes\": tags,\n",
    "            \"model_paths\": {\n",
    "                \"baseline\": f\"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/{model_baseline_name}\",\n",
    "                \"test\": f\"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/{model_test_name}\"\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"overall_metrics\": {\n",
    "            \"baseline\": {\n",
    "                \"accuracy\": float(baseline_metrics['accuracy']),\n",
    "                \"f1_score\": float(baseline_metrics['f1']),\n",
    "                \"recall\": float(baseline_metrics['recall']),\n",
    "                \"precision\": float(baseline_metrics['precision'])\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"accuracy\": float(test_metrics['accuracy']),\n",
    "                \"f1_score\": float(test_metrics['f1']),\n",
    "                \"recall\": float(test_metrics['recall']),\n",
    "                \"precision\": float(test_metrics['precision'])\n",
    "            },\n",
    "            \"improvements\": {}\n",
    "        },\n",
    "        \n",
    "        \"per_class_metrics\": {\n",
    "            \"baseline\": {},\n",
    "            \"test\": {},\n",
    "            \"improvements\": {}\n",
    "        },\n",
    "        \n",
    "        \"sample_counts\": {}\n",
    "    }\n",
    "    \n",
    "    # Calculate overall improvements\n",
    "    metrics_to_compare = ['accuracy', 'f1', 'recall', 'precision']\n",
    "    for metric in metrics_to_compare:\n",
    "        baseline_val = baseline_metrics[metric.replace('_score', '')]\n",
    "        test_val = test_metrics[metric.replace('_score', '')]\n",
    "        improvement = test_val - baseline_val\n",
    "        \n",
    "        evaluation_results[\"overall_metrics\"][\"improvements\"][f\"{metric}_delta\"] = float(improvement)\n",
    "        evaluation_results[\"overall_metrics\"][\"improvements\"][f\"{metric}_percent_change\"] = float(improvement * 100)\n",
    "    \n",
    "    # Add per-class metrics and improvements\n",
    "    for i, signal_type in enumerate(tags):\n",
    "        sample_count = int((true_classes == i).sum())\n",
    "        evaluation_results[\"sample_counts\"][signal_type] = sample_count\n",
    "        \n",
    "        # Baseline per-class metrics\n",
    "        evaluation_results[\"per_class_metrics\"][\"baseline\"][signal_type] = {\n",
    "            \"f1_score\": float(baseline_metrics['f1_per_class'][i]),\n",
    "            \"recall\": float(baseline_metrics['recall_per_class'][i]),\n",
    "            \"precision\": float(baseline_metrics['precision_per_class'][i])\n",
    "        }\n",
    "        \n",
    "        # Test per-class metrics\n",
    "        evaluation_results[\"per_class_metrics\"][\"test\"][signal_type] = {\n",
    "            \"f1_score\": float(test_metrics['f1_per_class'][i]),\n",
    "            \"recall\": float(test_metrics['recall_per_class'][i]),\n",
    "            \"precision\": float(test_metrics['precision_per_class'][i])\n",
    "        }\n",
    "        \n",
    "        # Per-class improvements\n",
    "        evaluation_results[\"per_class_metrics\"][\"improvements\"][signal_type] = {\n",
    "            \"f1_delta\": float(test_metrics['f1_per_class'][i] - baseline_metrics['f1_per_class'][i]),\n",
    "            \"recall_delta\": float(test_metrics['recall_per_class'][i] - baseline_metrics['recall_per_class'][i]),\n",
    "            \"precision_delta\": float(test_metrics['precision_per_class'][i] - baseline_metrics['precision_per_class'][i])\n",
    "        }\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    filename = f\"evaluation_metrics_comparison_{timestamp}_{model_baseline_name}_{model_test_name}_{test_folder}.json\"\n",
    "    filepath = f\"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Results/{filename}\"\n",
    "    \n",
    "    # Save to JSON file\n",
    "    try:\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… EVALUATION METRICS SAVED TO JSON\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"File: {filepath}\")\n",
    "        print(f\"Size: {len(json.dumps(evaluation_results, indent=2))} characters\")\n",
    "        \n",
    "        print(f\"\\nFile contains:\")\n",
    "        print(f\"â€¢ Overall metrics for both models (accuracy, F1, recall, precision)\")\n",
    "        print(f\"â€¢ Per-class metrics for all {len(tags)} signal types\")\n",
    "        print(f\"â€¢ Improvement calculations (deltas and percent changes)\")\n",
    "        print(f\"â€¢ Sample counts and metadata\")\n",
    "        \n",
    "        print(f\"\\nKey improvements saved:\")\n",
    "        for metric in metrics_to_compare:\n",
    "            improvement = evaluation_results[\"overall_metrics\"][\"improvements\"][f\"{metric}_delta\"]\n",
    "            print(f\"â€¢ {metric.capitalize()}: {improvement:+.4f}\")\n",
    "            \n",
    "        return filepath, evaluation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving evaluation metrics: {str(e)}\")\n",
    "        return None, evaluation_results\n",
    "\n",
    "# Call the function to save metrics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAVING EVALUATION METRICS TO JSON\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "saved_metrics_path, metrics_data = save_evaluation_metrics_to_json()\n",
    "\n",
    "if saved_metrics_path:\n",
    "    print(f\"\\nâœ… Successfully saved evaluation metrics!\")\n",
    "    print(f\"ðŸ“ Location: {saved_metrics_path}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Failed to save evaluation metrics.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Plot calibration curves for each class\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# For multiclass, we need to create binary classification problems for each class\n",
    "y_true_binary = label_binarize(true_classes, classes=list(range(len(tags))))\n",
    "\n",
    "# Plot calibration curve for each class\n",
    "for i, class_name in enumerate(tags):\n",
    "    # Get the predicted probabilities for this class\n",
    "    y_prob_class = cm_pred_test[:, i]\n",
    "    \n",
    "    # Get the true binary labels for this class (1 if true class, 0 otherwise)\n",
    "    y_true_class = y_true_binary[:, i]\n",
    "    \n",
    "    # Calculate calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_true_class, y_prob_class, n_bins=10, strategy='uniform'\n",
    "    )\n",
    "    \n",
    "    # Plot calibration curve for this class\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "             label=f'{class_name} calibration curve', linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.title(f'Calibration Curve - {class_name}')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text showing calibration statistics\n",
    "    reliability = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "    plt.text(0.05, 0.95, f'Reliability: {reliability:.3f}', \n",
    "             transform=plt.gca().transAxes, fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate overall calibration statistics\n",
    "print(\"\\nCALIBRATION STATISTICS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "overall_reliability_scores = []\n",
    "for i, class_name in enumerate(tags):\n",
    "    y_prob_class = cm_pred_test[:, i]\n",
    "    y_true_class = y_true_binary[:, i]\n",
    "    \n",
    "    if len(np.unique(y_true_class)) > 1:  # Only calculate if class exists in true labels\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_true_class, y_prob_class, n_bins=10, strategy='uniform'\n",
    "        )\n",
    "        \n",
    "        # Calculate reliability (lower is better, 0 is perfect)\n",
    "        reliability = np.mean(np.abs(fraction_of_positives - mean_predicted_value))\n",
    "        overall_reliability_scores.append(reliability)\n",
    "        \n",
    "        # Calculate resolution (higher is better)\n",
    "        resolution = np.mean((fraction_of_positives - np.mean(y_true_class))**2)\n",
    "        \n",
    "        # Calculate Brier score (lower is better)\n",
    "        brier_score = np.mean((y_prob_class - y_true_class)**2)\n",
    "        \n",
    "        print(f\"{class_name:>8}: Reliability={reliability:.4f}, Resolution={resolution:.4f}, Brier Score={brier_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{class_name:>8}: No samples in test set\")\n",
    "\n",
    "if overall_reliability_scores:\n",
    "    mean_reliability = np.mean(overall_reliability_scores)\n",
    "    print(f\"\\nOverall Mean Reliability: {mean_reliability:.4f}\")\n",
    "    \n",
    "    # Interpret calibration quality\n",
    "    if mean_reliability < 0.05:\n",
    "        calibration_quality = \"Excellent\"\n",
    "    elif mean_reliability < 0.10:\n",
    "        calibration_quality = \"Good\"\n",
    "    elif mean_reliability < 0.15:\n",
    "        calibration_quality = \"Fair\"\n",
    "    else:\n",
    "        calibration_quality = \"Poor\"\n",
    "    \n",
    "    print(f\"Calibration Quality: {calibration_quality}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Reliability < 0.05: Excellent calibration\")\n",
    "    print(\"- Reliability < 0.10: Good calibration\") \n",
    "    print(\"- Reliability < 0.15: Fair calibration\")\n",
    "    print(\"- Reliability â‰¥ 0.15: Poor calibration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined calibration curve showing all classes\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot perfect calibration line\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect calibration')\n",
    "\n",
    "# Color palette for different classes\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "# Plot calibration curve for each class on the same plot\n",
    "for i, class_name in enumerate(tags):\n",
    "    y_prob_class = cm_pred[:, i]\n",
    "    y_true_class = y_true_binary[:, i]\n",
    "    \n",
    "    if len(np.unique(y_true_class)) > 1:  # Only plot if class exists in true labels\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "            y_true_class, y_prob_class, n_bins=10, strategy='uniform'\n",
    "        )\n",
    "        \n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, 's-', \n",
    "                 color=colors[i % len(colors)], linewidth=2, markersize=8,\n",
    "                 label=f'{class_name} (n={(y_true_class == 1).sum()})')\n",
    "\n",
    "plt.xlabel('Mean Predicted Probability', fontsize=12)\n",
    "plt.ylabel('Fraction of Positives', fontsize=12)\n",
    "plt.title('Calibration Curves - All Signal Classes', fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add overall statistics as text\n",
    "plt.text(0.02, 0.98, f'Overall Mean Reliability: {mean_reliability:.4f}\\nCalibration Quality: {calibration_quality}', \n",
    "         transform=plt.gca().transAxes, fontsize=11, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional calibration analysis: Expected Calibration Error (ECE)\n",
    "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error (ECE)\n",
    "    ECE measures the difference between accuracy and confidence across all predictions\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        # Find predictions in this bin\n",
    "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        \n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = y_true[in_bin].mean()\n",
    "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    \n",
    "    return ece\n",
    "\n",
    "# Calculate ECE for the overall model (using max probability predictions)\n",
    "max_probs = np.max(cm_pred, axis=1)\n",
    "correct_predictions = (predicted_classes == true_classes).astype(int)\n",
    "overall_ece = expected_calibration_error(correct_predictions, max_probs)\n",
    "\n",
    "print(f\"\\nEXPECTED CALIBRATION ERROR (ECE):\")\n",
    "print(f\"Overall ECE: {overall_ece:.4f}\")\n",
    "print(f\"ECE Interpretation:\")\n",
    "print(f\"- ECE < 0.05: Well calibrated\")\n",
    "print(f\"- ECE < 0.10: Reasonably calibrated\")\n",
    "print(f\"- ECE â‰¥ 0.10: Poorly calibrated\")\n",
    "\n",
    "# Show calibration by confidence bins\n",
    "print(f\"\\nCALIBRATION BY CONFIDENCE BINS:\")\n",
    "print(f\"{'Confidence Range':<20} {'Count':<8} {'Accuracy':<10} {'Avg Confidence':<15} {'Difference':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "n_bins = 10\n",
    "bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "for i in range(n_bins):\n",
    "    bin_lower = bin_boundaries[i]\n",
    "    bin_upper = bin_boundaries[i + 1]\n",
    "    \n",
    "    in_bin = (max_probs > bin_lower) & (max_probs <= bin_upper)\n",
    "    count_in_bin = in_bin.sum()\n",
    "    \n",
    "    if count_in_bin > 0:\n",
    "        accuracy_in_bin = correct_predictions[in_bin].mean()\n",
    "        avg_confidence_in_bin = max_probs[in_bin].mean()\n",
    "        difference = abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "        \n",
    "        print(f\"{bin_lower:.1f} - {bin_upper:.1f}         {count_in_bin:<8} {accuracy_in_bin:<10.3f} {avg_confidence_in_bin:<15.3f} {difference:<12.3f}\")\n",
    "    else:\n",
    "        print(f\"{bin_lower:.1f} - {bin_upper:.1f}         {count_in_bin:<8} {'N/A':<10} {'N/A':<15} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying unknowns using Mahalanobis Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOD Detector definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis Detector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "from sklearn.covariance import EmpiricalCovariance, LedoitWolf\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class MahalanobisOODDetector:\n",
    "    \"\"\"\n",
    "    Mahalanobis distance-based Out-of-Distribution detector for neural networks.\n",
    "    \n",
    "    Based on \"A Simple Unified Framework for Detecting Out-of-Distribution Samples \n",
    "    and Adversarial Attacks\" (Lee et al., 2018)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_tied_cov=True, cov_estimator='ledoit_wolf'):\n",
    "        \"\"\"\n",
    "        Initialize Mahalanobis OOD detector.\n",
    "        \n",
    "        Args:\n",
    "            use_tied_cov: If True, use pooled covariance across all classes\n",
    "            cov_estimator: 'empirical' or 'ledoit_wolf' for covariance estimation\n",
    "        \"\"\"\n",
    "        self.use_tied_cov = use_tied_cov\n",
    "        self.cov_estimator = cov_estimator\n",
    "        self.class_means = {}\n",
    "        self.class_covs = {}\n",
    "        self.pooled_cov = None\n",
    "        self.inv_covs = {}\n",
    "        self.fitted = False\n",
    "        self.known_classes = None\n",
    "        self.threshold = None\n",
    "        \n",
    "    def fit(self, features, labels, class_names, validation_features=None, validation_labels=None):\n",
    "        \"\"\"\n",
    "        Fit the Mahalanobis detector using training features.\n",
    "        \n",
    "        Args:\n",
    "            features: Training features from penultimate layer, shape (n_samples, n_features)\n",
    "            labels: Training labels, shape (n_samples,)\n",
    "            class_names: List of class names corresponding to label indices\n",
    "            validation_features: Optional validation features for threshold calculation\n",
    "            validation_labels: Optional validation labels for threshold calculation\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"FITTING MAHALANOBIS OOD DETECTOR\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.known_classes = class_names\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        print(f\"Known classes: {class_names}\")\n",
    "        print(f\"Feature dimension: {features.shape[1]}\")\n",
    "        print(f\"Total training samples: {features.shape[0]}\")\n",
    "        \n",
    "        # Calculate class-conditional means\n",
    "        print(f\"\\nCalculating class-conditional means...\")\n",
    "        for label in unique_labels:\n",
    "            if label < len(class_names):  # Only process known classes\n",
    "                class_mask = labels == label\n",
    "                class_features = features[class_mask]\n",
    "                self.class_means[label] = np.mean(class_features, axis=0)\n",
    "                \n",
    "                print(f\"  {class_names[label]}: {class_features.shape[0]} samples\")\n",
    "        \n",
    "        # Calculate covariance matrices\n",
    "        print(f\"\\nCalculating covariance matrices...\")\n",
    "        if self.use_tied_cov:\n",
    "            # Use pooled covariance across all known classes\n",
    "            all_known_features = []\n",
    "            \n",
    "            for label in unique_labels:\n",
    "                if label < len(class_names):  # Only known classes\n",
    "                    class_mask = labels == label\n",
    "                    class_features = features[class_mask]\n",
    "                    all_known_features.append(class_features)\n",
    "            \n",
    "            all_known_features = np.vstack(all_known_features)\n",
    "            \n",
    "            # Estimate pooled covariance\n",
    "            if self.cov_estimator == 'ledoit_wolf':\n",
    "                print(\"  Using Ledoit-Wolf covariance estimation...\")\n",
    "                cov_estimator = LedoitWolf()\n",
    "                cov_estimator.fit(all_known_features)\n",
    "                self.pooled_cov = cov_estimator.covariance_\n",
    "            else:\n",
    "                print(\"  Using empirical covariance estimation...\")\n",
    "                self.pooled_cov = np.cov(all_known_features.T)\n",
    "            \n",
    "            # Use same covariance for all classes\n",
    "            for label in unique_labels:\n",
    "                if label < len(class_names):\n",
    "                    self.class_covs[label] = self.pooled_cov\n",
    "            \n",
    "            print(f\"  Tied covariance matrix shape: {self.pooled_cov.shape}\")\n",
    "            \n",
    "        else:\n",
    "            # Use class-specific covariances\n",
    "            print(\"  Using class-specific covariances...\")\n",
    "            for label in unique_labels:\n",
    "                if label < len(class_names):\n",
    "                    class_mask = labels == label\n",
    "                    class_features = features[class_mask]\n",
    "                    \n",
    "                    if self.cov_estimator == 'ledoit_wolf':\n",
    "                        cov_estimator = LedoitWolf()\n",
    "                        cov_estimator.fit(class_features)\n",
    "                        self.class_covs[label] = cov_estimator.covariance_\n",
    "                    else:\n",
    "                        self.class_covs[label] = np.cov(class_features.T)\n",
    "        \n",
    "        # Compute inverse covariances for efficiency\n",
    "        print(f\"\\nComputing inverse covariance matrices...\")\n",
    "        for label in self.class_covs:\n",
    "            try:\n",
    "                self.inv_covs[label] = linalg.inv(self.class_covs[label])\n",
    "                print(f\"  {class_names[label]}: Successfully inverted\")\n",
    "            except linalg.LinAlgError:\n",
    "                # Use pseudo-inverse if matrix is singular\n",
    "                self.inv_covs[label] = linalg.pinv(self.class_covs[label])\n",
    "                print(f\"  {class_names[label]}: Using pseudo-inverse (singular matrix)\")\n",
    "        \n",
    "        # Calculate threshold using validation data if provided\n",
    "        if validation_features is not None and validation_labels is not None:\n",
    "            print(f\"\\nCalculating optimal threshold using validation data...\")\n",
    "            val_distances = self._compute_min_distances(validation_features)\n",
    "            self.threshold = np.percentile(val_distances, 95)\n",
    "            print(f\"  Threshold (95th percentile): {self.threshold:.4f}\")\n",
    "        else:\n",
    "            # Use training data for threshold calculation\n",
    "            print(f\"\\nCalculating threshold using training data...\")\n",
    "            train_distances = self._compute_min_distances(features)\n",
    "            self.threshold = np.percentile(train_distances, 95)\n",
    "            print(f\"  Threshold (95th percentile): {self.threshold:.4f}\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        print(f\"\\nâœ… Mahalanobis OOD detector fitted successfully!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_min_distances(self, features):\n",
    "        \"\"\"Compute minimum Mahalanobis distances across all classes\"\"\"\n",
    "        if not self.fitted and len(self.class_means) == 0:\n",
    "            raise ValueError(\"Detector must be fitted before computing distances\")\n",
    "        \n",
    "        n_samples = features.shape[0]\n",
    "        distances = {}\n",
    "        \n",
    "        # Compute Mahalanobis distance to each class\n",
    "        for label in self.class_means:\n",
    "            class_mean = self.class_means[label]\n",
    "            inv_cov = self.inv_covs[label]\n",
    "            \n",
    "            # Center the features\n",
    "            centered_features = features - class_mean\n",
    "            \n",
    "            # Compute Mahalanobis distance: sqrt((x-Î¼)^T Î£^(-1) (x-Î¼))\n",
    "            mahal_dist = np.sqrt(np.sum(centered_features @ inv_cov * centered_features, axis=1))\n",
    "            distances[label] = mahal_dist\n",
    "        \n",
    "        # Find minimum distance across all classes\n",
    "        distance_matrix = np.stack(list(distances.values()), axis=1)\n",
    "        min_distances = np.min(distance_matrix, axis=1)\n",
    "        \n",
    "        return min_distances\n",
    "    \n",
    "    def predict_ood(self, features, threshold=None):\n",
    "        \"\"\"\n",
    "        Predict out-of-distribution samples.\n",
    "        \n",
    "        Args:\n",
    "            features: Test features, shape (n_samples, n_features)\n",
    "            threshold: Mahalanobis distance threshold. If None, uses fitted threshold\n",
    "            \n",
    "        Returns:\n",
    "            ood_predictions: Boolean array, True for OOD samples\n",
    "            min_distances: Minimum Mahalanobis distances\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Detector must be fitted before making predictions\")\n",
    "        \n",
    "        min_distances = self._compute_min_distances(features)\n",
    "        \n",
    "        if threshold is None:\n",
    "            threshold = self.threshold\n",
    "        \n",
    "        ood_predictions = min_distances > threshold\n",
    "        \n",
    "        return ood_predictions, min_distances\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the fitted detector to file\"\"\"\n",
    "        detector_data = {\n",
    "            'use_tied_cov': self.use_tied_cov,\n",
    "            'cov_estimator': self.cov_estimator,\n",
    "            'class_means': {int(k): v.tolist() for k, v in self.class_means.items()},\n",
    "            'class_covs': {int(k): v.tolist() for k, v in self.class_covs.items()},\n",
    "            'pooled_cov': self.pooled_cov.tolist() if self.pooled_cov is not None else None,\n",
    "            'inv_covs': {int(k): v.tolist() for k, v in self.inv_covs.items()},\n",
    "            'fitted': self.fitted,\n",
    "            'known_classes': self.known_classes,\n",
    "            'threshold': float(self.threshold) if self.threshold is not None else None\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(detector_data, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… OOD detector saved to: {filepath}\")\n",
    "\n",
    "print(\"âœ… Mahalanobis OOD detector class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained OOD Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained OOD Detector from JSON\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_ood_detector_from_json(detector_path):\n",
    "    \"\"\"\n",
    "    Load a pretrained MahalanobisOODDetector from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        detector_path: Path to the JSON file containing detector parameters\n",
    "        \n",
    "    Returns:\n",
    "        detector: Loaded MahalanobisOODDetector instance\n",
    "    \"\"\"\n",
    "    print(f\"Loading OOD detector from: {detector_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(detector_path, 'r') as f:\n",
    "            detector_data = json.load(f)\n",
    "        \n",
    "        # Create detector instance\n",
    "        detector = MahalanobisOODDetector(\n",
    "            use_tied_cov=detector_data.get('use_tied_cov', True),\n",
    "            cov_estimator=detector_data.get('cov_estimator', 'ledoit_wolf')\n",
    "        )\n",
    "        \n",
    "        # Load pretrained parameters\n",
    "        detector.class_means = {int(k): np.array(v) for k, v in detector_data['class_means'].items()}\n",
    "        detector.class_covs = {int(k): np.array(v) for k, v in detector_data['class_covs'].items()}\n",
    "        detector.inv_covs = {int(k): np.array(v) for k, v in detector_data['inv_covs'].items()}\n",
    "        \n",
    "        if detector_data['pooled_cov'] is not None:\n",
    "            detector.pooled_cov = np.array(detector_data['pooled_cov'])\n",
    "        else:\n",
    "            detector.pooled_cov = None\n",
    "            \n",
    "        detector.fitted = detector_data['fitted']\n",
    "        detector.known_classes = detector_data['known_classes']\n",
    "        detector.threshold = detector_data['threshold']\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded OOD detector:\")\n",
    "        print(f\"   â€¢ Known classes: {detector.known_classes}\")\n",
    "        print(f\"   â€¢ Feature dimension: {list(detector.class_means.values())[0].shape[0]}\")\n",
    "        print(f\"   â€¢ Covariance estimator: {detector.cov_estimator}\")\n",
    "        print(f\"   â€¢ Tied covariance: {detector.use_tied_cov}\")\n",
    "        print(f\"   â€¢ Detection threshold: {detector.threshold:.4f}\")\n",
    "        \n",
    "        return detector\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ OOD detector file not found: {detector_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading OOD detector: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Load OOD Detector from JSON file\n",
    "ood_detector_pretained_folder = \"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/ood_detectors\"\n",
    "ood_detector_pretrained_filename = \"mahalanobis_ood_detector_jul25_rms_1_20250726_193658_flatten_layer.json\"\n",
    "ood_detector_pretrained_path = os.path.join(ood_detector_pretained_folder, ood_detector_pretrained_filename)\n",
    "ood_detector = load_ood_detector_from_json(ood_detector_pretrained_path)\n",
    "\n",
    "if ood_detector is not None:\n",
    "    print(f\"âœ… Primary OOD detector loaded successfully\")\n",
    "    print(f\"   â€¢ Model: {ood_detector.known_classes}\")\n",
    "else:\n",
    "    print(f\"âŒ No OOD detector available\")\n",
    "    print(f\"   â€¢ Cannot perform OOD detection\")\n",
    "    print(f\"   â€¢ Run OOD detector training in classifier_training.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for extracting feature from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract Features for OOD Detector Training\n",
    "\n",
    "def extract_features_from_model(model, X_data, layer_name=None):\n",
    "    \"\"\"\n",
    "    Extract features from a specific layer of the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        X_data: Input data\n",
    "        layer_name: Name of layer to extract from. If None, uses penultimate layer\n",
    "        \n",
    "    Returns:\n",
    "        features: Extracted features, shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    if layer_name is None:\n",
    "        # Use penultimate layer (before final classification layer)\n",
    "        feature_layer = model.layers[-8]\n",
    "    else:\n",
    "        feature_layer = model.get_layer(layer_name)\n",
    "    \n",
    "    print(f\"Extracting features from layer: {feature_layer.name}\")\n",
    "    print(f\"Layer output shape: {feature_layer.output_shape}\")\n",
    "    \n",
    "    # Create feature extraction model\n",
    "    feature_extractor = tf.keras.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=feature_layer.output\n",
    "    )\n",
    "\n",
    "    # print(f\"\\nFeature extractor layers:\")\n",
    "    # for i, layer in enumerate(feature_extractor.layers):\n",
    "    #     print(f\"  {i}: {layer.name} - {type(layer).__name__}\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor.predict(X_data, verbose=1)\n",
    "    \n",
    "    # Flatten if needed (for dense layers, features might be 2D)\n",
    "    if len(features.shape) > 2:\n",
    "        features = features.reshape(features.shape[0], -1)\n",
    "    \n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features for the baseline model and the test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXTRACTING FEATURES FOR OOD DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract features from baseline model\n",
    "print(\"\\nExtracting features from BASELINE MODEL...\")\n",
    "print(\"-\" * 50)\n",
    "baseline_test_features = extract_features_from_model(model_baseline, X_test)\n",
    "\n",
    "# Extract features from test model  \n",
    "print(\"\\nExtracting features from TEST MODEL...\")\n",
    "print(\"-\" * 50)\n",
    "test_test_features = extract_features_from_model(model_test, X_test)\n",
    "\n",
    "# Display feature extraction summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FEATURE EXTRACTION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Input data shape: {X_test.shape}\")\n",
    "print(f\"Baseline model features shape: {baseline_test_features.shape}\")\n",
    "print(f\"Test model features shape: {test_test_features.shape}\")\n",
    "\n",
    "# Verify feature dimensions match\n",
    "if baseline_test_features.shape[1] == test_test_features.shape[1]:\n",
    "    print(f\"âœ… Feature dimensions match: {baseline_test_features.shape[1]} features\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Feature dimensions differ:\")\n",
    "    print(f\"   Baseline: {baseline_test_features.shape[1]} features\")\n",
    "    print(f\"   Test: {test_test_features.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nâœ… Feature extraction completed successfully!\")\n",
    "print(f\"   â€¢ Ready for OOD detection analysis\")\n",
    "print(f\"   â€¢ Features extracted from penultimate dense layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = test_test_features.shape[1]\n",
    "num_cols = test_test_features.shape[0]\n",
    "for row in range(num_cols):\n",
    "    print(f\"{df_test['filename'][row][:15]}\", end=' ')\n",
    "for row in range(num_rows):\n",
    "    for col in range(test_test_features.shape[0]):\n",
    "        print(f\"{test_test_features[col, row]:<15.4f}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy Scoring Analysis for Feature Layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_energy_score_clipped(features):\n",
    "    \"\"\"\n",
    "    Compute negative log sum exp energy score for feature vectors.\n",
    "    However, clip the scores to avoid extreme values. \n",
    "    \"\"\"\n",
    "    # Negative Log Sum Exp energy: -log(sum(exp(x_i)))\n",
    "    # For numerical stability, use the log-sum-exp trick\n",
    "    max_vals = np.max(features, axis=1, keepdims=True)\n",
    "    log_sum_exp = max_vals.flatten() + np.log(np.sum(np.exp(features - max_vals), axis=1))\n",
    "    # Calculate 95th percentile for clipping\n",
    "    percentile_95 = np.percentile(log_sum_exp, 95)\n",
    "    # Clip the scores to avoid extreme values\n",
    "    energy_scores = -np.clip(log_sum_exp,a_max=percentile_95,a_min=None)\n",
    "    \n",
    "    return energy_scores\n",
    "\n",
    "def compute_energy_score(features):\n",
    "    \"\"\"\n",
    "    Compute negative log sum exp energy score for feature vectors.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature vectors, shape (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        energy_scores: Array of negative log sum exp energy scores for each sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # Negative Log Sum Exp energy: -log(sum(exp(x_i)))\n",
    "    # For numerical stability, use the log-sum-exp trick\n",
    "    max_vals = np.max(features, axis=1, keepdims=True)\n",
    "    # log(sum(exp(x_i))) = max + log(sum(exp(x_i - max)))\n",
    "    log_sum_exp = max_vals.flatten() + np.log(np.sum(np.exp(features - max_vals), axis=1))\n",
    "    energy_scores = -log_sum_exp\n",
    "    \n",
    "    return energy_scores\n",
    "\n",
    "def analyze_energy_scores(energy_scores, true_labels, model_name):\n",
    "    \"\"\"\n",
    "    Analyze negative log sum exp energy scores and their relationship to signal classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"NEGATIVE LOG SUM EXP ENERGY ANALYSIS - {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"Energy Score Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(energy_scores):.4f}\")\n",
    "    print(f\"  Std:  {np.std(energy_scores):.4f}\")\n",
    "    print(f\"  Min:  {np.min(energy_scores):.4f}\")\n",
    "    print(f\"  Max:  {np.max(energy_scores):.4f}\")\n",
    "    print(f\"  Range: {np.max(energy_scores) - np.min(energy_scores):.4f}\")\n",
    "    \n",
    "    # Per-class analysis\n",
    "    print(f\"\\nPer-Class Energy Analysis:\")\n",
    "    print(f\"{'Class':<10} {'Count':<8} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    class_stats = {}\n",
    "    signal_names = ['16QAM', '8CPSK', 'FM', 'UNKNOWN']\n",
    "    \n",
    "    for class_idx in range(len(signal_names)):\n",
    "        class_mask = true_labels == class_idx\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_energies = energy_scores[class_mask]\n",
    "            \n",
    "            stats_dict = {\n",
    "                'count': len(class_energies),\n",
    "                'mean': np.mean(class_energies),\n",
    "                'std': np.std(class_energies),\n",
    "                'min': np.min(class_energies),\n",
    "                'max': np.max(class_energies)\n",
    "            }\n",
    "            \n",
    "            class_stats[class_idx] = stats_dict\n",
    "            \n",
    "            print(f\"{signal_names[class_idx]:<10} {stats_dict['count']:<8} {stats_dict['mean']:<10.4f} \"\n",
    "                  f\"{stats_dict['std']:<10.4f} {stats_dict['min']:<10.4f} {stats_dict['max']:<10.4f}\")\n",
    "    \n",
    "    # Statistical tests between classes\n",
    "    print(f\"\\nStatistical Significance Tests (p-values):\")\n",
    "    print(f\"{'Class 1':<10} {'Class 2':<10} {'T-test':<12} {'Mann-Whitney':<15} {'Interpretation':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    class_indices = list(class_stats.keys())\n",
    "    for i in range(len(class_indices)):\n",
    "        for j in range(i+1, len(class_indices)):\n",
    "            class1_idx, class2_idx = class_indices[i], class_indices[j]\n",
    "            class1_energies = energy_scores[true_labels == class1_idx]\n",
    "            class2_energies = energy_scores[true_labels == class2_idx]\n",
    "            \n",
    "            if len(class1_energies) > 1 and len(class2_energies) > 1:\n",
    "                # T-test\n",
    "                t_stat, t_pval = stats.ttest_ind(class1_energies, class2_energies)\n",
    "                \n",
    "                # Mann-Whitney U test (non-parametric)\n",
    "                mw_stat, mw_pval = stats.mannwhitneyu(class1_energies, class2_energies, alternative='two-sided')\n",
    "                \n",
    "                class1_name = signal_names[class1_idx]\n",
    "                class2_name = signal_names[class2_idx]\n",
    "                \n",
    "                # Interpret significance\n",
    "                if t_pval < 0.001 and mw_pval < 0.001:\n",
    "                    interpretation = \"Highly Significant\"\n",
    "                elif t_pval < 0.01 and mw_pval < 0.01:\n",
    "                    interpretation = \"Very Significant\"\n",
    "                elif t_pval < 0.05 or mw_pval < 0.05:\n",
    "                    interpretation = \"Significant\"\n",
    "                else:\n",
    "                    interpretation = \"Not Significant\"\n",
    "                \n",
    "                print(f\"{class1_name:<10} {class2_name:<10} {t_pval:<12.6f} {mw_pval:<15.6f} {interpretation:<20}\")\n",
    "    \n",
    "    return class_stats\n",
    "\n",
    "def energy_based_classification(energy_scores, true_labels, signal_names):\n",
    "    \"\"\"\n",
    "    Attempt classification based on negative log sum exp energy score thresholds.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nNEGATIVE LOG SUM EXP ENERGY-BASED CLASSIFICATION:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate class-based energy thresholds\n",
    "    class_thresholds = {}\n",
    "    for class_idx in range(len(signal_names)):\n",
    "        class_mask = true_labels == class_idx\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_energies = energy_scores[class_mask]\n",
    "            # Use mean Â± std as classification boundaries\n",
    "            class_thresholds[class_idx] = {\n",
    "                'mean': np.mean(class_energies),\n",
    "                'std': np.std(class_energies),\n",
    "                'lower': np.mean(class_energies) - np.std(class_energies),\n",
    "                'upper': np.mean(class_energies) + np.std(class_energies)\n",
    "            }\n",
    "    \n",
    "    # Simple threshold-based classification\n",
    "    # Assign each sample to class with closest mean energy\n",
    "    predicted_labels = np.zeros_like(true_labels)\n",
    "    \n",
    "    for i, energy in enumerate(energy_scores):\n",
    "        min_distance = float('inf')\n",
    "        best_class = 0\n",
    "        \n",
    "        for class_idx, threshold_info in class_thresholds.items():\n",
    "            distance = abs(energy - threshold_info['mean'])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_class = class_idx\n",
    "        \n",
    "        predicted_labels[i] = best_class\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    print(f\"Energy-based classification accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Show classification report\n",
    "    if len(np.unique(true_labels)) > 1:\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        report = classification_report(true_labels, predicted_labels, \n",
    "                                     target_names=signal_names, zero_division=0)\n",
    "        print(report)\n",
    "    \n",
    "    return predicted_labels, class_thresholds\n",
    "\n",
    "def visualize_energy_scores(energy_scores_dict, true_labels, signal_names):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for negative log sum exp energy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.suptitle('Negative Log Sum Exp Energy Score Analysis', fontsize=16, y=0.98)\n",
    "    \n",
    "    models = list(energy_scores_dict.keys())\n",
    "    ######################## # Plotting Energy Scores ########################\n",
    "    # # Plot 1-2: Energy score distributions by model\n",
    "    # for model_idx, model_name in enumerate(models):\n",
    "    #     plt.subplot(3, 4, model_idx + 1)\n",
    "        \n",
    "    #     energy_scores = energy_scores_dict[model_name]\n",
    "    #     plt.hist(energy_scores, bins=30, alpha=0.7, density=True, color=['blue', 'red'][model_idx], \n",
    "    #             edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "    #     plt.xlabel('Negative Log Sum Exp Energy Score')\n",
    "    #     plt.ylabel('Density')\n",
    "    #     plt.title(f'{model_name} - Energy Distribution')\n",
    "    #     plt.grid(True, alpha=0.3)\n",
    "        \n",
    "    #     # Add statistics text\n",
    "    #     mean_energy = np.mean(energy_scores)\n",
    "    #     std_energy = np.std(energy_scores)\n",
    "    #     plt.text(0.05, 0.95, f'Mean: {mean_energy:.3f}\\nStd: {std_energy:.3f}', \n",
    "    #             transform=plt.gca().transAxes, fontsize=10,\n",
    "    #             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "    #             verticalalignment='top')\n",
    "    \n",
    "    # # Plot 3-4: Energy scores by signal class\n",
    "    # for model_idx, model_name in enumerate(models):\n",
    "    #     plt.subplot(3, 4, model_idx + 3)\n",
    "        \n",
    "    #     energy_scores = energy_scores_dict[model_name]\n",
    "        \n",
    "    #     class_data = []\n",
    "    #     class_labels = []\n",
    "    #     colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "        \n",
    "    #     for class_idx, signal_name in enumerate(signal_names):\n",
    "    #         class_mask = true_labels == class_idx\n",
    "    #         if np.sum(class_mask) > 0:\n",
    "    #             class_data.append(energy_scores[class_mask])\n",
    "    #             class_labels.append(signal_name)\n",
    "        \n",
    "    #     if class_data:\n",
    "    #         box_plot = plt.boxplot(class_data, labels=class_labels, patch_artist=True)\n",
    "            \n",
    "    #         for patch, color in zip(box_plot['boxes'], colors):\n",
    "    #             patch.set_facecolor(color)\n",
    "        \n",
    "    #     plt.ylabel('Negative Log Sum Exp Energy')\n",
    "    #     plt.title(f'{model_name} - Energy by Class')\n",
    "    #     plt.xticks(rotation=45)\n",
    "    #     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # # Plot 5: Model comparison\n",
    "    # if len(models) == 2:\n",
    "    #     plt.subplot(3, 4, 5)\n",
    "        \n",
    "    #     model_data = []\n",
    "    #     model_labels = []\n",
    "        \n",
    "    #     for model_name in models:\n",
    "    #         energy_scores = energy_scores_dict[model_name]\n",
    "    #         model_data.append(energy_scores)\n",
    "    #         model_labels.append(model_name)\n",
    "        \n",
    "    #     box_plot = plt.boxplot(model_data, labels=model_labels, patch_artist=True)\n",
    "    #     colors = ['lightcyan', 'lightpink']\n",
    "        \n",
    "    #     for patch, color in zip(box_plot['boxes'], colors):\n",
    "    #         patch.set_facecolor(color)\n",
    "        \n",
    "    #     plt.ylabel('Negative Log Sum Exp Energy')\n",
    "    #     plt.title('Model Comparison')\n",
    "    #     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # # Plot 6: Energy score scatter plot (if two models available)\n",
    "    # if len(models) == 2:\n",
    "    #     plt.subplot(3, 4, 6)\n",
    "        \n",
    "    #     baseline_energy = energy_scores_dict[models[0]]\n",
    "    #     test_energy = energy_scores_dict[models[1]]\n",
    "        \n",
    "    #     # Color by signal class\n",
    "    #     for class_idx, signal_name in enumerate(signal_names):\n",
    "    #         class_mask = true_labels == class_idx\n",
    "    #         if np.sum(class_mask) > 0:\n",
    "    #             plt.scatter(baseline_energy[class_mask], test_energy[class_mask], \n",
    "    #                       label=signal_name, alpha=0.7, s=30)\n",
    "        \n",
    "    #     plt.xlabel(f'{models[0]} Energy Score')\n",
    "    #     plt.ylabel(f'{models[1]} Energy Score')\n",
    "    #     plt.title('Energy Score Correlation\\n(Between Models)')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True, alpha=0.3)\n",
    "        \n",
    "    #     # Add correlation coefficient\n",
    "    #     correlation = np.corrcoef(baseline_energy, test_energy)[0, 1]\n",
    "    #     plt.text(0.05, 0.95, f'Correlation: {correlation:.4f}', \n",
    "    #             transform=plt.gca().transAxes, fontsize=12,\n",
    "    #             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # # Plot 7-8: Class separation visualization for each model\n",
    "    # for model_idx, model_name in enumerate(models):\n",
    "    #     plt.subplot(3, 4, model_idx + 7)\n",
    "        \n",
    "    #     energy_scores = energy_scores_dict[model_name]\n",
    "        \n",
    "    #     # Create overlaid histograms for each class\n",
    "    #     for class_idx, signal_name in enumerate(signal_names):\n",
    "    #         class_mask = true_labels == class_idx\n",
    "    #         if np.sum(class_mask) > 0:\n",
    "    #             class_energies = energy_scores[class_mask]\n",
    "    #             plt.hist(class_energies, bins=20, alpha=0.6, label=signal_name, density=True)\n",
    "        \n",
    "    #     plt.xlabel('Negative Log Sum Exp Energy')\n",
    "    #     plt.ylabel('Density')\n",
    "    #     plt.title(f'{model_name} - Class Overlap')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # # Plot 9: Summary statistics\n",
    "    # plt.subplot(3, 4, 9)\n",
    "    # plt.axis('off')\n",
    "    #################################################################################################\n",
    "    # Plot 7-8: Class separation visualization for each model\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    for model_idx, model_name in enumerate(models):\n",
    "        \n",
    "        energy_scores = energy_scores_dict[model_name]\n",
    "        plt.subplot(2, 1, model_idx + 1)\n",
    "        # Create overlaid histograms for each class\n",
    "        for class_idx, signal_name in enumerate(signal_names):\n",
    "            class_mask = true_labels == class_idx\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_energies = energy_scores[class_mask]\n",
    "                plt.hist(class_energies, bins=20, alpha=0.6, label=signal_name, density=True)\n",
    "    plt.legend()\n",
    "    \n",
    "    summary_text = \"Negative Log Sum Exp Energy Summary:\\n\\n\"\n",
    "    \n",
    "    for model_name in models:\n",
    "        energy_scores = energy_scores_dict[model_name]\n",
    "        summary_text += f\"{model_name}:\\n\"\n",
    "        summary_text += f\"  Mean: {np.mean(energy_scores):.4f}\\n\"\n",
    "        summary_text += f\"  Std:  {np.std(energy_scores):.4f}\\n\"\n",
    "        summary_text += f\"  Range: {np.max(energy_scores) - np.min(energy_scores):.4f}\\n\\n\"\n",
    "    \n",
    "    if len(models) == 2:\n",
    "        correlation = np.corrcoef(energy_scores_dict[models[0]], energy_scores_dict[models[1]])[0, 1]\n",
    "        summary_text += f\"Model Correlation: {correlation:.4f}\\n\"\n",
    "        \n",
    "        if correlation > 0.8:\n",
    "            summary_text += \"âœ… High correlation - similar patterns\\n\"\n",
    "        elif correlation > 0.5:\n",
    "            summary_text += \"âš ï¸ Moderate correlation\\n\"\n",
    "        else:\n",
    "            summary_text += \"âŒ Low correlation - different patterns\\n\"\n",
    "    \n",
    "    summary_text += \"\\nInterpretation:\\n\"\n",
    "    summary_text += \"â€¢ Lower values = Higher energy\\n\"\n",
    "    summary_text += \"â€¢ Higher values = Lower energy\\n\"\n",
    "    summary_text += \"â€¢ Good separation between classes\\n\"\n",
    "    summary_text += \"  indicates effective features\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, summary_text, transform=plt.gca().transAxes, \n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Main Energy Scoring Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"NEGATIVE LOG SUM EXP ENERGY SCORING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "signal_names = ['16QAM', '8CPSK', 'FM', 'UNKNOWN']\n",
    "\n",
    "# Storage for results\n",
    "energy_results = {}\n",
    "\n",
    "# Analyze baseline model features\n",
    "if 'baseline_test_features' in locals() and baseline_test_features is not None:\n",
    "    print(\"ðŸ” Analyzing BASELINE MODEL negative log sum exp energy scores...\")\n",
    "    \n",
    "    baseline_energy_scores = compute_energy_score_clipped(baseline_test_features)\n",
    "    energy_results['baseline'] = baseline_energy_scores\n",
    "    \n",
    "    # Analyze energy scores\n",
    "    baseline_stats = analyze_energy_scores(baseline_energy_scores, true_classes, \"BASELINE MODEL\")\n",
    "    \n",
    "    # Attempt energy-based classification\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BASELINE MODEL - ENERGY-BASED CLASSIFICATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    baseline_predictions, baseline_thresholds = energy_based_classification(\n",
    "        baseline_energy_scores, true_classes, signal_names\n",
    "    )\n",
    "\n",
    "# Analyze test model features\n",
    "if 'test_test_features' in locals() and test_test_features is not None:\n",
    "    print(\"\\nðŸ” Analyzing TEST MODEL negative log sum exp energy scores...\")\n",
    "    \n",
    "    test_energy_scores = compute_energy_score_clipped(test_test_features)\n",
    "    energy_results['test'] = test_energy_scores\n",
    "    \n",
    "    # Analyze energy scores\n",
    "    test_stats = analyze_energy_scores(test_energy_scores, true_classes, \"TEST MODEL\")\n",
    "    \n",
    "    # Attempt energy-based classification\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TEST MODEL - ENERGY-BASED CLASSIFICATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    test_predictions, test_thresholds = energy_based_classification(\n",
    "        test_energy_scores, true_classes, signal_names\n",
    "    )\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "if energy_results:\n",
    "    print(f\"\\nðŸ“Š Creating negative log sum exp energy score visualizations...\")\n",
    "    visualize_energy_scores(energy_results, true_classes, signal_names)\n",
    "\n",
    "# Compare models if both available\n",
    "if len(energy_results) == 2:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"NEGATIVE LOG SUM EXP ENERGY COMPARISON BETWEEN MODELS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    baseline_mean = np.mean(energy_results['baseline'])\n",
    "    test_mean = np.mean(energy_results['test'])\n",
    "    \n",
    "    print(f\"MEAN ENERGY SCORES:\")\n",
    "    print(f\"  Baseline: {baseline_mean:.4f}\")\n",
    "    print(f\"  Test:     {test_mean:.4f}\")\n",
    "    print(f\"  Difference: {test_mean - baseline_mean:+.4f}\")\n",
    "    \n",
    "    # Correlation between model energy scores\n",
    "    correlation = np.corrcoef(energy_results['baseline'], energy_results['test'])[0, 1]\n",
    "    print(f\"\\nEnergy score correlation between models: {correlation:.4f}\")\n",
    "    \n",
    "    if correlation > 0.8:\n",
    "        print(\"âœ… High correlation - models show similar energy patterns\")\n",
    "    elif correlation > 0.5:\n",
    "        print(\"âš ï¸  Moderate correlation - some differences in energy patterns\")\n",
    "    else:\n",
    "        print(\"âŒ Low correlation - significant differences in energy patterns\")\n",
    "    \n",
    "    # Statistical test between models\n",
    "    from scipy.stats import ttest_rel\n",
    "    t_stat, p_value = ttest_rel(energy_results['baseline'], energy_results['test'])\n",
    "    print(f\"\\nPaired t-test between models: t={t_stat:.4f}, p={p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"âœ… Significant difference in energy patterns between models\")\n",
    "    else:\n",
    "        print(\"âŒ No significant difference in energy patterns between models\")\n",
    "\n",
    "print(f\"\\nâœ… Negative log sum exp energy scoring analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_energy_score_threshold(energy_scores, percentile=95):\n",
    "    \"\"\"\n",
    "    Compute energy score threshold based on a given percentile.\n",
    "    \n",
    "    Args:\n",
    "        energy_scores: Array of energy scores\n",
    "        percentile: Percentile to use for thresholding (default 95)\n",
    "    \n",
    "    Returns:\n",
    "        threshold: Energy score threshold value\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(energy_scores, percentile)\n",
    "    print(f\"Energy score threshold (percentile {percentile}): {threshold:.4f}\")\n",
    "    return threshold\n",
    "\n",
    "# count number of unknowns above threshold\n",
    "def count_unknowns_above_threshold(energy_scores, threshold):\n",
    "    return np.sum(energy_scores > threshold)\n",
    "\n",
    "def count_below_threshold(energy_scores, threshold):\n",
    "    return np.sum(energy_scores < threshold)\n",
    "\n",
    "percentile = 70\n",
    "baseline_threshold = compute_energy_score_threshold(baseline_energy_scores, percentile=percentile)\n",
    "test_threshold = compute_energy_score_threshold(test_energy_scores, percentile=percentile)\n",
    "unknowns_mask = true_classes == 3  # Assuming class 3 is 'UNKNOWN'\n",
    "baseline_unknowns_energy_score = baseline_energy_scores[unknowns_mask]\n",
    "test_unknowns_energy_score = test_energy_scores[unknowns_mask]\n",
    "\n",
    "# Count unknowns above thresholds (True negatives)\n",
    "unknowns_above_baseline = count_unknowns_above_threshold(baseline_unknowns_energy_score, baseline_threshold)\n",
    "unknowns_above_test = count_unknowns_above_threshold(test_unknowns_energy_score, test_threshold)\n",
    "total_unknowns = np.sum(true_classes == 3)  # Total number of unknown samples\n",
    "total_knowns = np.sum(true_classes != 3)  # Total number of known samples\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"UNKNOWN SAMPLES ABOVE ENERGY SCORE THRESHOLD (baseline): {unknowns_above_baseline}\")\n",
    "print(f\"Percentage of unknowns above baseline threshold (baseline): {unknowns_above_baseline / total_unknowns * 100:.2f}%\")\n",
    "\n",
    "print(f\"UNKNOWN SAMPLES ABOVE ENERGY SCORE THRESHOLD (test): {unknowns_above_test}\")\n",
    "print(f\"Percentage of unknowns above test threshold (test): {unknowns_above_test / total_unknowns * 100:.2f}%\")\n",
    "\n",
    "# Count knowns above thresholds (False negatives)\n",
    "knowns_mask = true_classes != 3\n",
    "baseline_knowns_energy_score = baseline_energy_scores[knowns_mask]\n",
    "test_knowns_energy_score = test_energy_scores[knowns_mask]\n",
    "baseline_knowns_above_threshold = count_unknowns_above_threshold(baseline_knowns_energy_score, baseline_threshold)\n",
    "test_knowns_above_threshold = count_unknowns_above_threshold(test_knowns_energy_score, test_threshold)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"KNOWN SAMPLES ABOVE ENERGY SCORE THRESHOLD (baseline): {baseline_knowns_above_threshold}\")\n",
    "print(f\"Percentage of knowns above baseline threshold (baseline): {baseline_knowns_above_threshold / total_knowns * 100:.2f}%\")\n",
    "\n",
    "print(f\"KNOWN SAMPLES ABOVE ENERGY SCORE THRESHOLD (test): {test_knowns_above_threshold}\")\n",
    "print(f\"Percentage of knowns above test threshold (test): {test_knowns_above_threshold / total_knowns * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(num_cols):\n",
    "    print(f\"{df_test['filename'][row][:15]}\", end=' ')\n",
    "print()\n",
    "sum_features = np.max(test_test_features, axis=1)\n",
    "for feature in sum_features:\n",
    "    print(f\"{feature:<15.4f}\", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OOD Detection on Test Data\n",
    "\n",
    "if ood_detector is not None and baseline_test_features is not None:\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"PERFORMING OUT-OF-DISTRIBUTION DETECTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    def evaluate_ood_detection(detector, features, true_labels, model_name):\n",
    "        \"\"\"Evaluate OOD detection performance on test data\"\"\"\n",
    "        \n",
    "        print(f\"\\n{model_name} OOD Detection:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Predict OOD samples\n",
    "        ood_predictions, distances = detector.predict_ood(features)\n",
    "        \n",
    "        # Create ground truth OOD labels (True if UNKNOWN class)\n",
    "        unknown_class_idx = signal_tags['UNKNOWN']\n",
    "        true_ood = true_labels == unknown_class_idx\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = np.sum(ood_predictions & true_ood)      # True positives (correctly detected OOD)\n",
    "        fp = np.sum(ood_predictions & ~true_ood)     # False positives (known classified as OOD)\n",
    "        tn = np.sum(~ood_predictions & ~true_ood)    # True negatives (known correctly as known)\n",
    "        fn = np.sum(~ood_predictions & true_ood)     # False negatives (OOD classified as known)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        tpr = recall  # True Positive Rate (same as recall)\n",
    "        \n",
    "        print(f\"  Total test samples: {len(true_labels)}\")\n",
    "        print(f\"  True OOD samples: {np.sum(true_ood)}\")\n",
    "        print(f\"  Predicted OOD samples: {np.sum(ood_predictions)}\")\n",
    "        print(f\"  Threshold: {detector.threshold:.4f}\")\n",
    "        \n",
    "        print(f\"\\n  Confusion Matrix:\")\n",
    "        print(f\"    True Positives (OODâ†’OOD):     {tp}\")\n",
    "        print(f\"    False Positives (Knownâ†’OOD):  {fp}\")\n",
    "        print(f\"    True Negatives (Knownâ†’Known): {tn}\")\n",
    "        print(f\"    False Negatives (OODâ†’Known):  {fn}\")\n",
    "        \n",
    "        print(f\"\\n  Performance Metrics:\")\n",
    "        print(f\"    Precision: {precision:.4f}\")\n",
    "        print(f\"    Recall:    {recall:.4f}\")\n",
    "        print(f\"    F1-Score:  {f1_score:.4f}\")\n",
    "        print(f\"    Accuracy:  {accuracy:.4f}\")\n",
    "        print(f\"    FPR:       {fpr:.4f}\")\n",
    "        print(f\"    TPR:       {tpr:.4f}\")\n",
    "        \n",
    "        # Distance statistics\n",
    "        print(f\"\\n  Distance Statistics:\")\n",
    "        if np.sum(~true_ood) > 0:\n",
    "            known_distances = distances[~true_ood]\n",
    "            print(f\"    Known samples - Mean: {np.mean(known_distances):.4f}, Std: {np.std(known_distances):.4f}\")\n",
    "        \n",
    "        if np.sum(true_ood) > 0:\n",
    "            ood_distances = distances[true_ood]\n",
    "            print(\"     Length of OOD samples:\", len(ood_distances))\n",
    "            print(ood_distances)\n",
    "            print(f\"    OOD samples - Mean: {np.mean(ood_distances):.4f}, Std: {np.std(ood_distances):.4f}\")\n",
    "        \n",
    "        # Per-class analysis for known samples\n",
    "        print(f\"\\n  Per-Class False Positive Analysis:\")\n",
    "        for i, class_name in enumerate(tags[:-1]):  # Exclude UNKNOWN class\n",
    "            class_mask = (true_labels == i)\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_ood_predictions = ood_predictions[class_mask]\n",
    "                class_distances = distances[class_mask]\n",
    "                fp_rate = np.mean(class_ood_predictions)\n",
    "                mean_dist = np.mean(class_distances)\n",
    "                \n",
    "                print(f\"    {class_name}: FP Rate={fp_rate:.3f}, Mean Distance={mean_dist:.3f}, Samples={np.sum(class_mask)}\")\n",
    "        \n",
    "        return {\n",
    "            'ood_predictions': ood_predictions,\n",
    "            'distances': distances,\n",
    "            'true_ood': true_ood,\n",
    "            'metrics': {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1_score,\n",
    "                'accuracy': accuracy,\n",
    "                'fpr': fpr,\n",
    "                'tpr': tpr,\n",
    "                'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Evaluate OOD detection for both models\n",
    "    baseline_ood_results = evaluate_ood_detection(\n",
    "        ood_detector, baseline_test_features, true_classes, \"BASELINE MODEL\"\n",
    "    )\n",
    "    \n",
    "    test_ood_results = evaluate_ood_detection(\n",
    "        ood_detector, test_test_features, true_classes, \"TEST MODEL\"\n",
    "    )\n",
    "    \n",
    "    # Compare OOD detection performance\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"OOD DETECTION COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Metric':<15} {'Baseline':<12} {'Test Model':<12} {'Improvement':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    comparison_metrics = ['precision', 'recall', 'f1_score', 'accuracy', 'fpr']\n",
    "    for metric in comparison_metrics:\n",
    "        baseline_val = baseline_ood_results['metrics'][metric]\n",
    "        test_val = test_ood_results['metrics'][metric]\n",
    "        improvement = test_val - baseline_val\n",
    "        print(f\"{metric.upper():<15} {baseline_val:<12.4f} {test_val:<12.4f} {improvement:+12.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ… OOD detection evaluation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Skipping OOD detection - detector or features not available\")\n",
    "    baseline_ood_results = None\n",
    "    test_ood_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOD Detection Results\n",
    "\n",
    "if baseline_ood_results is not None and test_ood_results is not None:\n",
    "    \n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.suptitle(f'Out-of-Distribution Detection Analysis for {test_folder}', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    models_data = [\n",
    "        (\"Baseline\", baseline_ood_results, baseline_test_features),\n",
    "        (\"Test Model\", test_ood_results, test_test_features)\n",
    "    ]\n",
    "    \n",
    "    for idx, (model_name, ood_results, features) in enumerate(models_data):\n",
    "        base_row = idx * 2\n",
    "        \n",
    "        # 1. Distance distribution comparison (Known vs OOD)\n",
    "        plt.subplot(3, 3, idx + 1)\n",
    "        \n",
    "        true_ood = ood_results['true_ood']\n",
    "        distances = ood_results['distances']\n",
    "        threshold = ood_detector.threshold\n",
    "        \n",
    "        # Plot histograms for known vs OOD samples\n",
    "        known_distances = distances[~true_ood]\n",
    "        if np.sum(true_ood) > 0:\n",
    "            ood_distances = distances[true_ood]\n",
    "            plt.hist(ood_distances, bins=20, alpha=0.6, label='True OOD', color='red', density=True)\n",
    "        \n",
    "        plt.hist(known_distances, bins=20, alpha=0.6, label='Known', color='blue', density=True)\n",
    "        plt.axvline(threshold, color='black', linestyle='--', linewidth=2, \n",
    "                    label=f'Threshold={threshold:.3f}')\n",
    "        \n",
    "        plt.xlabel('Mahalanobis Distance')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'{model_name}\\nDistance Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Per-class distance box plots\n",
    "        plt.subplot(3, 3, idx + 3 + 1)\n",
    "        \n",
    "        class_distances_list = []\n",
    "        class_labels_list = []\n",
    "        \n",
    "        for i, class_name in enumerate(tags):\n",
    "            class_mask = true_classes == i\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_distances_list.append(distances[class_mask])\n",
    "                class_labels_list.append(class_name)\n",
    "        \n",
    "        if class_distances_list:\n",
    "            box_plot = plt.boxplot(class_distances_list, labels=class_labels_list, patch_artist=True)\n",
    "            \n",
    "            # Color boxes differently for known vs unknown\n",
    "            colors = ['lightblue', 'lightblue', 'lightblue', 'lightcoral']\n",
    "            for patch, color in zip(box_plot['boxes'], colors[:len(box_plot['boxes'])]):\n",
    "                patch.set_facecolor(color)\n",
    "            \n",
    "            plt.axhline(threshold, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Threshold={threshold:.3f}')\n",
    "            plt.ylabel('Mahalanobis Distance')\n",
    "            plt.title(f'{model_name}\\nDistance by Class')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "\n",
    "    \n",
    "    # 6. Confusion matrices for OOD detection\n",
    "    plt.subplot(3, 3, 7)\n",
    "    baseline_cm_ood = np.array([\n",
    "        [baseline_ood_results['metrics']['tn'], baseline_ood_results['metrics']['fp']],\n",
    "        [baseline_ood_results['metrics']['fn'], baseline_ood_results['metrics']['tp']]\n",
    "    ])\n",
    "    sns.heatmap(baseline_cm_ood, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Predicted Known', 'Predicted OOD'],\n",
    "               yticklabels=['True Known', 'True OOD'])\n",
    "    plt.title('Baseline - OOD Confusion Matrix')\n",
    "    \n",
    "    plt.subplot(3, 3, 8)\n",
    "    test_cm_ood = np.array([\n",
    "        [test_ood_results['metrics']['tn'], test_ood_results['metrics']['fp']],\n",
    "        [test_ood_results['metrics']['fn'], test_ood_results['metrics']['tp']]\n",
    "    ])\n",
    "    sns.heatmap(test_cm_ood, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Predicted Known', 'Predicted OOD'],\n",
    "               yticklabels=['True Known', 'True OOD'])\n",
    "    plt.title('Test Model - OOD Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        # 5. Overall comparison metrics\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, 3, 6)\n",
    "    comparison_text = f\"\"\"\n",
    "OOD Detection Performance Comparison:\n",
    "\n",
    "Baseline Model:\n",
    "â€¢ Precision: {baseline_ood_results['metrics']['precision']:.4f}\n",
    "â€¢ Recall: {baseline_ood_results['metrics']['recall']:.4f}\n",
    "â€¢ F1-Score: {baseline_ood_results['metrics']['f1_score']:.4f}\n",
    "â€¢ FPR: {baseline_ood_results['metrics']['fpr']:.4f}\n",
    "\n",
    "Test Model:\n",
    "â€¢ Precision: {test_ood_results['metrics']['precision']:.4f}\n",
    "â€¢ Recall: {test_ood_results['metrics']['recall']:.4f}\n",
    "â€¢ F1-Score: {test_ood_results['metrics']['f1_score']:.4f}\n",
    "â€¢ FPR: {test_ood_results['metrics']['fpr']:.4f}\n",
    "\n",
    "Improvements:\n",
    "â€¢ Precision: {test_ood_results['metrics']['precision'] - baseline_ood_results['metrics']['precision']:+.4f}\n",
    "â€¢ Recall: {test_ood_results['metrics']['recall'] - baseline_ood_results['metrics']['recall']:+.4f}\n",
    "â€¢ F1-Score: {test_ood_results['metrics']['f1_score'] - baseline_ood_results['metrics']['f1_score']:+.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, comparison_text, transform=plt.gca().transAxes, \n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ðŸ“Š OOD detection visualization completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"ðŸ“Š Skipping OOD detection visualization - results not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOD Detection and Hybrid Classification Results\n",
    "\n",
    "if baseline_ood_results is not None and test_ood_results is not None and hybrid_results is not None:\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"SAVING OOD DETECTION & HYBRID CLASSIFICATION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = 'results'\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "        print(f\"âœ… Created results directory: {results_dir}\")\n",
    "    \n",
    "    # Prepare comprehensive results dictionary\n",
    "    prediction_results = {\n",
    "        'experiment_info': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'signal_types': tags,\n",
    "            'test_samples': int(len(true_classes)),\n",
    "            'baseline_model_path': 'models/baseline_resnet_signal_classifier.h5',\n",
    "            'test_model_path': 'models/test_resnet_signal_classifier.h5'\n",
    "        },\n",
    "        'ood_detection': {\n",
    "            'baseline_model': {\n",
    "                'detector_file': baseline_ood_results.get('detector_file', 'Unknown'),\n",
    "                'threshold': float(baseline_ood_results['threshold']),\n",
    "                'accuracy': float(baseline_ood_results['accuracy']),\n",
    "                'precision': float(baseline_ood_results['precision']),\n",
    "                'recall': float(baseline_ood_results['recall']),\n",
    "                'f1_score': float(baseline_ood_results['f1_score']),\n",
    "                'auc_roc': float(baseline_ood_results['auc_roc']),\n",
    "                'total_ood_detected': int(np.sum(baseline_ood_results['ood_predictions'])),\n",
    "                'per_class_stats': baseline_ood_results['per_class_stats']\n",
    "            },\n",
    "            'test_model': {\n",
    "                'detector_file': test_ood_results.get('detector_file', 'Unknown'),\n",
    "                'threshold': float(test_ood_results['threshold']),\n",
    "                'accuracy': float(test_ood_results['accuracy']),\n",
    "                'precision': float(test_ood_results['precision']),\n",
    "                'recall': float(test_ood_results['recall']),\n",
    "                'f1_score': float(test_ood_results['f1_score']),\n",
    "                'auc_roc': float(test_ood_results['auc_roc']),\n",
    "                'total_ood_detected': int(np.sum(test_ood_results['ood_predictions'])),\n",
    "                'per_class_stats': test_ood_results['per_class_stats']\n",
    "            }\n",
    "        },\n",
    "        'hybrid_classification': {\n",
    "            'baseline_model': hybrid_results['baseline'],\n",
    "            'test_model': hybrid_results['test']\n",
    "        },\n",
    "        'comparison_summary': {\n",
    "            'ood_detection_improvement': {\n",
    "                'accuracy': float(test_ood_results['accuracy'] - baseline_ood_results['accuracy']),\n",
    "                'precision': float(test_ood_results['precision'] - baseline_ood_results['precision']),\n",
    "                'recall': float(test_ood_results['recall'] - baseline_ood_results['recall']),\n",
    "                'f1_score': float(test_ood_results['f1_score'] - baseline_ood_results['f1_score']),\n",
    "                'auc_roc': float(test_ood_results['auc_roc'] - baseline_ood_results['auc_roc'])\n",
    "            },\n",
    "            'hybrid_classification_improvement': {\n",
    "                'baseline_ood_benefit': float(hybrid_results['baseline']['ood_benefit']),\n",
    "                'test_ood_benefit': float(hybrid_results['test']['ood_benefit']),\n",
    "                'model_improvement': float(hybrid_results['test']['hybrid_accuracy'] - hybrid_results['baseline']['hybrid_accuracy'])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save main results file\n",
    "    results_filename = f\"{results_dir}/ood_prediction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    def convert_numpy_types(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    prediction_results = convert_numpy_types(prediction_results)\n",
    "    \n",
    "    try:\n",
    "        with open(results_filename, 'w') as f:\n",
    "            json.dump(prediction_results, f, indent=2)\n",
    "        print(f\"âœ… Saved comprehensive results to: {results_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving results: {e}\")\n",
    "    \n",
    "    # Save detailed predictions for further analysis\n",
    "    detailed_predictions = {\n",
    "        'true_labels': true_classes.tolist(),\n",
    "        'baseline_model': {\n",
    "            'original_predictions': predicted_classes_baseline.tolist(),\n",
    "            'hybrid_predictions': hybrid_results['baseline']['hybrid_predictions'].tolist(),\n",
    "            'ood_predictions': baseline_ood_results['ood_predictions'].tolist(),\n",
    "            'ood_distances': baseline_ood_results['distances'].tolist()\n",
    "        },\n",
    "        'test_model': {\n",
    "            'original_predictions': predicted_classes_test.tolist(),\n",
    "            'hybrid_predictions': hybrid_results['test']['hybrid_predictions'].tolist(),\n",
    "            'ood_predictions': test_ood_results['ood_predictions'].tolist(),\n",
    "            'ood_distances': test_ood_results['distances'].tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    predictions_filename = f\"{results_dir}/detailed_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(predictions_filename, 'w') as f:\n",
    "            json.dump(detailed_predictions, f, indent=2)\n",
    "        print(f\"âœ… Saved detailed predictions to: {predictions_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving detailed predictions: {e}\")\n",
    "    \n",
    "    # Print summary of what was saved\n",
    "    print(f\"\\nðŸ“Š Results Summary:\")\n",
    "    print(f\"   â€¢ Main results file: {results_filename}\")\n",
    "    print(f\"   â€¢ Detailed predictions: {predictions_filename}\")\n",
    "    print(f\"   â€¢ Test samples analyzed: {len(true_classes):,}\")\n",
    "    print(f\"   â€¢ Signal types: {', '.join(tags)}\")\n",
    "    \n",
    "    # Print key findings\n",
    "    print(f\"\\nðŸ” Key Findings:\")\n",
    "    ood_acc_improvement = test_ood_results['accuracy'] - baseline_ood_results['accuracy']\n",
    "    hybrid_acc_improvement = hybrid_results['test']['hybrid_accuracy'] - hybrid_results['baseline']['hybrid_accuracy']\n",
    "    \n",
    "    print(f\"   â€¢ OOD Detection Accuracy Improvement: {ood_acc_improvement:+.4f}\")\n",
    "    print(f\"   â€¢ Hybrid Classification Improvement: {hybrid_acc_improvement:+.4f}\")\n",
    "    print(f\"   â€¢ Best OOD Detection F1-Score: {max(baseline_ood_results['f1_score'], test_ood_results['f1_score']):.4f}\")\n",
    "    print(f\"   â€¢ Best Hybrid Classification Accuracy: {max(hybrid_results['baseline']['hybrid_accuracy'], hybrid_results['test']['hybrid_accuracy']):.4f}\")\n",
    "    \n",
    "    if ood_acc_improvement > 0.01:\n",
    "        print(f\"   âœ… Test model shows significant OOD detection improvement\")\n",
    "    if hybrid_acc_improvement > 0.01:\n",
    "        print(f\"   âœ… Test model shows significant hybrid classification improvement\")\n",
    "    \n",
    "    print(f\"\\nâœ… All OOD detection and hybrid classification results saved successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Skipping results saving - required results not available\")\n",
    "    print(\"   Missing one or more of: baseline_ood_results, test_ood_results, hybrid_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering Analysis on Penultimate Layer Features\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"K-MEANS CLUSTERING ANALYSIS ON NEURAL NETWORK FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def perform_clustering_analysis(features, true_labels, model_name, k_range=(3, 6)):\n",
    "    \"\"\"\n",
    "    Perform k-means clustering analysis on feature vectors.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature vectors from penultimate layer\n",
    "        true_labels: Ground truth labels for comparison\n",
    "        model_name: Name of the model for reporting\n",
    "        k_range: Range of k values to try (min, max)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results containing optimal k and clustering metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTERING ANALYSIS - {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Standardize features for better clustering\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    print(f\"Standardized features - Mean: {np.mean(features_scaled):.6f}, Std: {np.std(features_scaled):.6f}\")\n",
    "    \n",
    "    # Test different k values\n",
    "    k_values = list(range(k_range[0], k_range[1] + 1))\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nTesting k values: {k_values}\")\n",
    "    print(f\"{'k':<3} {'Inertia':<12} {'Silhouette':<12} {'ARI':<8} {'NMI':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    best_k = k_values[0]\n",
    "    best_silhouette = -1\n",
    "    best_kmeans = None\n",
    "    \n",
    "    for k in k_values:\n",
    "        # Perform k-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(features_scaled)\n",
    "        \n",
    "        # Calculate clustering metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette = silhouette_score(features_scaled, cluster_labels)\n",
    "        \n",
    "        # Compare with true labels (if available)\n",
    "        if len(np.unique(true_labels)) > 1:\n",
    "            ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "            nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "        else:\n",
    "            ari = 0.0\n",
    "            nmi = 0.0\n",
    "        \n",
    "        print(f\"{k:<3} {inertia:<12.2f} {silhouette:<12.4f} {ari:<8.4f} {nmi:<8.4f}\")\n",
    "        \n",
    "        # Track best k based on silhouette score\n",
    "        if silhouette > best_silhouette:\n",
    "            best_silhouette = silhouette\n",
    "            best_k = k\n",
    "            best_kmeans = kmeans\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette,\n",
    "            'adjusted_rand_index': ari,\n",
    "            'normalized_mutual_info': nmi,\n",
    "            'cluster_labels': cluster_labels,\n",
    "            'kmeans_model': kmeans\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ BEST CLUSTERING RESULT:\")\n",
    "    print(f\"   Optimal k: {best_k}\")\n",
    "    print(f\"   Best silhouette score: {best_silhouette:.4f}\")\n",
    "    \n",
    "    # Analyze best clustering\n",
    "    best_result = next(r for r in results if r['k'] == best_k)\n",
    "    best_labels = best_result['cluster_labels']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š CLUSTER ANALYSIS (k={best_k}):\")\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    unique_clusters, cluster_counts = np.unique(best_labels, return_counts=True)\n",
    "    for cluster_id, count in zip(unique_clusters, cluster_counts):\n",
    "        percentage = (count / len(best_labels)) * 100\n",
    "        print(f\"   Cluster {cluster_id}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Compare clusters with true signal classes\n",
    "    if len(np.unique(true_labels)) > 1:\n",
    "        print(f\"\\nðŸ” CLUSTER vs TRUE LABEL ANALYSIS:\")\n",
    "        \n",
    "        # Create confusion matrix between clusters and true labels\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        \n",
    "        # Only consider known signal classes (exclude UNKNOWN)\n",
    "        # known_mask = true_labels < 3  # Assuming UNKNOWN is class 3\n",
    "        # if np.sum(known_mask) > 0:\n",
    "        if True:\n",
    "            cm = confusion_matrix(true_labels, best_labels)\n",
    "            # cm = confusion_matrix(true_labels[known_mask], best_labels[known_mask])\n",
    "            \n",
    "            print(f\"   Confusion Matrix (Clusters vs True Labels):\")\n",
    "            signal_names = ['16QAM', '8CPSK', 'FM', 'UNKNOWN']\n",
    "            \n",
    "            # Print header\n",
    "            header = \"True\\\\Pred  \" + \"  \".join([f\"C{i}\" for i in range(best_k)])\n",
    "            print(f\"   {header}\")\n",
    "            \n",
    "            # Print matrix\n",
    "            for i, signal_name in enumerate(signal_names):\n",
    "                if i < cm.shape[0]:\n",
    "                    row = f\"   {signal_name:<8}  \" + \"  \".join([f\"{cm[i,j]:>2}\" for j in range(min(best_k, cm.shape[1]))])\n",
    "                    print(row)\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'best_k': best_k,\n",
    "        'best_silhouette': best_silhouette,\n",
    "        'best_kmeans': best_kmeans,\n",
    "        'best_labels': best_labels,\n",
    "        'features_scaled': features_scaled,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "\n",
    "def visualize_clustering_results(clustering_results, model_name, true_labels):\n",
    "    \"\"\"\n",
    "    Visualize clustering results with multiple plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = clustering_results['results']\n",
    "    best_k = clustering_results['best_k']\n",
    "    best_labels = clustering_results['best_labels']\n",
    "    print(best_labels)\n",
    "    features_scaled = clustering_results['features_scaled']\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.suptitle(f'K-Means Clustering Analysis - {model_name}', fontsize=16, y=0.98)\n",
    "    \n",
    "    # Plot 1: Elbow curve (Inertia vs k)\n",
    "    plt.subplot(3, 4, 1)\n",
    "    k_values = [r['k'] for r in results]\n",
    "    inertias = [r['inertia'] for r in results]\n",
    "    \n",
    "    plt.plot(k_values, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "    plt.axvline(best_k, color='red', linestyle='--', label=f'Optimal k={best_k}')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia (Within-cluster Sum of Squares)')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Silhouette score vs k\n",
    "    plt.subplot(3, 4, 2)\n",
    "    silhouette_scores = [r['silhouette_score'] for r in results]\n",
    "    \n",
    "    plt.plot(k_values, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "    plt.axvline(best_k, color='red', linestyle='--', label=f'Optimal k={best_k}')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: ARI and NMI vs k\n",
    "    plt.subplot(3, 4, 3)\n",
    "    ari_scores = [r['adjusted_rand_index'] for r in results]\n",
    "    nmi_scores = [r['normalized_mutual_info'] for r in results]\n",
    "    \n",
    "    plt.plot(k_values, ari_scores, 'ro-', linewidth=2, label='ARI', markersize=6)\n",
    "    plt.plot(k_values, nmi_scores, 'bo-', linewidth=2, label='NMI', markersize=6)\n",
    "    plt.axvline(best_k, color='black', linestyle='--', alpha=0.7, label=f'Optimal k={best_k}')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('External Validation Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Cluster size distribution for optimal k\n",
    "    plt.subplot(3, 4, 4)\n",
    "    unique_clusters, cluster_counts = np.unique(best_labels, return_counts=True)\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))\n",
    "    bars = plt.bar(unique_clusters, cluster_counts, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, cluster_counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title(f'Cluster Size Distribution (k={best_k})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5-8: 2D PCA visualization for different k values\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    for i, k in enumerate([3, 4, 5, 6]):\n",
    "        if k <= max(k_values):\n",
    "            plt.subplot(3, 4, 5 + i)\n",
    "            \n",
    "            # Get clustering result for this k\n",
    "            k_result = next((r for r in results if r['k'] == k), None)\n",
    "            if k_result is not None:\n",
    "                labels = k_result['cluster_labels']\n",
    "                \n",
    "                # Plot clusters\n",
    "                scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], \n",
    "                                    c=labels, cmap='Set3', alpha=0.7, s=30, edgecolors='black', linewidth=0.5)\n",
    "                \n",
    "                # Plot cluster centers in PCA space\n",
    "                kmeans_model = k_result['kmeans_model']\n",
    "                centers_pca = pca.transform(kmeans_model.cluster_centers_)\n",
    "                plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "                          c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "                \n",
    "                plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "                plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "                plt.title(f'PCA Visualization (k={k})')\n",
    "                \n",
    "                if k == best_k:\n",
    "                    plt.title(f'PCA Visualization (k={k}) â­ BEST', fontweight='bold')\n",
    "                \n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 9: True labels vs best clustering (if available)\n",
    "    plt.subplot(3, 4, 9)\n",
    "    if len(np.unique(true_labels)) > 1:\n",
    "        # Create side-by-side comparison\n",
    "        fig_width = 0.35\n",
    "        \n",
    "        # True labels\n",
    "        known_mask = true_labels < 3  # Exclude UNKNOWN class\n",
    "        if np.sum(known_mask) > 0:\n",
    "            plt.scatter(features_pca[known_mask, 0] - 0.5, features_pca[known_mask, 1], \n",
    "                       c=true_labels[known_mask], cmap='viridis', alpha=0.7, s=30, \n",
    "                       edgecolors='black', linewidth=0.5, label='True Labels')\n",
    "            \n",
    "            # Best clustering\n",
    "            plt.scatter(features_pca[known_mask, 0] + 0.5, features_pca[known_mask, 1], \n",
    "                       c=best_labels[known_mask], cmap='Set3', alpha=0.7, s=30,\n",
    "                       edgecolors='black', linewidth=0.5, label='Clusters')\n",
    "            \n",
    "            plt.xlabel('PC1 (shifted for comparison)')\n",
    "            plt.ylabel('PC2')\n",
    "            plt.title('True Labels vs Clusters')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No known class\\nlabels available', \n",
    "                    ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)\n",
    "            plt.title('True Labels vs Clusters')\n",
    "    \n",
    "    # Plot 10: Metrics summary\n",
    "    plt.subplot(3, 4, 10)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "Clustering Results Summary:\n",
    "\n",
    "Optimal k: {best_k}\n",
    "Best Silhouette Score: {clustering_results['best_silhouette']:.4f}\n",
    "\n",
    "All k Results:\n",
    "\"\"\"\n",
    "    \n",
    "    for r in results:\n",
    "        metrics_text += f\"k={r['k']}: Sil={r['silhouette_score']:.3f}, ARI={r['adjusted_rand_index']:.3f}\\n\"\n",
    "    \n",
    "    metrics_text += f\"\"\"\n",
    "Feature Information:\n",
    "â€¢ Original features: {features_scaled.shape[1]}\n",
    "â€¢ PCA variance explained: {pca.explained_variance_ratio_[:2].sum():.2%}\n",
    "â€¢ Standardization: Applied\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, metrics_text, transform=plt.gca().transAxes, \n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform clustering analysis on both models (if features are available)\n",
    "clustering_results = {}\n",
    "\n",
    "if 'baseline_test_features' in locals() and baseline_test_features is not None:\n",
    "    print(\"ðŸ” Analyzing BASELINE MODEL features...\")\n",
    "    clustering_results['baseline'] = perform_clustering_analysis(\n",
    "        baseline_test_features, true_classes, \"BASELINE MODEL\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ“Š Visualizing baseline model clustering...\")\n",
    "    visualize_clustering_results(clustering_results['baseline'], \"BASELINE MODEL\", true_classes)\n",
    "\n",
    "if 'test_test_features' in locals() and test_test_features is not None:\n",
    "    print(\"\\nðŸ” Analyzing TEST MODEL features...\")\n",
    "    clustering_results['test'] = perform_clustering_analysis(\n",
    "        test_test_features, true_classes, \"TEST MODEL\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ“Š Visualizing test model clustering...\")\n",
    "    visualize_clustering_results(clustering_results['test'], \"TEST MODEL\", true_classes)\n",
    "\n",
    "# Comparison between models (if both available)\n",
    "if len(clustering_results) == 2:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CLUSTERING COMPARISON BETWEEN MODELS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    baseline_best = clustering_results['baseline']['best_k']\n",
    "    test_best = clustering_results['test']['best_k']\n",
    "    baseline_sil = clustering_results['baseline']['best_silhouette']\n",
    "    test_sil = clustering_results['test']['best_silhouette']\n",
    "    \n",
    "    print(f\"Baseline Model:\")\n",
    "    print(f\"  â€¢ Optimal k: {baseline_best}\")\n",
    "    print(f\"  â€¢ Best silhouette: {baseline_sil:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Model:\")\n",
    "    print(f\"  â€¢ Optimal k: {test_best}\")\n",
    "    print(f\"  â€¢ Best silhouette: {test_sil:.4f}\")\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    if test_sil > baseline_sil:\n",
    "        print(f\"  âœ… Test model shows better clustering structure (+{test_sil - baseline_sil:.4f})\")\n",
    "    elif baseline_sil > test_sil:\n",
    "        print(f\"  âš ï¸  Baseline model shows better clustering structure (+{baseline_sil - test_sil:.4f})\")\n",
    "    else:\n",
    "        print(f\"  âž– Both models show similar clustering structure\")\n",
    "    \n",
    "    if test_best == baseline_best:\n",
    "        print(f\"  ðŸŽ¯ Both models agree on optimal k={test_best}\")\n",
    "    else:\n",
    "        print(f\"  ðŸ”„ Models suggest different k values: baseline={baseline_best}, test={test_best}\")\n",
    "\n",
    "print(f\"\\nâœ… K-means clustering analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "new_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
