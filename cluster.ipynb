{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526814d0",
   "metadata": {},
   "source": [
    "## Helper functions for processing signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7488ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def slice_iq(iq: np.ndarray, frame_len: int = 2048, hop_len: int | None = None):\n",
    "    \"\"\"\n",
    "    iq: 1‑D complex128 array [I + 1jQ]  \n",
    "    frame_len: samples per frame  \n",
    "    hop_len:   overlap; default = frame_len (no overlap)\n",
    "    \"\"\"\n",
    "    if hop_len is None:\n",
    "        hop_len = frame_len\n",
    "    num_frames = 1 + (len(iq) - frame_len) // hop_len\n",
    "    frames = np.lib.stride_tricks.as_strided(\n",
    "        iq,\n",
    "        shape=(num_frames, frame_len),\n",
    "        strides=(iq.strides[0]*hop_len, iq.strides[0]),\n",
    "        writeable=False,\n",
    "    )\n",
    "    return frames.copy()          # make it C‑contiguous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632552de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(frames: np.ndarray, eps: float = 1e-12):\n",
    "    power = np.sqrt(np.mean(np.abs(frames)**2, axis=1, keepdims=True)) + eps\n",
    "    return frames / power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bba0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fft\n",
    "\n",
    "def log_mag_fft(frames: np.ndarray):\n",
    "    # 1‑sided spectrum (N/2 bins, dropping DC & Nyquist)\n",
    "    spec = fft(frames, axis=1)[:, 1:frames.shape[1]//2]\n",
    "    mag  = np.abs(spec)\n",
    "    return np.log1p(mag)          # log(1 + |X|)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def preprocess(X: np.ndarray, pca_energy: float | None = 0.95):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xz = scaler.transform(X)\n",
    "    if pca_energy:\n",
    "        pca = PCA(n_components=pca_energy, svd_solver=\"full\").fit(Xz)\n",
    "        Xz = pca.transform(Xz)\n",
    "    else:\n",
    "        pca = None\n",
    "    return Xz, scaler, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5319812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def sweep_k(X, ks=(2,3,4,5,6,8,10), minibatch=True, **kmeans_kw):\n",
    "    inertias, sils = [], []\n",
    "    KM = KMeans\n",
    "    for k in ks:\n",
    "        print(f\"Fitting k={k}...X shape = {X.shape}\")\n",
    "        km = KM(n_clusters=k, random_state=0, n_init=\"auto\", **kmeans_kw).fit(X)\n",
    "        inertias.append(km.inertia_)\n",
    "        labels = km.labels_\n",
    "        sils.append(silhouette_score(X, labels))\n",
    "    return inertias, sils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9c3f1",
   "metadata": {},
   "source": [
    "## Loading signal data from directory\n",
    "This section loads signal data from a directory and processes them to be ready for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b611ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_and_process_directory(data_dir, frame_len=2048, hop_len=None, max_files=None):\n",
    "    \"\"\"\n",
    "    Load all CSV files from directory and process them for clustering\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Path to directory containing CSV files\n",
    "    - frame_len: Frame length for slicing (default: 2048)\n",
    "    - hop_len: Hop length for overlap (default: frame_len, no overlap)\n",
    "    - max_files: Maximum number of files to process (None = all files)\n",
    "    \n",
    "    Returns:\n",
    "    - X: Feature matrix ready for clustering\n",
    "    - file_info: List of (filename, frame_indices) for tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = pathlib.Path(data_dir)\n",
    "    csv_files = sorted(list(data_path.glob('*.csv')))\n",
    "    \n",
    "    if max_files:\n",
    "        csv_files = csv_files[:max_files]\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {data_dir}\")\n",
    "    \n",
    "    all_features = []\n",
    "    file_info = []\n",
    "    \n",
    "    for file_idx, csv_file in enumerate(tqdm(csv_files, desc=\"Processing files\")):\n",
    "        try:\n",
    "            # Load CSV file (assuming it has I/Q data after metadata rows)\n",
    "            print(f\"Loading {csv_file.name}...\", end=\" \")\n",
    "            \n",
    "            # Skip first 10 rows (metadata) and load I/Q data\n",
    "            df = pd.read_csv(csv_file, skiprows=10, names=['I', 'Q'])\n",
    "            \n",
    "            # Remove any non-numeric rows\n",
    "            df = df.loc[~df['I'].isin(['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'])]\n",
    "            df['I'] = df['I'].astype('float')\n",
    "            df['Q'] = df['Q'].astype('float')\n",
    "            \n",
    "            # Convert to complex IQ signal\n",
    "            iq_signal = np.vstack((df['I'].values, df['Q'].values)).reshape(-1, 2)\n",
    "            idx = np.random.choice(iq_signal.shape[0], size=10000, replace=False)\n",
    "            sampled = iq_signal[idx]   \n",
    "            plt.plot(df['I'].values, df['Q'].values, 'o', markersize=1, alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "            # Identify number of clusters\n",
    "            ks = [4, 8, 16, 64]\n",
    "            inertias, sils = sweep_k(sampled, ks)\n",
    "\n",
    "            \n",
    "            print(f\"Shape: {iq_signal.shape}\")\n",
    "            best_k = ks[int(np.argmax(sils))]\n",
    "            print(f\"Best k based on silhouette score: {best_k}\")\n",
    "            print(f\"Silhouette scores: {dict(zip(ks, [f'{s:.3f}' for s in sils]))}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# Set your data directory path here\n",
    "data_directory = \"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/synthetic/visual\"\n",
    "\n",
    "# Process all files in the directory\n",
    "X_raw, file_info = load_and_process_directory(\n",
    "    data_directory, \n",
    "    frame_len=20000, \n",
    "    hop_len=20000,  # No overlap\n",
    "    max_files=None  # Process all files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff848aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_process_directory(data_dir, frame_len=2048, hop_len=None, max_files=None):\n",
    "    \"\"\"\n",
    "    Load all CSV files from directory and process them for clustering\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dir: Path to directory containing CSV files\n",
    "    - frame_len: Frame length for slicing (default: 2048)\n",
    "    - hop_len: Hop length for overlap (default: frame_len, no overlap)\n",
    "    - max_files: Maximum number of files to process (None = all files)\n",
    "    \n",
    "    Returns:\n",
    "    - X: Feature matrix ready for clustering\n",
    "    - file_info: List of (filename, frame_indices) for tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = pathlib.Path(data_dir)\n",
    "    csv_files = sorted(list(data_path.glob('*.csv')))\n",
    "    \n",
    "    if max_files:\n",
    "        csv_files = csv_files[:max_files]\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files in {data_dir}\")\n",
    "    \n",
    "    all_features = []\n",
    "    file_info = []\n",
    "    \n",
    "    for file_idx, csv_file in enumerate(tqdm(csv_files, desc=\"Processing files\")):\n",
    "        try:\n",
    "            # Load CSV file (assuming it has I/Q data after metadata rows)\n",
    "            print(f\"Loading {csv_file.name}...\", end=\" \")\n",
    "            \n",
    "            # Skip first 10 rows (metadata) and load I/Q data\n",
    "            df = pd.read_csv(csv_file, skiprows=10, names=['I', 'Q'])\n",
    "            \n",
    "            # Remove any non-numeric rows\n",
    "            df = df.loc[~df['I'].isin(['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'])]\n",
    "            df['I'] = df['I'].astype('float')\n",
    "            df['Q'] = df['Q'].astype('float')\n",
    "            \n",
    "            # Convert to complex IQ signal\n",
    "            iq_signal = np.vstack((df['I'].values, df['Q'].values)).reshape(-1, 2)\n",
    "            \n",
    "            print(f\"Shape: {iq_signal.shape}\")\n",
    "            \n",
    "            # Slice into frames\n",
    "            frames = slice_iq(iq_signal, frame_len=frame_len, hop_len=hop_len)\n",
    "            print(f\"  -> {frames.shape[0]} frames\")\n",
    "            \n",
    "            # Normalize frames\n",
    "            frames_norm = rms_norm(frames)\n",
    "            \n",
    "            # Extract FFT features\n",
    "            features = log_mag_fft(frames_norm)\n",
    "            features = frames_norm\n",
    "            \n",
    "            # Store features and track which file they came from\n",
    "            all_features.append(features)\n",
    "            \n",
    "            # Track file info for each frame\n",
    "            for frame_idx in range(features.shape[0]):\n",
    "                file_info.append({\n",
    "                    'file_name': csv_file.name,\n",
    "                    'file_index': file_idx,\n",
    "                    'frame_index': frame_idx,\n",
    "                    'total_frames': features.shape[0]\n",
    "                })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all features\n",
    "    if all_features:\n",
    "        X = np.vstack(all_features)\n",
    "        print(f\"\\nTotal features shape: {X.shape}\")\n",
    "        print(f\"Total frames from {len(csv_files)} files: {len(file_info)}\")\n",
    "        return X, file_info\n",
    "    else:\n",
    "        print(\"No features extracted!\")\n",
    "        return None, None\n",
    "\n",
    "# Set your data directory path here\n",
    "data_directory = \"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/synthetic/synthetic_set0\"\n",
    "\n",
    "# Process all files in the directory\n",
    "X_raw, file_info = load_and_process_directory(\n",
    "    data_directory, \n",
    "    frame_len=20000, \n",
    "    hop_len=20000,  # No overlap\n",
    "    max_files=None  # Process all files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the features for clustering\n",
    "if X_raw is not None:\n",
    "    print(\"Preprocessing features...\")\n",
    "    Xz, scaler, pca = preprocess(X_raw, pca_energy=0.95)\n",
    "    \n",
    "    print(f\"Original features shape: {X_raw.shape}\")\n",
    "    print(f\"After preprocessing: {Xz.shape}\")\n",
    "    if pca is not None:\n",
    "        print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "        print(f\"Number of components: {pca.n_components_}\")\n",
    "    \n",
    "    # Show some statistics about the data\n",
    "    file_counts = {}\n",
    "    for info in file_info:\n",
    "        file_name = info['file_name']\n",
    "        if file_name not in file_counts:\n",
    "            file_counts[file_name] = 0\n",
    "        file_counts[file_name] += 1\n",
    "    \n",
    "    print(f\"\\nFrames per file:\")\n",
    "    for file_name, count in file_counts.items():\n",
    "        print(f\"  {file_name}: {count} frames\")\n",
    "else:\n",
    "    print(\"No data loaded. Please check your directory path and file format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85333457",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Only run clustering if we have data\n",
    "print(\"Running K-means clustering analysis...\")\n",
    "    \n",
    "ks = range(2, 11)\n",
    "inertias, sils = sweep_k(Xz, ks)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(ks, inertias, \"o-\")\n",
    "ax[0].set_title(\"Elbow Method (WCSS)\")\n",
    "ax[0].set_xlabel(\"Number of Clusters (k)\")\n",
    "ax[0].set_ylabel(\"Within-Cluster Sum of Squares\")\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax[1].plot(ks, sils, \"o-\")\n",
    "ax[1].set_title(\"Average Silhouette Score\")\n",
    "ax[1].set_xlabel(\"Number of Clusters (k)\")\n",
    "ax[1].set_ylabel(\"Silhouette Score\")\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = ks[int(np.argmax(sils))]\n",
    "print(f\"Best k based on silhouette score: {best_k}\")\n",
    "print(f\"Silhouette scores: {dict(zip(ks, [f'{s:.3f}' for s in sils]))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02300e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final clustering with best k\n",
    "\n",
    "print(f\"Performing final clustering with k={best_k}...\")\n",
    "\n",
    "km = KMeans(n_clusters=best_k, random_state=0, n_init=\"auto\").fit(Xz)\n",
    "labels = km.labels_\n",
    "centroids = km.cluster_centers_\n",
    "\n",
    "print(f\"Clustering completed!\")\n",
    "print(f\"Cluster assignments:\")\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    percentage = count / len(labels) * 100\n",
    "    print(f\"  Cluster {label}: {count} frames ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze cluster assignments by file\n",
    "print(f\"\\nCluster distribution by file:\")\n",
    "cluster_by_file = {}\n",
    "for i, info in enumerate(file_info):\n",
    "    file_name = info['file_name']\n",
    "    cluster = labels[i]\n",
    "    \n",
    "    if file_name not in cluster_by_file:\n",
    "        cluster_by_file[file_name] = {}\n",
    "    if cluster not in cluster_by_file[file_name]:\n",
    "        cluster_by_file[file_name][cluster] = 0\n",
    "    cluster_by_file[file_name][cluster] += 1\n",
    "\n",
    "for file_name, clusters in cluster_by_file.items():\n",
    "    total_frames = sum(clusters.values())\n",
    "    print(f\"\\n  {file_name} ({total_frames} frames):\")\n",
    "    for cluster in sorted(clusters.keys()):\n",
    "        count = clusters[cluster]\n",
    "        percentage = count / total_frames * 100\n",
    "        print(f\"    Cluster {cluster}: {count} frames ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2df8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Create UMAP visualization if clustering is complete\n",
    "print(\"Creating UMAP visualization...\")\n",
    "\n",
    "embedding = umap.UMAP(n_neighbors=50, min_dist=0.1, metric=\"euclidean\",\n",
    "                        random_state=0).fit_transform(Xz)\n",
    "\n",
    "# Create two visualizations: one colored by cluster, one by file\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot 1: Color by cluster\n",
    "colors = plt.cm.tab10(labels)  # Use tab10 colormap for distinct colors\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=colors, s=3, alpha=0.7)\n",
    "plt.title(f\"UMAP: Colored by Cluster (k={best_k})\")\n",
    "plt.axis(\"off\")\n",
    "# Create custom legend for clusters\n",
    "unique_labels = np.unique(labels)\n",
    "legend_elements = []\n",
    "for label in unique_labels:\n",
    "    color = plt.cm.tab10(label)\n",
    "    count = np.sum(labels == label)\n",
    "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                    markerfacecolor=color, markersize=8, \n",
    "                                    label=f'Cluster {label} ({count} points)'))\n",
    "plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Print legend for file indices\n",
    "print(\"File index legend:\")\n",
    "unique_files = {}\n",
    "for info in file_info:\n",
    "    unique_files[info['file_index']] = info['file_name']\n",
    "\n",
    "for idx in sorted(unique_files.keys()):\n",
    "    print(f\"  Index {idx}: {unique_files[idx]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, pathlib, json\n",
    "\n",
    "# Save the clustering model and results if everything is complete\n",
    "print(\"Saving clustering model and results...\")\n",
    "\n",
    "pathlib.Path(\"models\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save the trained models\n",
    "model_data = {\n",
    "    \"scaler\": scaler, \n",
    "    \"pca\": pca, \n",
    "    \"kmeans\": km,\n",
    "    \"best_k\": best_k,\n",
    "    \"feature_shape\": X_raw.shape,\n",
    "    \"processed_shape\": Xz.shape\n",
    "}\n",
    "joblib.dump(model_data, \"models/clustering_model.pkl\")\n",
    "\n",
    "# Save metadata\n",
    "meta_data = {\n",
    "    \"frame_len\": 2048, \n",
    "    \"hop_len\": 2048,\n",
    "    \"pca_components\": pca.n_components_ if pca else None,\n",
    "    \"pca_variance_explained\": float(pca.explained_variance_ratio_.sum()) if pca else None,\n",
    "    \"num_clusters\": best_k,\n",
    "    \"total_frames\": len(labels),\n",
    "    \"files_processed\": len(set(info['file_name'] for info in file_info))\n",
    "}\n",
    "json.dump(meta_data, open(\"models/clustering_meta.json\", \"w\"), indent=2)\n",
    "\n",
    "# Save detailed results\n",
    "results_data = {\n",
    "    \"file_info\": file_info,\n",
    "    \"cluster_labels\": labels.tolist(),\n",
    "    \"cluster_counts\": {int(k): int(v) for k, v in zip(*np.unique(labels, return_counts=True))}\n",
    "}\n",
    "json.dump(results_data, open(\"models/clustering_results.json\", \"w\"), indent=2)\n",
    "\n",
    "print(f\"✓ Saved clustering model to: models/clustering_model.pkl\")\n",
    "print(f\"✓ Saved metadata to: models/clustering_meta.json\") \n",
    "print(f\"✓ Saved results to: models/clustering_results.json\")\n",
    "print(f\"✓ Processed {len(set(info['file_name'] for info in file_info))} files\")\n",
    "print(f\"✓ Generated {len(labels)} feature vectors\")\n",
    "print(f\"✓ Found {best_k} optimal clusters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
