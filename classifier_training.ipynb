{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# Test GPU usage\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"✓ GPU detected and available\")\n",
        "    with tf.device('/GPU:0'):\n",
        "        a = tf.constant([1.0, 2.0, 3.0])\n",
        "        b = tf.constant([4.0, 5.0, 6.0])\n",
        "        c = tf.add(a, b)\n",
        "        print(f\"GPU computation result: {c}\")\n",
        "else:\n",
        "    print(\"❌ No GPU available - using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Housekeeping Matters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0HyUdmk6G_j",
        "outputId": "9d8fa821-2481-479b-d753-8bc85d6b3af0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This ipynb file is adapted from the previous ResNet training script\n",
        "Author: Jason Niow\n",
        "DtaeL \n",
        "'''\n",
        "\n",
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "\n",
        "print(f'tensorflow version: {tf.__version__}')\n",
        "print(f'pandas version: {pd.__version__}')\n",
        "print(f'numpy version: {np.__version__}')\n",
        "print(f'seaborn version: {sns.__version__}')\n",
        "\n",
        "# check tensorflow GPU device support\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    print('GPU present')\n",
        "else:\n",
        "    print('GPU absent')\n",
        "\n",
        "# paths to load datasets from\n",
        "train_store_path = 'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/train_datasets/train_jul30_ood'\n",
        "test_store_path = 'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/test_datasets/test_jul22_real'\n",
        "\n",
        "# convert to pathlib Path objects\n",
        "train_dir = pathlib.Path(train_store_path)\n",
        "test_dir = pathlib.Path(test_store_path)\n",
        "\n",
        "# get list of datasets paths in dir\n",
        "train_ds_paths = sorted(list(train_dir.glob('*.csv')))\n",
        "test_ds_paths = sorted(list(test_dir.glob('*.csv')))\n",
        "\n",
        "\n",
        "# extract classification target from file names\n",
        "train_ds_type = np.array([x.parts[-1].split('_')[:2] for x in train_ds_paths])\n",
        "test_ds_type = np.array([x.parts[-1].split('_')[:2] for x in test_ds_paths])\n",
        "\n",
        "# Get list of classification labels of dataset e.g. 8CPSK, FM, 16qam\n",
        "train_ds_mod = [s.upper() for s in train_ds_type[:,0]]\n",
        "test_ds_mod = [s.upper() for s in test_ds_type[:,0]]\n",
        "\n",
        "# Get list of classification frequency\n",
        "train_ds_freq = [s.upper() for s in train_ds_type[:, 1]]\n",
        "test_ds_freq = [s.upper() for s in test_ds_type[:, 1]]\n",
        "\n",
        "# generate signal type tags\n",
        "known_signal_tags = {'16QAM', '8CPSK', 'FM'}\n",
        "signal_tags = {'16QAM': 0, '8CPSK': 1, 'FM': 2}\n",
        "# signal_tags = {k : i for i, k in enumerate(np.unique(sorted([s.upper() for s in train_ds_mod] + ['UNKNOWN'])))}\n",
        "\n",
        "print(signal_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RMS Normalization (with optional DC offset correction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def rms_normalize(iq, target_rms=1.0, eps=1e-12, remove_dc=True):\n",
        "    \"\"\"\n",
        "    iq: complex64/complex128 numpy array, shape (N,)\n",
        "    target_rms: desired RMS after scaling\n",
        "    \"\"\"\n",
        "    x = iq.astype(np.complex64, copy=False)\n",
        "    if remove_dc:\n",
        "        x = x - x.mean()\n",
        "    rms = np.sqrt(np.mean(np.abs(x)**2) + eps)\n",
        "    return x * (target_rms / rms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P-lvxTnbtJ2-",
        "outputId": "1221943f-c330-4f99-9b19-34f6caeb02f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load the dataset(s)\n",
        "\n",
        "# load dataset information\n",
        "specs = []\n",
        "datasets = []\n",
        "\n",
        "for dataset_paths in [train_ds_paths, test_ds_paths]:\n",
        "    temp_ds = []\n",
        "    temp_specs = []\n",
        "\n",
        "    for path in dataset_paths:\n",
        "        print(f'loading {path}...', end=' ')\n",
        "\n",
        "        # load dataset details - Sampling frequency, Number of Samples, Number of Records\n",
        "        df_spec = pd.read_csv(path, nrows=10, header=None, index_col=0, names=['info'])\n",
        "        df_spec = df_spec.drop(['Version', 'DateTime', 'TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'], axis=0).astype('int')\n",
        "\n",
        "        temp_specs.append(df_spec)\n",
        "\n",
        "        # load data, strip unnecessary bits out - I/Q data\n",
        "        df = pd.read_csv(path, skiprows=10, names=['I', 'Q'])\n",
        "\n",
        "        df = df.loc[~df['I'].isin(['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'])]\n",
        "        df['I'] = df['I'].astype('float')\n",
        "\n",
        "        print(f'loaded')\n",
        "\n",
        "        temp_ds.append(df)\n",
        "\n",
        "    datasets.append(temp_ds)\n",
        "    specs.append(temp_specs)\n",
        "\n",
        "print('done.')\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(specs[0]), len(specs[1])\n",
        "len(datasets[0]), len(datasets[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IXJ_06DTwIaY",
        "outputId": "d35c419b-9e43-4ad9-bf5f-f835a396dedf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# split dataset(s) into records, extract test dataset\n",
        "processed = []\n",
        "\n",
        "# number of test records to extract\n",
        "ntest = 100\n",
        "rlength = 1024\n",
        "nrecords = 1\n",
        "nsamples = 10000\n",
        "\n",
        "for h, dataset in enumerate(datasets): # loops through training, then testing\n",
        "    if h == 0:\n",
        "        dataset_type = 'TRAINING'\n",
        "    else: dataset_type = 'TESTING'\n",
        "    temp_processed = []\n",
        "    specs_df = specs[h]\n",
        "\n",
        "    print(f'\\nType\\t\\tLocation\\tTotal Records\\tSamples/Record')\n",
        "    # Loops through each data point in the dataset\n",
        "    for i in range(len(dataset)):\n",
        "        # nrecords = specs_df[i].loc['NumberRecords']['info'] if dataset_type == 'TRAINING' else 400 ### wtf is this\n",
        "        # nrecords = specs_df[i].loc['NumberRecords']['info']\n",
        "        # nsamples = specs_df[i].loc['NumberSamples']['info']\n",
        "\n",
        "        ds_length = dataset[i].shape[0]\n",
        "\n",
        "        # make life easier\n",
        "        ds_mod = train_ds_mod if dataset_type == 'TRAINING' else test_ds_mod\n",
        "        ds_freq = train_ds_freq if dataset_type == 'TRAINING' else test_ds_freq\n",
        "\n",
        "        # sanity check\n",
        "        print(f'{ds_mod[i]:<13}\\t{ds_freq[i]:<15}\\t{nrecords:<7}\\t\\t{nsamples:<7}')\n",
        "\n",
        "        # loop through dataset to split\n",
        "        for j in range(nrecords):\n",
        "            # extract sample length worth of samples for each record, then transpose for easier access later\n",
        "            # record = dataset[i].iloc[(nsamples * j):(nsamples * (j+1))].values.T\n",
        "            iq = dataset[i].iloc[(nsamples * j):(nsamples * (j+1))].values\n",
        "            iq_complex = iq[:, 0] + 1j * iq[:, 1]\n",
        "            iq_rms = rms_normalize(iq_complex, target_rms=1.0, remove_dc=False).T\n",
        "            i_rms = iq_rms.real.reshape((1, -1))\n",
        "            q_rms = iq_rms.imag.reshape((1, -1))\n",
        "            record = np.vstack((i_rms, q_rms))\n",
        "            # print(f\"Shape of record: {record.shape}\")\n",
        "            # pad shorter records with random padding to rlength\n",
        "            if nsamples < rlength:\n",
        "                print(f\"i: {i} j : {j} Sample length {nsamples} is lesser than {rlength}\")\n",
        "                # deterine pad amount\n",
        "                pad_length = rlength - nsamples\n",
        "                lpad_length = np.random.randint(0, pad_length+1)\n",
        "                rpad_length = pad_length - lpad_length\n",
        "\n",
        "                # generate pad\n",
        "                lpad = np.zeros((2, lpad_length))\n",
        "                rpad = np.zeros((2, rpad_length))\n",
        "\n",
        "                # concatenate pad\n",
        "                record = np.concatenate([lpad, record, rpad], axis=1)\n",
        "\n",
        "            # truncate longer records to rlength\n",
        "            elif nsamples > rlength:\n",
        "                # print(f\"i: {i} j : {j} Sample length {nsamples} is greater than {rlength}\")\n",
        "                record = record[:,:rlength]\n",
        "\n",
        "            # add processed record to list\n",
        "            signal_tag = signal_tags.get(ds_mod[i], 3) # 3 is for proxy OOD samples\n",
        "            # signal_tag = signal_tags[ds_mod[i].upper()]\n",
        "            temp_processed.append([ds_mod[i], signal_tag, ds_freq[i], record])\n",
        "\n",
        "    processed.append(temp_processed)\n",
        "\n",
        "# convert list into dataframes for later use, randomise, extract test records\n",
        "df_train = pd.DataFrame(processed[0], columns=['signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
        "df_test = pd.DataFrame(processed[1], columns=['signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
        "\n",
        "# print dataset statistics\n",
        "print(f'\\n{\"Stats\":^30}')\n",
        "print(f'Dataset\\tLength\\tRecords/Sample')\n",
        "print(f'Train\\t{df_train.shape[0]:<5}\\t{df_train[\"record\"].iloc[0].shape[1]}')\n",
        "print(f'Test\\t{df_test.shape[0]:<5}\\t{df_train[\"record\"].iloc[0].shape[1]}')\n",
        "\n",
        "\n",
        "# define one hot encode function\n",
        "def one_hot(arr, n_cat):\n",
        "    output = []\n",
        "    for n in arr:\n",
        "        if n == 3: # for proxy OOD samples\n",
        "            result = np.zeros(n_cat)\n",
        "        else: # for known classes\n",
        "            result = np.zeros(n_cat)\n",
        "            result[n] = 1\n",
        "\n",
        "        output.append(result)\n",
        "\n",
        "    return np.array(output, dtype=int)\n",
        "\n",
        "# extract train and test data\n",
        "X_train = np.concatenate(df_train['record'].values).reshape((df_train.shape[0], 2, rlength, 1))\n",
        "y_train = one_hot(df_train['tag'].values, len(signal_tags))\n",
        "y_ood = df_train['tag'] == 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# print(y_ood)\n",
        "y_train = np.hstack((y_train, y_ood.values.reshape(-1, 1)))  # Append OOD flag\n",
        "\n",
        "#X_test = np.concatenate(df_test['record'].values).reshape((df_test.shape[0], 2, rlength, 1))\n",
        "#y_test = one_hot(df_test['tag'].values, len(signal_tags))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Model (ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1pn2XwbDPQp",
        "outputId": "4c73e934-7fbb-4f44-888d-020580ebec42"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import model stuff\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Dense, Input, Flatten\n",
        "\n",
        "# functions for model segments\n",
        "def res_unit(x, dim, n):\n",
        "    '''\n",
        "    function that creates a residual unit for each residual stack.\n",
        "\n",
        "    INPUT PARAMETERS\n",
        "    x: layer to connect to\n",
        "    dim: size of layer\n",
        "    n: number of units to create\n",
        "    '''\n",
        "\n",
        "    for _ in range(n):\n",
        "        u = Conv2D(dim, 2, activation='relu', padding='same')(x)\n",
        "        u = Conv2D(dim, 2, activation='linear', padding='same')(u)\n",
        "\n",
        "        # skip-con\n",
        "        x = concatenate([u, x])\n",
        "\n",
        "    return x\n",
        "\n",
        "def res_stack(x, dim):\n",
        "    '''\n",
        "    function that creates a residual stack for the model\n",
        "\n",
        "    INPUT PARAMETERS\n",
        "    x: layer to connect to\n",
        "    dim: size of stack\n",
        "    '''\n",
        "\n",
        "    s = Conv2D(dim, 1, activation='linear', padding='same')(x)\n",
        "    s = res_unit(s, dim, 2)\n",
        "    s = MaxPooling2D(2, padding='same')(s)\n",
        "\n",
        "    return s\n",
        "\n",
        "# function to create main model\n",
        "def create_model(in_dim, out_dim):\n",
        "    '''\n",
        "    function to construct the actual resnet model.\n",
        "\n",
        "    INPUT PARAMETERS\n",
        "    in_dim: dimensions of input\n",
        "    out_dim: size of output\n",
        "    '''\n",
        "\n",
        "    input_layer = Input(in_dim)\n",
        "\n",
        "    # res stacks\n",
        "    x = res_stack(input_layer, 512)\n",
        "    x = res_stack(x, 256)\n",
        "    x = res_stack(x, 128)\n",
        "    x = res_stack(x, 64)\n",
        "    x = res_stack(x, 32)\n",
        "    x = res_stack(x, 16)\n",
        "\n",
        "    # fully connected layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    output_layer = Dense(out_dim, activation='softmax')(x)\n",
        "\n",
        "    # turn layers into model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name='resnet_rf_classification_model')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model((2, rlength, 1), len(signal_tags))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss with entropy regularization (generated by o3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def masked_mean(values, mask):\n",
        "    mask = tf.cast(mask, values.dtype)            # [B]\n",
        "    num = tf.reduce_sum(values * mask)            # sum over selected\n",
        "    den = tf.reduce_sum(mask)                     # count selected\n",
        "    return tf.math.divide_no_nan(num, den)        # safe mean\n",
        "\n",
        "class EntropyRegularizedLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    Custom loss function that combines classification loss with entropy regularization\n",
        "    for better OOD detection performance.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, entropy_weight=0.1, ood_entropy_weight=1.0, name=\"entropy_regularized\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            entropy_weight: Weight for entropy regularization on in-distribution samples\n",
        "            ood_entropy_weight: Weight for entropy maximization on OOD proxy samples\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "        self.entropy_weight = entropy_weight\n",
        "        self.ood_entropy_weight = ood_entropy_weight\n",
        "    \n",
        "    def call(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Compute combined loss: classification + entropy regularization\n",
        "        \n",
        "        y_true format: [class_labels, is_ood_flag]\n",
        "        - class_labels: one-hot encoded class labels \n",
        "        - is_ood_flag: 1 for OOD samples, 0 for in-distribution samples\n",
        "        \"\"\"\n",
        "        \n",
        "        # Split true labels and OOD flag\n",
        "        class_labels = y_true[:, :-1]  # All columns except last\n",
        "        is_ood = y_true[:, -1:] # Last column indicates OOD (1) or ID (0)\n",
        "        \n",
        "        # Standard classification loss (only for in-distribution samples)\n",
        "        id_mask = tf.equal(is_ood, 0)\n",
        "        id_mask = tf.squeeze(id_mask, axis=1)\n",
        "        \n",
        "        # Classification loss for in-distribution samples\n",
        "        classification_loss = tf.where(\n",
        "            id_mask,\n",
        "            tf.keras.losses.categorical_crossentropy(class_labels, y_pred),\n",
        "            0.0\n",
        "        )\n",
        "        classification_loss = tf.reduce_mean(classification_loss)\n",
        "        \n",
        "        # Entropy calculation: H(p) = -sum(p * log(p))\n",
        "        epsilon = 1e-8  # Small constant to prevent log(0)\n",
        "        entropy = -tf.reduce_sum(y_pred * tf.math.log(y_pred + epsilon), axis=1)\n",
        "        max_entropy = tf.math.log(tf.cast(tf.shape(y_pred)[1], tf.float32))  # log(num_classes)\n",
        "        \n",
        "        # Entropy regularization for in-distribution samples (encourage lower entropy/higher confidence)\n",
        "        id_entropy_loss = tf.where(\n",
        "            id_mask,\n",
        "            entropy,  # Minimize entropy for ID samples (encourage high confidence)\n",
        "            0.0\n",
        "        )\n",
        "        id_entropy_loss = tf.reduce_mean(id_entropy_loss) * self.entropy_weight\n",
        "        \n",
        "        # Entropy maximization for OOD samples (encourage higher entropy/lower confidence)\n",
        "        ood_mask = tf.equal(is_ood, 1)\n",
        "        ood_mask = tf.squeeze(ood_mask, axis=1)\n",
        "        \n",
        "        ood_entropy_loss = tf.where(\n",
        "            ood_mask,\n",
        "            -(entropy),  # Maximize entropy for OOD samples (encourage low confidence)\n",
        "            0.0\n",
        "        )\n",
        "        ood_entropy_loss = tf.reduce_mean(ood_entropy_loss) * self.ood_entropy_weight\n",
        "        \n",
        "        # Combined loss\n",
        "        total_loss = classification_loss + id_entropy_loss + ood_entropy_loss\n",
        "        \n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Accuracy Metric - ID Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Model with Entropy Regularization\n",
        "\n",
        "# Custom accuracy metric that only considers in-distribution samples\n",
        "class IDAccuracy(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='id_accuracy', **kwargs):\n",
        "        super(IDAccuracy, self).__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Extract class labels and OOD flags\n",
        "        class_labels = y_true[:, :-1]\n",
        "        is_ood = y_true[:, -1]\n",
        "        \n",
        "        # Only consider in-distribution samples (is_ood == 0)\n",
        "        id_mask = tf.equal(is_ood, 0)\n",
        "        \n",
        "        # Get predictions for ID samples only\n",
        "        id_predictions = tf.boolean_mask(y_pred, id_mask)\n",
        "        id_true_labels = tf.boolean_mask(class_labels, id_mask)\n",
        "        \n",
        "        # Calculate accuracy for ID samples\n",
        "        if tf.size(id_predictions) > 0:\n",
        "            matches = tf.equal(\n",
        "                tf.argmax(id_true_labels, axis=1),\n",
        "                tf.argmax(id_predictions, axis=1)\n",
        "            )\n",
        "            matches = tf.cast(matches, tf.float32)\n",
        "            \n",
        "            self.total.assign_add(tf.reduce_sum(matches))\n",
        "            self.count.assign_add(tf.cast(tf.size(matches), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.total, self.count)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total.assign(0)\n",
        "        self.count.assign(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING MODEL WITH ENTROPY REGULARIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Training parameters\n",
        "lr = 0.0003\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "# Initialize entropy regularized loss\n",
        "entropy_loss = EntropyRegularizedLoss(\n",
        "    entropy_weight=0.1,      # Weight for ID entropy regularization\n",
        "    ood_entropy_weight=1.0   # Weight for OOD entropy maximization\n",
        ")\n",
        "\n",
        "# Enhanced callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        patience=15, \n",
        "        verbose=1, \n",
        "        mode='min',\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=8,\n",
        "        verbose=1,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "]\n",
        "\n",
        "# Compile model with custom loss and metrics\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=lr),\n",
        "    loss=entropy_loss,\n",
        "    metrics=[IDAccuracy()]\n",
        ")\n",
        "\n",
        "print(f\"🔧 Training configuration:\")\n",
        "print(f\"  • Optimizer: Adam (lr={lr})\")\n",
        "print(f\"  • Batch size: {batch_size}\")\n",
        "print(f\"  • Max epochs: {epochs}\")\n",
        "print(f\"  • Entropy weight: {entropy_loss.entropy_weight}\")\n",
        "print(f\"  • OOD entropy weight: {entropy_loss.ood_entropy_weight}\")\n",
        "print(f\"  • Total training samples: {len(X_train):,}\")\n",
        "print(f\"  • ID samples: {np.sum(y_train[:, -1] == 0):,}\")\n",
        "print(f\"  • OOD samples: {np.sum(y_train[:, -1] == 1):,}\")\n",
        "\n",
        "print(f\"\\n🚀 Starting entropy regularized training...\")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=callbacks,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "train_hist = history.history\n",
        "\n",
        "print(f\"\\n✅ Training completed!\")\n",
        "print(f\"  • Final training loss: {train_hist['loss'][-1]:.4f}\")\n",
        "print(f\"  • Final validation loss: {train_hist['val_loss'][-1]:.4f}\")\n",
        "print(f\"  • Final ID accuracy: {train_hist['id_accuracy'][-1]:.4f}\")\n",
        "print(f\"  • Final val ID accuracy: {train_hist['val_id_accuracy'][-1]:.4f}\")\n",
        "print(f\"  • Epochs trained: {len(train_hist['loss'])}\")\n",
        "\n",
        "# Store entropy training flag for later use\n",
        "model.entropy_regularized = True\n",
        "model.entropy_weights = {\n",
        "    'entropy_weight': entropy_loss.entropy_weight,\n",
        "    'ood_entropy_weight': entropy_loss.ood_entropy_weight\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# save model\n",
        "model.save('C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/jul30_entropy_1')  # No extension - uses SavedModel format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM5iOvfb9n2Y"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_hist.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "ae180da1",
        "outputId": "d08ceb53-f6c6-4138-a0b5-0f48b3afaf52"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_hist['id_accuracy'], label='ID Training Accuracy')\n",
        "plt.plot(train_hist['val_id_accuracy'], label='ID Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy (Jul 11)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_hist['loss'], label='Training Loss')\n",
        "plt.plot(train_hist['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss (Jul 11)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Out-of-Distribution (OOD) Detection Training\n",
        "\n",
        "## Mahalanobis Distance-Based OOD Detector\n",
        "\n",
        "After training the main classifier, we now train a separate Out-of-Distribution detector using Mahalanobis distance in the feature space. This detector will help identify unknown signal types that don't belong to any of our known classes (16QAM, 8CPSK, FM).\n",
        "\n",
        "**Key Concepts:**\n",
        "- **Feature Space Analysis**: Uses learned representations from the penultimate layer\n",
        "- **Class-Conditional Statistics**: Calculates mean and covariance for each known signal type\n",
        "- **Mahalanobis Distance**: Measures statistical distance accounting for feature correlations\n",
        "- **OOD Threshold**: Signals beyond threshold distance are classified as UNKNOWN\n",
        "\n",
        "**Training Pipeline:**\n",
        "1. Extract features from trained classifier\n",
        "2. Calculate class-conditional means and covariances\n",
        "3. Compute optimal threshold using validation data\n",
        "4. Save OOD detector for deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import linalg\n",
        "from sklearn.covariance import EmpiricalCovariance, LedoitWolf\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "class MahalanobisOODDetector:\n",
        "    \"\"\"\n",
        "    Mahalanobis distance-based Out-of-Distribution detector for neural networks.\n",
        "    \n",
        "    Based on \"A Simple Unified Framework for Detecting Out-of-Distribution Samples \n",
        "    and Adversarial Attacks\" (Lee et al., 2018)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, use_tied_cov=True, cov_estimator='ledoit_wolf'):\n",
        "        \"\"\"\n",
        "        Initialize Mahalanobis OOD detector.\n",
        "        \n",
        "        Args:\n",
        "            use_tied_cov: If True, use pooled covariance across all classes\n",
        "            cov_estimator: 'empirical' or 'ledoit_wolf' for covariance estimation\n",
        "        \"\"\"\n",
        "        self.use_tied_cov = use_tied_cov\n",
        "        self.cov_estimator = cov_estimator\n",
        "        self.class_means = {}\n",
        "        self.class_covs = {}\n",
        "        self.pooled_cov = None\n",
        "        self.inv_covs = {}\n",
        "        self.fitted = False\n",
        "        self.known_classes = None\n",
        "        self.threshold = None\n",
        "        \n",
        "    def fit(self, features, labels, class_names, validation_features=None, validation_labels=None):\n",
        "        \"\"\"\n",
        "        Fit the Mahalanobis detector using training features.\n",
        "        \n",
        "        Args:\n",
        "            features: Training features from penultimate layer, shape (n_samples, n_features)\n",
        "            labels: Training labels, shape (n_samples,)\n",
        "            class_names: List of class names corresponding to label indices\n",
        "            validation_features: Optional validation features for threshold calculation\n",
        "            validation_labels: Optional validation labels for threshold calculation\n",
        "        \"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"FITTING MAHALANOBIS OOD DETECTOR\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        self.known_classes = class_names\n",
        "        unique_labels = np.unique(labels)\n",
        "        \n",
        "        print(f\"Known classes: {class_names}\")\n",
        "        print(f\"Feature dimension: {features.shape[1]}\")\n",
        "        print(f\"Total training samples: {features.shape[0]}\")\n",
        "        \n",
        "        # Calculate class-conditional means\n",
        "        print(f\"\\nCalculating class-conditional means...\")\n",
        "        for label in unique_labels:\n",
        "            if label < len(class_names):  # Only process known classes\n",
        "                class_mask = labels == label\n",
        "                class_features = features[class_mask]\n",
        "                self.class_means[label] = np.mean(class_features, axis=0)\n",
        "                \n",
        "                print(f\"  {class_names[label]}: {class_features.shape[0]} samples\")\n",
        "        \n",
        "        # Calculate covariance matrices\n",
        "        print(f\"\\nCalculating covariance matrices...\")\n",
        "        if self.use_tied_cov:\n",
        "            # Use pooled covariance across all known classes\n",
        "            all_known_features = []\n",
        "            \n",
        "            for label in unique_labels:\n",
        "                if label < len(class_names):  # Only known classes\n",
        "                    class_mask = labels == label\n",
        "                    class_features = features[class_mask]\n",
        "                    all_known_features.append(class_features)\n",
        "            \n",
        "            all_known_features = np.vstack(all_known_features)\n",
        "            \n",
        "            # Estimate pooled covariance\n",
        "            if self.cov_estimator == 'ledoit_wolf':\n",
        "                print(\"  Using Ledoit-Wolf covariance estimation...\")\n",
        "                cov_estimator = LedoitWolf()\n",
        "                cov_estimator.fit(all_known_features)\n",
        "                self.pooled_cov = cov_estimator.covariance_\n",
        "            else:\n",
        "                print(\"  Using empirical covariance estimation...\")\n",
        "                self.pooled_cov = np.cov(all_known_features.T)\n",
        "            \n",
        "            # Use same covariance for all classes\n",
        "            for label in unique_labels:\n",
        "                if label < len(class_names):\n",
        "                    self.class_covs[label] = self.pooled_cov\n",
        "            \n",
        "            print(f\"  Tied covariance matrix shape: {self.pooled_cov.shape}\")\n",
        "            \n",
        "        else:\n",
        "            # Use class-specific covariances\n",
        "            print(\"  Using class-specific covariances...\")\n",
        "            for label in unique_labels:\n",
        "                if label < len(class_names):\n",
        "                    class_mask = labels == label\n",
        "                    class_features = features[class_mask]\n",
        "                    \n",
        "                    if self.cov_estimator == 'ledoit_wolf':\n",
        "                        cov_estimator = LedoitWolf()\n",
        "                        cov_estimator.fit(class_features)\n",
        "                        self.class_covs[label] = cov_estimator.covariance_\n",
        "                    else:\n",
        "                        self.class_covs[label] = np.cov(class_features.T)\n",
        "        \n",
        "        # Compute inverse covariances for efficiency\n",
        "        print(f\"\\nComputing inverse covariance matrices...\")\n",
        "        for label in self.class_covs:\n",
        "            try:\n",
        "                self.inv_covs[label] = linalg.inv(self.class_covs[label])\n",
        "                print(f\"  {class_names[label]}: Successfully inverted\")\n",
        "            except linalg.LinAlgError:\n",
        "                # Use pseudo-inverse if matrix is singular\n",
        "                self.inv_covs[label] = linalg.pinv(self.class_covs[label])\n",
        "                print(f\"  {class_names[label]}: Using pseudo-inverse (singular matrix)\")\n",
        "        \n",
        "        # Calculate threshold using validation data if provided\n",
        "        if validation_features is not None and validation_labels is not None:\n",
        "            print(f\"\\nCalculating optimal threshold using validation data...\")\n",
        "            val_distances = self._compute_min_distances(validation_features)\n",
        "            self.threshold = np.percentile(val_distances, 95)\n",
        "            print(f\"  Threshold (95th percentile): {self.threshold:.4f}\")\n",
        "        else:\n",
        "            # Use training data for threshold calculation\n",
        "            print(f\"\\nCalculating threshold using training data...\")\n",
        "            train_distances = self._compute_min_distances(features)\n",
        "            self.threshold = np.percentile(train_distances, 95)\n",
        "            print(f\"  Threshold (95th percentile): {self.threshold:.4f}\")\n",
        "        \n",
        "        self.fitted = True\n",
        "        print(f\"\\n✅ Mahalanobis OOD detector fitted successfully!\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def _compute_min_distances(self, features):\n",
        "        \"\"\"Compute minimum Mahalanobis distances across all classes\"\"\"\n",
        "        if not self.fitted and len(self.class_means) == 0:\n",
        "            raise ValueError(\"Detector must be fitted before computing distances\")\n",
        "        \n",
        "        n_samples = features.shape[0]\n",
        "        distances = {}\n",
        "        \n",
        "        # Compute Mahalanobis distance to each class\n",
        "        for label in self.class_means:\n",
        "            class_mean = self.class_means[label]\n",
        "            inv_cov = self.inv_covs[label]\n",
        "            \n",
        "            # Center the features\n",
        "            centered_features = features - class_mean\n",
        "            \n",
        "            # Compute Mahalanobis distance: sqrt((x-μ)^T Σ^(-1) (x-μ))\n",
        "            mahal_dist = np.sqrt(np.sum(centered_features @ inv_cov * centered_features, axis=1))\n",
        "            distances[label] = mahal_dist\n",
        "        \n",
        "        # Find minimum distance across all classes\n",
        "        distance_matrix = np.stack(list(distances.values()), axis=1)\n",
        "        min_distances = np.min(distance_matrix, axis=1)\n",
        "        \n",
        "        return min_distances\n",
        "    \n",
        "    def predict_ood(self, features, threshold=None):\n",
        "        \"\"\"\n",
        "        Predict out-of-distribution samples.\n",
        "        \n",
        "        Args:\n",
        "            features: Test features, shape (n_samples, n_features)\n",
        "            threshold: Mahalanobis distance threshold. If None, uses fitted threshold\n",
        "            \n",
        "        Returns:\n",
        "            ood_predictions: Boolean array, True for OOD samples\n",
        "            min_distances: Minimum Mahalanobis distances\n",
        "        \"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Detector must be fitted before making predictions\")\n",
        "        \n",
        "        min_distances = self._compute_min_distances(features)\n",
        "        \n",
        "        if threshold is None:\n",
        "            threshold = self.threshold\n",
        "        \n",
        "        ood_predictions = min_distances > threshold\n",
        "        \n",
        "        return ood_predictions, min_distances\n",
        "    \n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save the fitted detector to file\"\"\"\n",
        "        detector_data = {\n",
        "            'use_tied_cov': self.use_tied_cov,\n",
        "            'cov_estimator': self.cov_estimator,\n",
        "            'class_means': {int(k): v.tolist() for k, v in self.class_means.items()},\n",
        "            'class_covs': {int(k): v.tolist() for k, v in self.class_covs.items()},\n",
        "            'pooled_cov': self.pooled_cov.tolist() if self.pooled_cov is not None else None,\n",
        "            'inv_covs': {int(k): v.tolist() for k, v in self.inv_covs.items()},\n",
        "            'fitted': self.fitted,\n",
        "            'known_classes': self.known_classes,\n",
        "            'threshold': float(self.threshold) if self.threshold is not None else None\n",
        "        }\n",
        "        \n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(detector_data, f, indent=2)\n",
        "        \n",
        "        print(f\"✅ OOD detector saved to: {filepath}\")\n",
        "\n",
        "print(\"✅ Mahalanobis OOD detector class implemented!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Extract Features for OOD Detector Training\n",
        "\n",
        "def extract_features_from_model(model, X_data, layer_name=None):\n",
        "    \"\"\"\n",
        "    Extract features from a specific layer of the model.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        X_data: Input data\n",
        "        layer_name: Name of layer to extract from. If None, uses penultimate layer\n",
        "        \n",
        "    Returns:\n",
        "        features: Extracted features, shape (n_samples, n_features)\n",
        "    \"\"\"\n",
        "    if layer_name is None:\n",
        "        # Use penultimate layer (before final classification layer)\n",
        "        feature_layer = model.layers[-4]\n",
        "    else:\n",
        "        feature_layer = model.get_layer(layer_name)\n",
        "    \n",
        "    print(f\"Extracting features from layer: {feature_layer.name}\")\n",
        "    print(f\"Layer output shape: {feature_layer.output_shape}\")\n",
        "    \n",
        "    # Create feature extraction model\n",
        "    feature_extractor = tf.keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=feature_layer.output\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFeature extractor layers:\")\n",
        "    for i, layer in enumerate(feature_extractor.layers):\n",
        "        print(f\"  {i}: {layer.name} - {type(layer).__name__}\")\n",
        "    \n",
        "    # Extract features\n",
        "    features = feature_extractor.predict(X_data, verbose=1)\n",
        "    \n",
        "    # Flatten if needed (for dense layers, features might be 2D)\n",
        "    if len(features.shape) > 2:\n",
        "        features = features.reshape(features.shape[0], -1)\n",
        "    \n",
        "    print(f\"Extracted features shape: {features.shape}\")\n",
        "    return features\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXTRACTING FEATURES FOR OOD DETECTOR TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Extract features from the trained model using training data\n",
        "print(f\"\\nExtracting features from training data...\")\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "\n",
        "train_features = extract_features_from_model(model, X_train)\n",
        "train_labels = np.argmax(y_train, axis=1)  # Convert from one-hot to class indices\n",
        "\n",
        "print(f\"\\nFeature extraction completed:\")\n",
        "print(f\"  Training features shape: {train_features.shape}\")\n",
        "print(f\"  Training labels shape: {train_labels.shape}\")\n",
        "\n",
        "# Show class distribution\n",
        "print(f\"\\nClass distribution in training data:\")\n",
        "for i, class_name in enumerate(list(signal_tags.keys())):\n",
        "    count = np.sum(train_labels == i)\n",
        "    print(f\"  {class_name}: {count} samples\")\n",
        "\n",
        "print(\"✅ Feature extraction completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Split Data for OOD Detector Training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the training data for OOD detector training and validation\n",
        "# Use 80% for fitting the detector, 20% for threshold calculation\n",
        "print(\"=\" * 60)\n",
        "print(\"SPLITTING DATA FOR OOD DETECTOR TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure we have enough samples for each class\n",
        "min_samples_per_class = min([np.sum(train_labels == i) for i in np.unique(train_labels)])\n",
        "print(f\"Minimum samples per class: {min_samples_per_class}\")\n",
        "\n",
        "if min_samples_per_class < 10:\n",
        "    print(\"⚠️  Warning: Very few samples per class. Consider using all data for training.\")\n",
        "    # Use all data for training if we have very limited samples\n",
        "    ood_train_features = train_features\n",
        "    ood_train_labels = train_labels\n",
        "    ood_val_features = None\n",
        "    ood_val_labels = None\n",
        "    print(\"Using all training data for OOD detector fitting.\")\n",
        "else:\n",
        "    # Split data while preserving class distribution\n",
        "    ood_train_features, ood_val_features, ood_train_labels, ood_val_labels = train_test_split(\n",
        "        train_features, \n",
        "        train_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=train_labels\n",
        "    )\n",
        "    \n",
        "    print(f\"OOD training features shape: {ood_train_features.shape}\")\n",
        "    print(f\"OOD validation features shape: {ood_val_features.shape}\")\n",
        "    \n",
        "    print(f\"\\nOOD training set class distribution:\")\n",
        "    for i, class_name in enumerate(list(signal_tags.keys())):\n",
        "        count = np.sum(ood_train_labels == i)\n",
        "        print(f\"  {class_name}: {count} samples\")\n",
        "    \n",
        "    print(f\"\\nOOD validation set class distribution:\")\n",
        "    for i, class_name in enumerate(list(signal_tags.keys())):\n",
        "        count = np.sum(ood_val_labels == i)\n",
        "        print(f\"  {class_name}: {count} samples\")\n",
        "\n",
        "print(\"✅ Data splitting completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train Mahalanobis OOD Detector\n",
        "\n",
        "# Initialize and train the OOD detector\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING MAHALANOBIS OOD DETECTOR\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create class names list from signal_tags\n",
        "class_names = list(signal_tags.keys())\n",
        "print(f\"Training OOD detector for classes: {class_names}\")\n",
        "\n",
        "# Initialize the detector\n",
        "ood_detector = MahalanobisOODDetector(\n",
        "    use_tied_cov=True,  # Use pooled covariance for robustness\n",
        "    cov_estimator='ledoit_wolf'  # Use Ledoit-Wolf for better covariance estimation\n",
        ")\n",
        "\n",
        "# Train the detector\n",
        "ood_detector.fit(\n",
        "    features=ood_train_features,\n",
        "    labels=ood_train_labels,\n",
        "    class_names=class_names,\n",
        "    validation_features=ood_val_features,\n",
        "    validation_labels=ood_val_labels\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"OOD DETECTOR TRAINING SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"✅ Detector fitted successfully!\")\n",
        "print(f\"📊 Feature dimension: {ood_train_features.shape[1]}\")\n",
        "print(f\"🎯 Number of known classes: {len(class_names)}\")\n",
        "print(f\"📈 Threshold: {ood_detector.threshold:.4f}\")\n",
        "print(f\"🔧 Covariance estimator: {ood_detector.cov_estimator}\")\n",
        "print(f\"🔗 Tied covariance: {ood_detector.use_tied_cov}\")\n",
        "\n",
        "# Quick validation on training data\n",
        "if ood_val_features is not None:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"QUICK VALIDATION ON HELD-OUT DATA\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Predict OOD on validation set (should be all known, so mostly False)\n",
        "    val_ood_pred, val_distances = ood_detector.predict_ood(ood_val_features)\n",
        "    \n",
        "    print(f\"Validation samples: {len(ood_val_features)}\")\n",
        "    print(f\"Predicted as OOD: {np.sum(val_ood_pred)} ({np.mean(val_ood_pred)*100:.1f}%)\")\n",
        "    print(f\"Mean distance: {np.mean(val_distances):.4f}\")\n",
        "    print(f\"Distance std: {np.std(val_distances):.4f}\")\n",
        "    print(f\"Distance range: [{np.min(val_distances):.4f}, {np.max(val_distances):.4f}]\")\n",
        "    \n",
        "    # Show per-class statistics\n",
        "    print(f\"\\nPer-class validation statistics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_mask = ood_val_labels == i\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_distances = val_distances[class_mask]\n",
        "            class_ood_rate = np.mean(val_ood_pred[class_mask])\n",
        "            print(f\"  {class_name}: Mean dist={np.mean(class_distances):.4f}, OOD rate={class_ood_rate*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n✅ OOD detector training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(-1, -6, -1):\n",
        "    print(f\"Layer name: {model.layers[i].name}, Output shape: {model.layers[i].output_shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Visualize OOD Detector Training Results\n",
        "\n",
        "if ood_val_features is not None:\n",
        "    # Create visualizations of the OOD detector performance\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Mahalanobis OOD Detector Training Results', fontsize=16)\n",
        "    \n",
        "    # 1. Distance distribution by class\n",
        "    axes[0, 0].set_title('Mahalanobis Distance Distribution by Class')\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_mask = ood_val_labels == i\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_distances = val_distances[class_mask]\n",
        "            axes[0, 0].hist(class_distances, bins=15, alpha=0.6, label=class_name, density=True)\n",
        "    \n",
        "    axes[0, 0].axvline(ood_detector.threshold, color='red', linestyle='--', \n",
        "                       label=f'Threshold={ood_detector.threshold:.3f}')\n",
        "    axes[0, 0].set_xlabel('Mahalanobis Distance')\n",
        "    axes[0, 0].set_ylabel('Density')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Box plot of distances by class\n",
        "    axes[0, 1].set_title('Distance Distribution by Class (Box Plot)')\n",
        "    \n",
        "    class_distances_list = []\n",
        "    class_labels_list = []\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_mask = ood_val_labels == i\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_distances_list.append(val_distances[class_mask])\n",
        "            class_labels_list.append(class_name)\n",
        "    \n",
        "    if class_distances_list:\n",
        "        axes[0, 1].boxplot(class_distances_list, labels=class_labels_list)\n",
        "        axes[0, 1].axhline(ood_detector.threshold, color='red', linestyle='--', \n",
        "                           label=f'Threshold={ood_detector.threshold:.3f}')\n",
        "        axes[0, 1].set_ylabel('Mahalanobis Distance')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Threshold sensitivity analysis\n",
        "    axes[1, 0].set_title('Threshold Sensitivity Analysis')\n",
        "    \n",
        "    # Test different thresholds\n",
        "    thresholds = np.linspace(np.min(val_distances), np.max(val_distances), 50)\n",
        "    ood_rates = []\n",
        "    \n",
        "    for thresh in thresholds:\n",
        "        ood_pred_thresh = val_distances > thresh\n",
        "        ood_rate = np.mean(ood_pred_thresh)\n",
        "        ood_rates.append(ood_rate)\n",
        "    \n",
        "    axes[1, 0].plot(thresholds, ood_rates, 'b-', linewidth=2)\n",
        "    axes[1, 0].axvline(ood_detector.threshold, color='red', linestyle='--', \n",
        "                       label=f'Selected Threshold={ood_detector.threshold:.3f}')\n",
        "    axes[1, 0].set_xlabel('Threshold')\n",
        "    axes[1, 0].set_ylabel('OOD Rate (False Positive Rate)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Summary statistics\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    summary_text = f\"\"\"\n",
        "OOD Detector Training Summary:\n",
        "\n",
        "Training Data:\n",
        "• Samples: {ood_train_features.shape[0]}\n",
        "• Features: {ood_train_features.shape[1]}\n",
        "• Classes: {len(class_names)}\n",
        "\n",
        "Validation Data:\n",
        "• Samples: {ood_val_features.shape[0]}\n",
        "• Predicted OOD: {np.sum(val_ood_pred)} ({np.mean(val_ood_pred)*100:.1f}%)\n",
        "\n",
        "Distance Statistics:\n",
        "• Mean: {np.mean(val_distances):.4f}\n",
        "• Std: {np.std(val_distances):.4f}\n",
        "• Min: {np.min(val_distances):.4f}\n",
        "• Max: {np.max(val_distances):.4f}\n",
        "\n",
        "Configuration:\n",
        "• Threshold: {ood_detector.threshold:.4f}\n",
        "• Tied Covariance: {ood_detector.use_tied_cov}\n",
        "• Estimator: {ood_detector.cov_estimator}\n",
        "    \"\"\"\n",
        "    \n",
        "    axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes, \n",
        "                    fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
        "                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"📊 OOD detector visualization completed!\")\n",
        "else:\n",
        "    print(\"📊 Skipping visualization (no validation data available)\")\n",
        "    \n",
        "    # Simple histogram of training distances\n",
        "    train_distances = ood_detector._compute_min_distances(ood_train_features)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(train_distances, bins=30, alpha=0.7, density=True)\n",
        "    plt.axvline(ood_detector.threshold, color='red', linestyle='--', \n",
        "                label=f'Threshold={ood_detector.threshold:.3f}')\n",
        "    plt.xlabel('Mahalanobis Distance')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Training Data Distance Distribution')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_mask = ood_train_labels == i\n",
        "        if np.sum(class_mask) > 0:\n",
        "            class_distances = train_distances[class_mask]\n",
        "            plt.hist(class_distances, bins=15, alpha=0.6, label=class_name, density=True)\n",
        "    \n",
        "    plt.axvline(ood_detector.threshold, color='red', linestyle='--', \n",
        "                label=f'Threshold={ood_detector.threshold:.3f}')\n",
        "    plt.xlabel('Mahalanobis Distance')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Distance Distribution by Class')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save OOD Detector and Training Metadata\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create directory for OOD detectors if it doesn't exist\n",
        "ood_detector_dir = 'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/ood_detectors'\n",
        "os.makedirs(ood_detector_dir, exist_ok=True)\n",
        "\n",
        "# Generate filename with timestamp and model info\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_name = 'jul25_rms_1'  # Match the saved model name\n",
        "ood_detector_filename = f\"mahalanobis_ood_detector_{model_name}_{timestamp}_flatten_layer.json\"\n",
        "ood_detector_filepath = os.path.join(ood_detector_dir, ood_detector_filename)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING OOD DETECTOR AND METADATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save the OOD detector\n",
        "ood_detector.save(ood_detector_filepath)\n",
        "\n",
        "# Create and save comprehensive training metadata\n",
        "training_metadata = {\n",
        "    \"training_info\": {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"model_name\": model_name,\n",
        "        \"model_path\": f\"C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/{model_name}\",\n",
        "        \"ood_detector_path\": ood_detector_filepath,\n",
        "        \"training_duration\": \"Post-training (after main classifier)\",\n",
        "        \"tensorflow_version\": tf.__version__\n",
        "    },\n",
        "    \n",
        "    \"data_info\": {\n",
        "        \"train_store_path\": train_store_path,\n",
        "        \"total_training_samples\": int(X_train.shape[0]),\n",
        "        \"ood_training_samples\": int(ood_train_features.shape[0]),\n",
        "        \"ood_validation_samples\": int(ood_val_features.shape[0]) if ood_val_features is not None else 0,\n",
        "        \"feature_dimension\": int(ood_train_features.shape[1]),\n",
        "        \"record_length\": rlength,\n",
        "        \"signal_classes\": class_names,\n",
        "        \"class_distribution\": {\n",
        "            class_name: int(np.sum(ood_train_labels == i)) \n",
        "            for i, class_name in enumerate(class_names)\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"model_architecture\": {\n",
        "        \"classifier_layers\": len(model.layers),\n",
        "        \"feature_extraction_layer\": model.layers[-4].name,\n",
        "        \"feature_layer_output_shape\": str(model.layers[-4].output_shape),\n",
        "        \"total_parameters\": int(model.count_params()),\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"learning_rate\": lr,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs_trained\": len(train_hist['loss'])\n",
        "    },\n",
        "    \n",
        "    \"ood_detector_config\": {\n",
        "        \"detector_type\": \"Mahalanobis\",\n",
        "        \"use_tied_covariance\": ood_detector.use_tied_cov,\n",
        "        \"covariance_estimator\": ood_detector.cov_estimator,\n",
        "        \"threshold\": float(ood_detector.threshold),\n",
        "        \"threshold_method\": \"95th_percentile\"\n",
        "    },\n",
        "    \n",
        "    \"training_results\": {\n",
        "        \"final_train_accuracy\": float(train_hist['accuracy'][-1]),\n",
        "        \"final_val_accuracy\": float(train_hist['val_accuracy'][-1]),\n",
        "        \"final_train_loss\": float(train_hist['loss'][-1]),\n",
        "        \"final_val_loss\": float(train_hist['val_loss'][-1]),\n",
        "        \"ood_validation_stats\": {\n",
        "            \"total_samples\": int(len(ood_val_features)) if ood_val_features is not None else 0,\n",
        "            \"predicted_ood\": int(np.sum(val_ood_pred)) if ood_val_features is not None else 0,\n",
        "            \"ood_rate\": float(np.mean(val_ood_pred)) if ood_val_features is not None else 0.0,\n",
        "            \"mean_distance\": float(np.mean(val_distances)) if ood_val_features is not None else 0.0,\n",
        "            \"distance_std\": float(np.std(val_distances)) if ood_val_features is not None else 0.0\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    \"usage_instructions\": {\n",
        "        \"loading\": \"Use json.load() to load detector parameters\",\n",
        "        \"feature_extraction\": f\"Extract features from layer '{model.layers[-4].name}' of the trained classifier\",\n",
        "        \"prediction\": \"Use extracted features with loaded detector for OOD detection\",\n",
        "        \"threshold\": \"Distances above threshold indicate OOD samples\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save training metadata\n",
        "metadata_filename = f\"ood_training_metadata_{model_name}_{timestamp}.json\"\n",
        "metadata_filepath = os.path.join(ood_detector_dir, metadata_filename)\n",
        "\n",
        "with open(metadata_filepath, 'w') as f:\n",
        "    json.dump(training_metadata, f, indent=2)\n",
        "\n",
        "print(f\"📁 Files saved:\")\n",
        "print(f\"  • OOD Detector: {ood_detector_filepath}\")\n",
        "print(f\"  • Training Metadata: {metadata_filepath}\")\n",
        "\n",
        "print(f\"\\n📋 Training Summary:\")\n",
        "print(f\"  • Model: {model_name}\")\n",
        "print(f\"  • Training samples: {X_train.shape[0]}\")\n",
        "print(f\"  • Feature dimension: {ood_train_features.shape[1]}\")\n",
        "print(f\"  • Known classes: {len(class_names)}\")\n",
        "print(f\"  • OOD threshold: {ood_detector.threshold:.4f}\")\n",
        "print(f\"  • Final accuracy: {train_hist['accuracy'][-1]:.4f}\")\n",
        "\n",
        "print(f\"\\n🚀 Deployment Ready:\")\n",
        "print(f\"  1. Load classifier model: tf.keras.models.load_model('{model_name}')\")\n",
        "print(f\"  2. Load OOD detector: json.load('{ood_detector_filename}')\")\n",
        "print(f\"  3. Extract features from penultimate layer\")\n",
        "print(f\"  4. Apply Mahalanobis distance for OOD detection\")\n",
        "\n",
        "print(f\"\\n✅ OOD detector training and saving completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
