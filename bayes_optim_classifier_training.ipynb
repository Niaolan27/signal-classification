{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# Test GPU usage\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"✓ GPU detected and available\")\n",
        "    with tf.device('/GPU:0'):\n",
        "        a = tf.constant([1.0, 2.0, 3.0])\n",
        "        b = tf.constant([4.0, 5.0, 6.0])\n",
        "        c = tf.add(a, b)\n",
        "        print(f\"GPU computation result: {c}\")\n",
        "else:\n",
        "    print(\"❌ No GPU available - using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Housekeeping Matters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0HyUdmk6G_j",
        "outputId": "9d8fa821-2481-479b-d753-8bc85d6b3af0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This ipynb file is adapted from the previous ResNet training script\n",
        "Author: Jason Niow\n",
        "DtaeL \n",
        "'''\n",
        "\n",
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import pathlib\n",
        "\n",
        "print(f'tensorflow version: {tf.__version__}')\n",
        "print(f'pandas version: {pd.__version__}')\n",
        "print(f'numpy version: {np.__version__}')\n",
        "print(f'seaborn version: {sns.__version__}')\n",
        "\n",
        "# check tensorflow GPU device support\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    print('GPU present')\n",
        "else:\n",
        "    print('GPU absent')\n",
        "\n",
        "# paths to load datasets from\n",
        "train_store_path = 'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/synthetic/synthetic_set3_train'\n",
        "test_store_path = 'C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/Data/synthetic/synthetic_set3_test'\n",
        "\n",
        "# convert to pathlib Path objects\n",
        "train_dir = pathlib.Path(train_store_path)\n",
        "test_dir = pathlib.Path(test_store_path)\n",
        "\n",
        "# get list of datasets paths in dir\n",
        "train_ds_paths = sorted(list(train_dir.glob('*.csv')))\n",
        "test_ds_paths = sorted(list(test_dir.glob('*.csv')))\n",
        "\n",
        "\n",
        "# extract classification target from file names\n",
        "train_ds_type = np.array([x.parts[-1].split('_')[:2] for x in train_ds_paths])\n",
        "test_ds_type = np.array([x.parts[-1].split('_')[:2] for x in test_ds_paths])\n",
        "\n",
        "# Get list of classification labels of dataset e.g. 8CPSK, FM, 16qam\n",
        "train_ds_mod = [s.upper() for s in train_ds_type[:,0]]\n",
        "test_ds_mod = [s.upper() for s in test_ds_type[:,0]]\n",
        "\n",
        "# Get list of classification frequency\n",
        "train_ds_freq = [s.upper() for s in train_ds_type[:, 1]]\n",
        "test_ds_freq = [s.upper() for s in test_ds_type[:, 1]]\n",
        "\n",
        "# generate signal type tags\n",
        "known_signal_tags = {'16QAM', '8CPSK', 'FM'}\n",
        "signal_tags = {'16QAM': 0, '8CPSK': 1, 'FM': 2, 'UNKNOWN': 3}\n",
        "# signal_tags = {k : i for i, k in enumerate(np.unique(sorted([s.upper() for s in train_ds_mod] + ['UNKNOWN'])))}\n",
        "\n",
        "print(signal_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, s in enumerate(train_ds_type[:,1]):\n",
        "    if s.upper() == 'FM':\n",
        "        print(train_ds_type[index, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(set(train_ds_freq))\n",
        "print(set(train_ds_mod))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P-lvxTnbtJ2-",
        "outputId": "1221943f-c330-4f99-9b19-34f6caeb02f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load the dataset(s)\n",
        "\n",
        "# load dataset information\n",
        "specs = []\n",
        "datasets = []\n",
        "\n",
        "for dataset_paths in [train_ds_paths, test_ds_paths]:\n",
        "    temp_ds = []\n",
        "    temp_specs = []\n",
        "\n",
        "    for path in dataset_paths:\n",
        "        print(f'loading {path}...', end=' ')\n",
        "\n",
        "        # load dataset details - Sampling frequency, Number of Samples, Number of Records\n",
        "        df_spec = pd.read_csv(path, nrows=10, header=None, index_col=0, names=['info'])\n",
        "        df_spec = df_spec.drop(['Version', 'DateTime', 'TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'], axis=0).astype('int')\n",
        "\n",
        "        temp_specs.append(df_spec)\n",
        "\n",
        "        # load data, strip unnecessary bits out - I/Q data\n",
        "        df = pd.read_csv(path, skiprows=10, names=['I', 'Q'])\n",
        "\n",
        "        df = df.loc[~df['I'].isin(['TimestampOffset', 'TriggerPosition', 'FastFrameID', 'IDInFastFrame', 'TotalInFastFrame'])]\n",
        "        df['I'] = df['I'].astype('float')\n",
        "\n",
        "        print(f'loaded')\n",
        "\n",
        "        temp_ds.append(df)\n",
        "\n",
        "    datasets.append(temp_ds)\n",
        "    specs.append(temp_specs)\n",
        "\n",
        "print('done.')\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(specs[0]), len(specs[1])\n",
        "len(datasets[0]), len(datasets[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Processing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IXJ_06DTwIaY",
        "outputId": "d35c419b-9e43-4ad9-bf5f-f835a396dedf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# split dataset(s) into records, extract test dataset\n",
        "processed = []\n",
        "\n",
        "# number of test records to extract\n",
        "ntest = 100\n",
        "rlength = 1024\n",
        "nrecords = 1\n",
        "nsamples = 10000\n",
        "\n",
        "for h, dataset in enumerate(datasets): # loops through training, then testing\n",
        "    if h == 0:\n",
        "        dataset_type = 'TRAINING'\n",
        "    else: dataset_type = 'TESTING'\n",
        "    temp_processed = []\n",
        "    specs_df = specs[h]\n",
        "\n",
        "    print(f'\\nType\\t\\tLocation\\tTotal Records\\tSamples/Record')\n",
        "    # Loops through each data point in the dataset\n",
        "    for i in range(len(dataset)):\n",
        "        # nrecords = specs_df[i].loc['NumberRecords']['info'] if dataset_type == 'TRAINING' else 400 ### wtf is this\n",
        "        # nrecords = specs_df[i].loc['NumberRecords']['info']\n",
        "        # nsamples = specs_df[i].loc['NumberSamples']['info']\n",
        "\n",
        "        ds_length = dataset[i].shape[0]\n",
        "\n",
        "        # make life easier\n",
        "        ds_mod = train_ds_mod if dataset_type == 'TRAINING' else test_ds_mod\n",
        "        ds_freq = train_ds_freq if dataset_type == 'TRAINING' else test_ds_freq\n",
        "\n",
        "        # sanity check\n",
        "        print(f'{ds_mod[i]:<13}\\t{ds_freq[i]:<15}\\t{nrecords:<7}\\t\\t{nsamples:<7}')\n",
        "\n",
        "        # loop through dataset to split\n",
        "        for j in range(nrecords):\n",
        "            # extract sample length worth of samples for each record, then transpose for easier access later\n",
        "            record = dataset[i].iloc[(nsamples * j):(nsamples * (j+1))].values.T\n",
        "\n",
        "            # pad shorter records with random padding to rlength\n",
        "            if nsamples < rlength:\n",
        "                print(f\"i: {i} j : {j} Sample length {nsamples} is lesser than {rlength}\")\n",
        "                # deterine pad amount\n",
        "                pad_length = rlength - nsamples\n",
        "                lpad_length = np.random.randint(0, pad_length+1)\n",
        "                rpad_length = pad_length - lpad_length\n",
        "\n",
        "                # generate pad\n",
        "                lpad = np.zeros((2, lpad_length))\n",
        "                rpad = np.zeros((2, rpad_length))\n",
        "\n",
        "                # concatenate pad\n",
        "                record = np.concatenate([lpad, record, rpad], axis=1)\n",
        "\n",
        "            # truncate longer records to rlength\n",
        "            elif nsamples > rlength:\n",
        "                # print(f\"i: {i} j : {j} Sample length {nsamples} is greater than {rlength}\")\n",
        "                record = record[:,:rlength]\n",
        "\n",
        "            # add processed record to list\n",
        "            signal_tag = signal_tags.get(ds_mod[i], signal_tags['UNKNOWN']) # default to UNKNOWN if not one of the known classes\n",
        "            temp_processed.append([ds_mod[i], signal_tag, ds_freq[i], record])\n",
        "\n",
        "    processed.append(temp_processed)\n",
        "\n",
        "# convert list into dataframes for later use, randomise, extract test records\n",
        "df_train = pd.DataFrame(processed[0], columns=['signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
        "df_test = pd.DataFrame(processed[1], columns=['signal_type', 'tag', 'location', 'record']).sample(frac=1, random_state=42)\n",
        "\n",
        "# print dataset statistics\n",
        "print(f'\\n{\"Stats\":^30}')\n",
        "print(f'Dataset\\tLength\\tRecords/Sample')\n",
        "print(f'Train\\t{df_train.shape[0]:<5}\\t{df_train[\"record\"].iloc[0].shape[1]}')\n",
        "print(f'Test\\t{df_test.shape[0]:<5}\\t{df_train[\"record\"].iloc[0].shape[1]}')\n",
        "\n",
        "\n",
        "# define one hot encode function\n",
        "def one_hot(arr, n_cat):\n",
        "    output = []\n",
        "    for n in arr:\n",
        "        result = np.zeros(n_cat)\n",
        "        result[n] = 1\n",
        "\n",
        "        output.append(result)\n",
        "\n",
        "    return np.array(output, dtype=int)\n",
        "\n",
        "# extract train and test data\n",
        "X_train = np.concatenate(df_train['record'].values).reshape((df_train.shape[0], 2, rlength, 1))\n",
        "y_train = one_hot(df_train['tag'].values, len(signal_tags))\n",
        "\n",
        "#X_test = np.concatenate(df_test['record'].values).reshape((df_test.shape[0], 2, rlength, 1))\n",
        "#y_test = one_hot(df_test['tag'].values, len(signal_tags))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Model (without dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1pn2XwbDPQp",
        "outputId": "4c73e934-7fbb-4f44-888d-020580ebec42"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import model stuff\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Dense, Input, Flatten\n",
        "\n",
        "# functions for model segments\n",
        "def res_unit(x, dim, n):\n",
        "    '''\n",
        "    function that creates a residual unit for each residual stack.\n",
        "\n",
        "    INPUT PARAMETERS\n",
        "    x: layer to connect to\n",
        "    dim: size of layer\n",
        "    n: number of units to create\n",
        "    '''\n",
        "\n",
        "    for _ in range(n):\n",
        "        u = Conv2D(dim, 2, activation='relu', padding='same')(x)\n",
        "        u = Conv2D(dim, 2, activation='linear', padding='same')(u)\n",
        "\n",
        "        # skip-con\n",
        "        x = concatenate([u, x])\n",
        "\n",
        "    return x\n",
        "\n",
        "def res_stack(x, dim):\n",
        "    '''\n",
        "    function that creates a residual stack for the model\n",
        "\n",
        "    INPUT PARAMETERS\n",
        "    x: layer to connect to\n",
        "    dim: size of stack\n",
        "    '''\n",
        "\n",
        "    s = Conv2D(dim, 1, activation='linear', padding='same')(x)\n",
        "    s = res_unit(s, dim, 2)\n",
        "    s = MaxPooling2D(2, padding='same')(s)\n",
        "\n",
        "    return s\n",
        "\n",
        "# function to create main model\n",
        "def create_model(in_dim, out_dim):\n",
        "    '''\n",
        "    function to construct the actual resnet model.\n",
        "\n",
        "    INPUT PARAMETERS\n",
        "    in_dim: dimensions of input\n",
        "    out_dim: size of output\n",
        "    '''\n",
        "\n",
        "    input_layer = Input(in_dim)\n",
        "\n",
        "    # res stacks\n",
        "    x = res_stack(input_layer, 512)\n",
        "    x = res_stack(x, 256)\n",
        "    x = res_stack(x, 128)\n",
        "    x = res_stack(x, 64)\n",
        "    x = res_stack(x, 32)\n",
        "    x = res_stack(x, 16)\n",
        "\n",
        "    # fully connected layers\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "\n",
        "    output_layer = Dense(out_dim, activation='softmax')(x)\n",
        "\n",
        "    # turn layers into model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name='resnet_rf_classification_model')\n",
        "\n",
        "    return model\n",
        "\n",
        "# model = create_model((2, rlength, 1), len(signal_tags))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian Optimization for Hyperparameter Tuning\n",
        "\n",
        "In this section, we'll use Bayesian Optimization to automatically find the best hyperparameters for our ResNet model. This includes optimizing learning rate, batch size, epochs, dropout rates, and dense layer sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Bayesian Optimization\n",
        "import subprocess\n",
        "import sys\n",
        "import skopt\n",
        "\n",
        "# Import Bayesian Optimization libraries\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.plots import plot_convergence, plot_objective\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameter search space\n",
        "dimensions = [\n",
        "    Real(low=1e-5, high=1e-2, prior='log-uniform', name='learning_rate'),\n",
        "    Integer(low=16, high=128, name='batch_size'),\n",
        "    Integer(low=10, high=100, name='epochs'),\n",
        "    Real(low=0.0, high=0.5, name='dropout_rate'),\n",
        "    Integer(low=64, high=512, name='dense_units'),\n",
        "    Integer(low=5, high=20, name='patience')\n",
        "]\n",
        "\n",
        "# Extract dimension names for easy reference\n",
        "dimension_names = [dim.name for dim in dimensions]\n",
        "print(\"Hyperparameter search space:\")\n",
        "for dim in dimensions:\n",
        "    print(f\"  {dim.name}: {dim}\")\n",
        "\n",
        "# Store results for analysis\n",
        "optimization_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced model creation function with hyperparameters\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Dense, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def create_optimized_model(in_dim, out_dim, dropout_rate=0.2, dense_units=128):\n",
        "    \"\"\"\n",
        "    Create ResNet model with configurable hyperparameters\n",
        "    \n",
        "    Args:\n",
        "        in_dim: Input dimensions\n",
        "        out_dim: Output dimensions  \n",
        "        dropout_rate: Dropout rate for regularization\n",
        "        dense_units: Number of units in dense layers\n",
        "    \"\"\"\n",
        "    input_layer = Input(in_dim)\n",
        "\n",
        "    # res stacks (same as original)\n",
        "    x = res_stack(input_layer, 512)\n",
        "    x = res_stack(x, 256)\n",
        "    x = res_stack(x, 128)\n",
        "    x = res_stack(x, 64)\n",
        "    x = res_stack(x, 32)\n",
        "    x = res_stack(x, 16)\n",
        "\n",
        "    # fully connected layers with dropout\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Dense(dense_units, activation='relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    output_layer = Dense(out_dim, activation='softmax')(x)\n",
        "\n",
        "    # turn layers into model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer, name='optimized_resnet_rf_classification_model')\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"Enhanced model creation function defined with dropout and configurable dense units\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the objective function for Bayesian Optimization\n",
        "@use_named_args(dimensions)\n",
        "def objective_function(**params):\n",
        "    \"\"\"\n",
        "    Objective function to minimize (validation loss)\n",
        "    \n",
        "    Returns the validation loss after training with given hyperparameters\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training with hyperparameters: {params}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        # Clear any existing models from memory\n",
        "        tf.keras.backend.clear_session()\n",
        "        \n",
        "        # Create model with current hyperparameters\n",
        "        model = create_optimized_model(\n",
        "            in_dim=(2, rlength, 1), \n",
        "            out_dim=len(signal_tags),\n",
        "            dropout_rate=params['dropout_rate'],\n",
        "            dense_units=params['dense_units']\n",
        "        )\n",
        "        \n",
        "        # Compile model\n",
        "        optimizer = Adam(learning_rate=params['learning_rate'])\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        # Create early stopping callback\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=params['patience'],\n",
        "            verbose=1,\n",
        "            mode='min',\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "        \n",
        "        # Train model\n",
        "        start_time = time.time()\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=params['epochs'],\n",
        "            batch_size=params['batch_size'],\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        # Get best validation loss\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "        val_accuracy = max(history.history['val_accuracy'])\n",
        "        \n",
        "        # Store results\n",
        "        result = {\n",
        "            'params': params,\n",
        "            'val_loss': val_loss,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'training_time': training_time,\n",
        "            'epochs_trained': len(history.history['loss']),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        optimization_results.append(result)\n",
        "        \n",
        "        print(f\"\\\\nResult: Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
        "        print(f\"Training time: {training_time:.2f} seconds\")\n",
        "        print(f\"Epochs trained: {len(history.history['loss'])}\")\n",
        "        \n",
        "        return val_loss\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {str(e)}\")\n",
        "        # Return a high loss value for failed trials\n",
        "        return 10.0\n",
        "\n",
        "print(\"Objective function defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Initial Hyperparameters\n",
        "\n",
        "Define your preferred starting hyperparameters for the Bayesian optimization. These will be evaluated first before the algorithm begins its intelligent search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your preferred initial hyperparameters\n",
        "# You can modify these based on your domain knowledge or previous experiments\n",
        "\n",
        "initial_hyperparameters = [\n",
        "    # Hyperparameter set 1: Your current baseline/default\n",
        "    {\n",
        "        'learning_rate': 0.0003,\n",
        "        'batch_size': 32,\n",
        "        'epochs': 50,\n",
        "        'dropout_rate': 0.2,\n",
        "        'dense_units': 128,\n",
        "        'patience': 10\n",
        "    },\n",
        "    \n",
        "    # Hyperparameter set 2: Lower learning rate, higher regularization\n",
        "    {\n",
        "        'learning_rate': 0.0001,\n",
        "        'batch_size': 64,\n",
        "        'epochs': 80,\n",
        "        'dropout_rate': 0.3,\n",
        "        'dense_units': 256,\n",
        "        'patience': 15\n",
        "    },\n",
        "    \n",
        "    # Hyperparameter set 3: Higher learning rate, less regularization\n",
        "    {\n",
        "        'learning_rate': 0.001,\n",
        "        'batch_size': 16,\n",
        "        'epochs': 40,\n",
        "        'dropout_rate': 0.1,\n",
        "        'dense_units': 512,\n",
        "        'patience': 8\n",
        "    },\n",
        "    \n",
        "    # Hyperparameter set 4: Conservative balanced approach\n",
        "    {\n",
        "        'learning_rate': 0.0005,\n",
        "        'batch_size': 48,\n",
        "        'epochs': 60,\n",
        "        'dropout_rate': 0.25,\n",
        "        'dense_units': 192,\n",
        "        'patience': 12\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"🎯 INITIAL HYPERPARAMETER SETS DEFINED:\")\n",
        "print(\"=\"*60)\n",
        "for i, params in enumerate(initial_hyperparameters, 1):\n",
        "    print(f\"Set {i}:\")\n",
        "    for param, value in params.items():\n",
        "        if param == 'learning_rate':\n",
        "            print(f\"  {param}: {value:.1e}\")\n",
        "        else:\n",
        "            print(f\"  {param}: {value}\")\n",
        "    print()\n",
        "\n",
        "# Convert initial hyperparameters to the format expected by skopt\n",
        "initial_points = []\n",
        "for params in initial_hyperparameters:\n",
        "    point = [params[name] for name in dimension_names]\n",
        "    initial_points.append(point)\n",
        "\n",
        "print(f\"✓ {len(initial_hyperparameters)} initial hyperparameter sets ready for optimization\")\n",
        "print(f\"✓ Initial points converted to skopt format: {len(initial_points)} points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to test individual hyperparameter sets before full optimization\n",
        "def test_hyperparameter_set(params, set_name=\"Custom\"):\n",
        "    \"\"\"\n",
        "    Test a single hyperparameter set and return results\n",
        "    \"\"\"\n",
        "    print(f\"\\n🧪 TESTING HYPERPARAMETER SET: {set_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    for param, value in params.items():\n",
        "        if param == 'learning_rate':\n",
        "            print(f\"  {param}: {value:.1e}\")\n",
        "        else:\n",
        "            print(f\"  {param}: {value}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Convert params to list format for objective function\n",
        "    param_values = [params[name] for name in dimension_names]\n",
        "    \n",
        "    # Test the hyperparameters\n",
        "    result = objective_function(*param_values)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def test_all_initial_sets():\n",
        "    \"\"\"\n",
        "    Test all predefined initial hyperparameter sets\n",
        "    \"\"\"\n",
        "    print(\"🚀 TESTING ALL INITIAL HYPERPARAMETER SETS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    results_summary = []\n",
        "    \n",
        "    for i, params in enumerate(initial_hyperparameters, 1):\n",
        "        set_name = f\"Initial Set {i}\"\n",
        "        result = test_hyperparameter_set(params, set_name)\n",
        "        \n",
        "        if result is not None:\n",
        "            results_summary.append({\n",
        "                'set_name': set_name,\n",
        "                'params': params,\n",
        "                'val_loss': result\n",
        "            })\n",
        "    \n",
        "    # Display summary\n",
        "    if results_summary:\n",
        "        print(f\"\\n📊 INITIAL HYPERPARAMETER SETS SUMMARY:\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"{'Set Name':<15} {'Val Loss':<12} {'LR':<12} {'Batch':<8} {'Dropout':<10} {'Dense':<8}\")\n",
        "        print(f\"{'-'*80}\")\n",
        "        \n",
        "        # Sort by validation loss\n",
        "        results_summary.sort(key=lambda x: x['val_loss'])\n",
        "        \n",
        "        for result in results_summary:\n",
        "            params = result['params']\n",
        "            print(f\"{result['set_name']:<15} {result['val_loss']:<12.4f} \"\n",
        "                  f\"{params['learning_rate']:<12.2e} {params['batch_size']:<8} \"\n",
        "                  f\"{params['dropout_rate']:<10.3f} {params['dense_units']:<8}\")\n",
        "        \n",
        "        best_set = results_summary[0]\n",
        "        print(f\"\\n🏆 BEST INITIAL SET: {best_set['set_name']} (Val Loss: {best_set['val_loss']:.4f})\")\n",
        "        \n",
        "        return results_summary\n",
        "    \n",
        "    return None\n",
        "\n",
        "print(\"\\n🔧 TESTING FUNCTIONS AVAILABLE:\")\n",
        "print(\"1. test_all_initial_sets() - Test all predefined sets\")\n",
        "print(\"2. test_hyperparameter_set(your_params, 'Your Name') - Test custom set\")\n",
        "print(\"\\nExample usage:\")\n",
        "print(\"  results = test_all_initial_sets()\")\n",
        "print(\"  custom_result = test_hyperparameter_set(initial_hyperparameters[0], 'My Test')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Customize Your Initial Hyperparameters\n",
        "\n",
        "### 📝 **Editing Initial Hyperparameters**\n",
        "\n",
        "To use your own starting hyperparameters:\n",
        "\n",
        "1. **Modify the `initial_hyperparameters` list above** with your preferred values\n",
        "2. **Add or remove hyperparameter sets** as needed\n",
        "3. **Ensure all parameters are within the search space bounds** defined in `dimensions`\n",
        "\n",
        "### 🎯 **Parameter Guidelines**\n",
        "\n",
        "- **learning_rate**: 1e-5 to 1e-2 (try: 0.0001, 0.0003, 0.001, 0.003)\n",
        "- **batch_size**: 16 to 128 (try: 16, 32, 48, 64, 96, 128)\n",
        "- **epochs**: 10 to 100 (try: 30, 50, 80, 100)\n",
        "- **dropout_rate**: 0.0 to 0.5 (try: 0.1, 0.2, 0.3, 0.4)\n",
        "- **dense_units**: 64 to 512 (try: 128, 192, 256, 384, 512)\n",
        "- **patience**: 5 to 20 (try: 8, 10, 12, 15)\n",
        "\n",
        "### 🚀 **Recommended Workflow**\n",
        "\n",
        "1. **Test individual sets first**: `test_all_initial_sets()`\n",
        "2. **Run quick optimization**: `run_quick_optimization()`\n",
        "3. **Run full optimization**: `run_full_optimization()`\n",
        "4. **Analyze results** with the visualization cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Bayesian Optimization with Custom Initial Points\n",
        "def run_bayesian_optimization_with_initial_points(n_calls=15, n_random_starts=2, use_initial_points=True):\n",
        "    \"\"\"\n",
        "    Run Bayesian optimization with your custom initial hyperparameters\n",
        "    \n",
        "    Args:\n",
        "        n_calls: Total number of optimization calls\n",
        "        n_random_starts: Additional random points beyond initial points\n",
        "        use_initial_points: Whether to use your predefined initial points\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"🚀 STARTING BAYESIAN OPTIMIZATION\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if use_initial_points and 'initial_points' in globals():\n",
        "        print(f\"📊 Configuration:\")\n",
        "        print(f\"  Total calls: {n_calls}\")\n",
        "        print(f\"  Initial points: {len(initial_points)} (your predefined sets)\")\n",
        "        print(f\"  Additional random starts: {n_random_starts}\")\n",
        "        print(f\"  Guided optimization calls: {n_calls - len(initial_points) - n_random_starts}\")\n",
        "        \n",
        "        # Clear previous results\n",
        "        optimization_results.clear()\n",
        "        \n",
        "        # Run optimization with your initial points\n",
        "        optimization_start_time = time.time()\n",
        "        \n",
        "        result = gp_minimize(\n",
        "            func=objective_function,\n",
        "            dimensions=dimensions,\n",
        "            n_calls=n_calls,\n",
        "            n_initial_points=n_random_starts,  # Additional random points\n",
        "            x0=initial_points,  # Your predefined starting points\n",
        "            acq_func='EI',  # Expected Improvement acquisition function\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        print(f\"📊 Configuration (Standard Mode):\")\n",
        "        print(f\"  Total calls: {n_calls}\")\n",
        "        print(f\"  Random initial points: {n_random_starts + 2}\")\n",
        "        print(f\"  Guided optimization calls: {n_calls - n_random_starts - 2}\")\n",
        "        \n",
        "        # Clear previous results\n",
        "        optimization_results.clear()\n",
        "        \n",
        "        # Run standard optimization\n",
        "        optimization_start_time = time.time()\n",
        "        \n",
        "        result = gp_minimize(\n",
        "            func=objective_function,\n",
        "            dimensions=dimensions,\n",
        "            n_calls=n_calls,\n",
        "            n_initial_points=n_random_starts + 2,  # Standard random points\n",
        "            acq_func='EI',  # Expected Improvement acquisition function\n",
        "            random_state=42\n",
        "        )\n",
        "    \n",
        "    return result, optimization_start_time\n",
        "\n",
        "# Quick optimization (fewer calls for testing)\n",
        "def run_quick_optimization():\n",
        "    \"\"\"Run a quick optimization for testing\"\"\"\n",
        "    return run_bayesian_optimization_with_initial_points(n_calls=8, n_random_starts=1)\n",
        "\n",
        "# Full optimization \n",
        "def run_full_optimization():\n",
        "    \"\"\"Run full optimization with more calls\"\"\"\n",
        "    return run_bayesian_optimization_with_initial_points(n_calls=20, n_random_starts=3)\n",
        "\n",
        "# Execute the optimization (you can choose which one to run)\n",
        "print(\"🎮 OPTIMIZATION OPTIONS:\")\n",
        "print(\"1. run_quick_optimization() - Fast test (8 total calls)\")\n",
        "print(\"2. run_full_optimization() - Thorough search (20 calls)\")\n",
        "print(\"3. run_bayesian_optimization_with_initial_points(n_calls, n_random_starts) - Custom\")\n",
        "print(\"\\nChoose and run one of the above functions, or run the default below:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Default run (you can modify this)\n",
        "result, optimization_start_time = run_bayesian_optimization_with_initial_points(n_calls=15, n_random_starts=2)\n",
        "\n",
        "optimization_total_time = time.time() - optimization_start_time\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(f\"🎉 BAYESIAN OPTIMIZATION COMPLETED!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"⏱️  Total optimization time: {optimization_total_time:.2f} seconds\")\n",
        "print(f\"🏆 Best validation loss: {result.fun:.4f}\")\n",
        "print(f\"\\\\n🎯 Best hyperparameters:\")\n",
        "best_params = {}\n",
        "for name, value in zip(dimension_names, result.x):\n",
        "    best_params[name] = value\n",
        "    if name == 'learning_rate':\n",
        "        print(f\"  {name}: {value:.2e}\")\n",
        "    elif name in ['batch_size', 'dense_units', 'epochs', 'patience']:\n",
        "        print(f\"  {name}: {int(value)}\")\n",
        "    else:\n",
        "        print(f\"  {name}: {value:.4f}\")\n",
        "\n",
        "# Analyze initial points performance if they were used\n",
        "if 'initial_points' in globals() and len(optimization_results) >= len(initial_points):\n",
        "    print(f\"\\\\n📈 INITIAL POINTS PERFORMANCE:\")\n",
        "    print(f\"{'Point':<20} {'Val Loss':<12} {'Type':<15}\")\n",
        "    print(f\"{'-'*50}\")\n",
        "    \n",
        "    for i in range(len(initial_points)):\n",
        "        if i < len(optimization_results):\n",
        "            loss = optimization_results[i]['val_loss']\n",
        "            print(f\"Initial Set {i+1:<12} {loss:<12.4f} Predefined\")\n",
        "\n",
        "# Save optimization results\n",
        "results_dict = {\n",
        "    'best_params': best_params,\n",
        "    'best_loss': float(result.fun),\n",
        "    'all_results': optimization_results,\n",
        "    'optimization_time': optimization_total_time,\n",
        "    'n_calls': len(optimization_results),\n",
        "    'initial_points_used': initial_hyperparameters if 'initial_hyperparameters' in globals() else None\n",
        "}\n",
        "\n",
        "# Save to JSON file with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f'bayesian_optimization_results_{timestamp}.json'\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(results_dict, f, indent=2)\n",
        "\n",
        "print(f\"\\\\n💾 Results saved to '{filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Optimization Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Convergence plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plot_convergence(result)\n",
        "plt.title('Bayesian Optimization Convergence')\n",
        "plt.xlabel('Number of calls')\n",
        "plt.ylabel('Validation Loss')\n",
        "\n",
        "# 2. Hyperparameter vs Performance plots\n",
        "param_names = ['learning_rate', 'batch_size', 'dropout_rate', 'dense_units']\n",
        "param_indices = [dimension_names.index(name) for name in param_names if name in dimension_names]\n",
        "\n",
        "for i, (param_idx, param_name) in enumerate(zip(param_indices[:4], param_names[:4])):\n",
        "    plt.subplot(2, 3, i+2)\n",
        "    param_values = [res['params'][param_name] for res in optimization_results]\n",
        "    val_losses = [res['val_loss'] for res in optimization_results]\n",
        "    \n",
        "    plt.scatter(param_values, val_losses, alpha=0.7)\n",
        "    plt.xlabel(param_name)\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.title(f'{param_name} vs Val Loss')\n",
        "    \n",
        "    if param_name == 'learning_rate':\n",
        "        plt.xscale('log')\n",
        "\n",
        "# 3. Training time vs Performance\n",
        "plt.subplot(2, 3, 6)\n",
        "training_times = [res['training_time'] for res in optimization_results]\n",
        "val_losses = [res['val_loss'] for res in optimization_results]\n",
        "plt.scatter(training_times, val_losses, alpha=0.7)\n",
        "plt.xlabel('Training Time (seconds)')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Training Time vs Val Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed results\n",
        "print(\"\\\\nDetailed Results Summary:\")\n",
        "print(\"=\"*80)\n",
        "sorted_results = sorted(optimization_results, key=lambda x: x['val_loss'])\n",
        "\n",
        "print(f\"{'Rank':<4} {'Val Loss':<10} {'Val Acc':<10} {'LR':<10} {'Batch':<6} {'Dropout':<8} {'Dense':<6}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, res in enumerate(sorted_results[:10]):  # Top 10 results\n",
        "    params = res['params']\n",
        "    print(f\"{i+1:<4} {res['val_loss']:<10.4f} {res['val_accuracy']:<10.4f} \"\n",
        "          f\"{params['learning_rate']:<10.2e} {params['batch_size']:<6} \"\n",
        "          f\"{params['dropout_rate']:<8.3f} {params['dense_units']:<6}\")\n",
        "\n",
        "print(\"\\\\nBest hyperparameters found:\")\n",
        "best_params = dict(zip(dimension_names, result.x))\n",
        "for param, value in best_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\\\nBest validation loss: {result.fun:.4f}\")\n",
        "\n",
        "# Calculate improvement over baseline (if you want to compare with your original hyperparameters)\n",
        "baseline_loss = max([res['val_loss'] for res in optimization_results])  # Worst case as baseline\n",
        "improvement = ((baseline_loss - result.fun) / baseline_loss) * 100\n",
        "print(f\"Improvement over worst trial: {improvement:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Final Model with Optimal Hyperparameters\n",
        "print(\"Training final model with optimal hyperparameters...\")\n",
        "\n",
        "# Clear any existing models\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = dict(zip(dimension_names, result.x))\n",
        "\n",
        "# Create and compile the optimal model\n",
        "optimal_model = create_optimized_model(\n",
        "    in_dim=(2, rlength, 1),\n",
        "    out_dim=len(signal_tags),\n",
        "    dropout_rate=best_params['dropout_rate'],\n",
        "    dense_units=int(best_params['dense_units'])\n",
        ")\n",
        "\n",
        "optimal_model.compile(\n",
        "    optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train with optimal hyperparameters\n",
        "final_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=int(best_params['patience']),\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(f\"Training with optimal hyperparameters:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "final_history = optimal_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=int(best_params['epochs']),\n",
        "    batch_size=int(best_params['batch_size']),\n",
        "    validation_split=0.2,\n",
        "    callbacks=[final_callback],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the optimal model\n",
        "optimal_model.save('C:/Users/UserAdmin/Desktop/Jason - Signal Classification/AI Models/trained_models/bayesian_optimized_model')\n",
        "print(\"\\\\nOptimal model saved to 'trained_models/bayesian_optimized_model'\")\n",
        "\n",
        "# Store final training history\n",
        "final_train_hist = final_history.history\n",
        "\n",
        "print(f\"\\\\nFinal model performance:\")\n",
        "print(f\"Best validation loss: {min(final_train_hist['val_loss']):.4f}\")\n",
        "print(f\"Best validation accuracy: {max(final_train_hist['val_accuracy']):.4f}\")\n",
        "print(f\"Epochs trained: {len(final_train_hist['loss'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Original vs Optimized Model Performance\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Training and Validation Loss Comparison\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(final_train_hist['loss'], label='Optimized - Training Loss', linewidth=2)\n",
        "plt.plot(final_train_hist['val_loss'], label='Optimized - Validation Loss', linewidth=2)\n",
        "plt.title('Optimized Model: Training vs Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training and Validation Accuracy Comparison  \n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(final_train_hist['accuracy'], label='Optimized - Training Accuracy', linewidth=2)\n",
        "plt.plot(final_train_hist['val_accuracy'], label='Optimized - Validation Accuracy', linewidth=2)\n",
        "plt.title('Optimized Model: Training vs Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Hyperparameter Importance (based on variance in results)\n",
        "plt.subplot(2, 3, 3)\n",
        "param_variances = {}\n",
        "for param_name in ['learning_rate', 'batch_size', 'dropout_rate', 'dense_units']:\n",
        "    if param_name in dimension_names:\n",
        "        param_values = [res['params'][param_name] for res in optimization_results]\n",
        "        losses = [res['val_loss'] for res in optimization_results]\n",
        "        \n",
        "        # Normalize parameters for fair comparison\n",
        "        if param_name == 'learning_rate':\n",
        "            param_values = np.log10(param_values)\n",
        "        \n",
        "        correlation = np.corrcoef(param_values, losses)[0, 1]\n",
        "        param_variances[param_name] = abs(correlation)\n",
        "\n",
        "param_names = list(param_variances.keys())\n",
        "param_importance = list(param_variances.values())\n",
        "\n",
        "plt.bar(param_names, param_importance)\n",
        "plt.title('Hyperparameter Importance\\\\n(Absolute Correlation with Val Loss)')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 4: Optimization Progress\n",
        "plt.subplot(2, 3, 4)\n",
        "cumulative_best = []\n",
        "current_best = float('inf')\n",
        "for res in optimization_results:\n",
        "    if res['val_loss'] < current_best:\n",
        "        current_best = res['val_loss']\n",
        "    cumulative_best.append(current_best)\n",
        "\n",
        "plt.plot(range(1, len(cumulative_best)+1), cumulative_best, 'b-', linewidth=2, marker='o')\n",
        "plt.title('Optimization Progress\\\\n(Best Loss Found So Far)')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Best Validation Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Parameter Distribution of Best Results\n",
        "plt.subplot(2, 3, 5)\n",
        "best_results = sorted(optimization_results, key=lambda x: x['val_loss'])[:5]\n",
        "learning_rates = [res['params']['learning_rate'] for res in best_results]\n",
        "plt.scatter(range(len(learning_rates)), learning_rates, s=100, alpha=0.7)\n",
        "plt.yscale('log')\n",
        "plt.title('Learning Rates of Top 5 Models')\n",
        "plt.xlabel('Rank (1=best)')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Training Time vs Performance Trade-off\n",
        "plt.subplot(2, 3, 6)\n",
        "training_times = [res['training_time'] for res in optimization_results]\n",
        "val_accuracies = [res['val_accuracy'] for res in optimization_results]\n",
        "colors = [res['val_loss'] for res in optimization_results]\n",
        "\n",
        "scatter = plt.scatter(training_times, val_accuracies, c=colors, cmap='viridis_r', alpha=0.7)\n",
        "plt.colorbar(scatter, label='Validation Loss')\n",
        "plt.xlabel('Training Time (seconds)')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.title('Training Time vs Accuracy\\\\n(Color = Val Loss)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary Statistics\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"BAYESIAN OPTIMIZATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\\\nOptimization Configuration:\")\n",
        "print(f\"  Number of trials: {len(optimization_results)}\")\n",
        "print(f\"  Total optimization time: {optimization_total_time:.2f} seconds\")\n",
        "print(f\"  Average time per trial: {optimization_total_time/len(optimization_results):.2f} seconds\")\n",
        "\n",
        "print(f\"\\\\nBest Model Performance:\")\n",
        "best_result = min(optimization_results, key=lambda x: x['val_loss'])\n",
        "print(f\"  Validation Loss: {best_result['val_loss']:.4f}\")\n",
        "print(f\"  Validation Accuracy: {best_result['val_accuracy']:.4f}\")\n",
        "print(f\"  Training Time: {best_result['training_time']:.2f} seconds\")\n",
        "print(f\"  Epochs Trained: {best_result['epochs_trained']}\")\n",
        "\n",
        "print(f\"\\\\nOptimal Hyperparameters:\")\n",
        "for param, value in best_params.items():\n",
        "    if param == 'learning_rate':\n",
        "        print(f\"  {param}: {value:.2e}\")\n",
        "    elif param in ['batch_size', 'dense_units', 'epochs', 'patience']:\n",
        "        print(f\"  {param}: {int(value)}\")\n",
        "    else:\n",
        "        print(f\"  {param}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\\\nPerformance Distribution:\")\n",
        "val_losses = [res['val_loss'] for res in optimization_results]\n",
        "print(f\"  Best validation loss: {min(val_losses):.4f}\")\n",
        "print(f\"  Worst validation loss: {max(val_losses):.4f}\")\n",
        "print(f\"  Mean validation loss: {np.mean(val_losses):.4f}\")\n",
        "print(f\"  Std validation loss: {np.std(val_losses):.4f}\")\n",
        "\n",
        "improvement = ((max(val_losses) - min(val_losses)) / max(val_losses)) * 100\n",
        "print(f\"  Improvement range: {improvement:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new_tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
